<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Evidential Reasoning Approach for Predicting Popularity of Instagram Posts  </p>

<p>Authors: L. Rivadeneira, I. Loor  </p>

<p>DOI: 10.1109/ACCESS.2024.3510637  </p>

<p>Year: 2024  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Computer Science / Social Media Analytics  </p>

<p>Subdomain/Topic: Predictive modelling of social media engagement using evidential reasoning  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 78  </p>

<p>Operationalization Score: 85  </p>

<p>Contains Definition of Actionability: Yes (implicit)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (MAKER)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Quantitative / Predictive Modelling (Machine Learning)  </p>

<p>Study Context: Instagram post popularity prediction using visual and textual features  </p>

<p>Geographic/Institutional Context: Harvard University (USA) &amp; University of Oxford (UK) Instagram accounts  </p>

<p>Target Users/Stakeholders: Social media managers, marketing professionals, academic institutions, content strategists  </p>

<p>Primary Contribution Type: Methodological framework and comparative evaluation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Partial  </p>

<p>TI: No  </p>

<p>EX: Yes  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Evidential Reasoning Approach for Predicting Popularity of Instagram Posts  </p>

<p><strong>Authors:</strong>  </p>

<p>L. Rivadeneira, I. Loor  </p>

<p><strong>DOI:</strong>  </p>

<p>10.1109/ACCESS.2024.3510637  </p>

<p><strong>Year:</strong>  </p>

<p>2024  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Computer Science / Social Media Analytics  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Predictive modelling of social media engagement using evidential reasoning  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper evaluates the MAKER (Maximum likelihood evidential reasoning) approach for predicting Instagram post popularity, using data from Harvard and Oxford’s official accounts. The focus is on achieving predictive accuracy while ensuring transparency and interpretability—key challenges for machine learning in social media analytics. The study addresses how features like emojis, sentiment, hashtags, mentions, seasons, image type, time of day, and dominant colour affect engagement.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>United States (Harvard University) and United Kingdom (University of Oxford).  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Social media managers, marketing teams, academic communication officers, influencers, and analytics researchers.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Quantitative — predictive modelling with machine learning algorithms (MAKER, DT, SVM, KNN).  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Methodological framework and empirical validation.  </p>

<h2>General Summary of the Paper</h2>

<p>The study applies the MAKER algorithm, grounded in evidential reasoning, to predict the popularity of Instagram posts (binary classification: high/low based on median likes). Using 2022 data from Harvard and Oxford, two models are built for each institution—one using textual features (emojis, sentiment, hashtags, mentions, season) and the other using visual features (image type, time of day, dominant colour). MAKER is compared against decision trees, SVM, and KNN, achieving higher precision and interpretability. The paper not only evaluates predictive performance but also extracts actionable patterns, such as Harvard’s popular posts tending toward vibrant, scenic images in certain seasons, and Oxford’s benefiting from emoji use and specific content structures.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Implicitly defined as the capacity of model outputs to guide content strategy decisions through transparent, interpretable insights that reveal which post attributes are most likely to increase engagement.  </p>

<blockquote>
  <p>“MAKER’s interpretability means that it provides actionable insights… help users make informed decisions based on its insights and improve content strategies by revealing which features most influence engagement.” (p. 1)  </p>
</blockquote>

<blockquote>
  <p>“While this study focuses on proposing a model for prediction purposes, it is essential to translate these findings into actionable strategies for decision-makers…” (p. 13)  </p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Ability to identify specific post attributes correlated with higher popularity.</p></li>
<li><p>Transparency in reasoning (weights, reliabilities, evidence interdependencies).</p></li>
<li><p>Interpretability enabling justification of model outputs.</p></li>
<li><p>Context-specific feature patterns rather than one-size-fits-all rules.</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> MAKER (Maximum likelihood evidential reasoning).  </p></li>
<li><p><strong>Methods/Levers:</strong> Integration of textual and visual post features into interpretable evidential reasoning models; optimisation of evidence weights and reliabilities.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Data collection → Preprocessing → Feature extraction (textual/visual) → Model training/testing (5-fold split) → MAKER optimisation → Comparative performance evaluation → Pattern extraction for actionable strategies.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Median likes threshold, emoji/hashtag/mention counts, sentiment, season, image type, dominant colour, time of day.  </p></li>
<li><p><strong>Implementation Context:</strong> Official university Instagram accounts.  </p></li>
</ul>

<blockquote>
  <p>“This transparency yields an interpretable model… examining the relationship between output and input variables, as well as the rationale behind the assignment of weights and reliabilities…” (p. 3)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — outputs are interpretable and grounded in transparent parameter assignment.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — feature influence patterns are institution-specific.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Partial — focuses on achievable content adjustments but omits resource constraints.  </p></li>
<li><p><strong>TI (Timeliness):</strong> No explicit link.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — full traceability of decision process.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — aligns model with engagement improvement goals but not broader organisational KPIs.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Transparency, interpretability, data completeness handling.</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Evidential reasoning (ER) rule, based on Dempster-Shafer theory.</p></li>
<li><p>Transparency and interpretability in AI (Rudin, 2019).</p></li>
<li><p>Multimodal content engagement theory from prior social media analytics research.</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Precision, recall, F1-score, AUC, RMSE (used to assess predictive reliability).</p></li>
<li><p>Likelihood scores for evidence patterns.</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> API restrictions limiting automated data collection; exclusion of non-picture post formats; limited engagement metrics; manual feature categorisation.  </p></li>
<li><p><strong>Enablers:</strong> MAKER’s robustness to incomplete data; integration of multimodal features; transparent modelling process.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Extends prior predictive models for Instagram by addressing interpretability and transparency gaps. Unlike black-box models (e.g., CNNs, fusion networks), MAKER enables actionable feature-level insights.</p>

<h2>Summary</h2>

<p>The paper demonstrates how MAKER—a maximum likelihood evidential reasoning approach—can deliver not only accurate predictions of Instagram post popularity but also actionable, interpretable insights for content strategy. By modelling both textual and visual features, and optimising evidence weights/reliabilities, the approach surfaces institution-specific patterns (e.g., seasonal effects, colour palettes, emoji usage) linked to higher engagement. Actionability here is tied to the transparency and contextual relevance of outputs, empowering decision-makers to adjust strategies based on identified drivers of popularity. Compared to decision trees, SVM, and KNN, MAKER offers superior precision and interpretability, making it a valuable tool for data-informed social media management.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 78 — Strong implicit definition of actionability and systematic feature linkages; slightly less emphasis on broader contextual constraints.  </p></li>
<li><p><strong>Operationalization Score:</strong> 85 — Clear step-by-step operational process tied directly to achieving actionability; validated through comparative performance and feature interpretation.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“MAKER’s interpretability means that it provides actionable insights… help users make informed decisions based on its insights…” (p. 1)  </p></li>
<li><p>“Transparency yields an interpretable model… examining the relationship between output and input variables…” (p. 3)  </p></li>
<li><p>“It is essential to translate these findings into actionable strategies for decision-makers…” (p. 13)  </p></li>
<li><p>“Harvard’s popular posts typically show positive or neutral sentiment… Oxford’s popular posts… use more emojis, hashtags, and mentions.” (p. 11)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Rudin, C. (2019) on interpretable models vs. black-box AI.  </p></li>
<li><p>Yang &amp; Xu (2017) on inferential modelling with data in evidential reasoning.  </p></li>
<li><p>Aramendia-Muneta et al. (2021) on key image attributes for engagement.</p></li>
</ul>
