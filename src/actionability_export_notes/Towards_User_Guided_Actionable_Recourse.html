<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Towards User Guided Actionable Recourse</p>

<p>Authors: Jayanth Yetukuri, Ian Hardy, Yang Liu</p>

<p>DOI: https://doi.org/10.1145/3600211.3604708</p>

<p>Year: 2023</p>

<p>Publication Type: Conference</p>

<p>Discipline/Domain: Artificial Intelligence / Human-Centered Computing</p>

<p>Subdomain/Topic: Actionable Recourse, User Preferences in ML Explanations</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 92</p>

<p>Operationalization Score: 95</p>

<p>Contains Definition of Actionability: Yes (implicit, user-preference-centered)</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Yes</p>

<p>Contains Interpretability: Partial</p>

<p>Contains Framework/Model: Yes (UP-AR optimization &amp; workflow)</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Conceptual with empirical evaluation</p>

<p>Study Context: Actionable recourse in ML decision-making across domains such as credit, hiring, insurance, and criminal justice</p>

<p>Geographic/Institutional Context: University of California, Santa Cruz; U.S.</p>

<p>Target Users/Stakeholders: End-users affected by ML decisions (e.g., loan applicants), ML system designers</p>

<p>Primary Contribution Type: Method/Framework Proposal with Empirical Validation</p>

<p>CL: Yes — “communicating in terms of preference scores… improves the explainability of a recourse generation mechanism” (p.1)</p>

<p>CR: Yes — “actionability… centered explicitly around individual preferences… may not necessarily be equally actionable” (p.1)</p>

<p>FE: Yes — “feasible action set… actionable by Alice” (p.1)</p>

<p>TI: Partial — timeliness not a primary dimension, but operational efficiency is addressed</p>

<p>EX: Yes — “preference scores… improves the explainability of a recourse generation mechanism” (p.1)</p>

<p>GA: Yes — goal alignment with user’s own constraints and desires (p.1–2)</p>

<p>Reason if Not Eligible: n/a</p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Towards User Guided Actionable Recourse  </p>

<p><strong>Authors:</strong>  </p>

<p>Jayanth Yetukuri, Ian Hardy, Yang Liu  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1145/3600211.3604708  </p>

<p><strong>Year:</strong>  </p>

<p>2023  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Artificial Intelligence / Human-Centered Computing  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Actionable Recourse, User Preferences in ML Explanations  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the challenge of making ML-generated recourse actionable for individuals adversely affected by automated decisions in domains such as lending, hiring, insurance, and criminal justice. It critiques existing methods for ignoring individual user preferences, proposing a method to integrate them directly into the recourse generation process.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>University of California, Santa Cruz; U.S.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>End-users denied desired outcomes by ML systems; system designers and policymakers interested in trustworthy, user-centered AI.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual framework and algorithm development with empirical evaluation across multiple datasets.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Method/Framework Proposal with Empirical Validation  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The authors introduce <strong>User Preferred Actionable Recourse (UP-AR)</strong>, a novel method for generating actionable recourse that incorporates explicit user preferences into the optimization process. They argue that existing actionable recourse (AR) approaches prioritize technical efficiency (e.g., proximity, sparsity, cost) but often ignore individual feasibility rooted in user constraints and desires. UP-AR captures preferences in three forms — scoring continuous features, bounding feature values, and ranking categorical features — and embeds them as soft and hard constraints in a gradient-based optimization framework. The approach is validated empirically across credit, income, and criminal recidivism datasets, showing better adherence to user preferences (lower pRMSE) and competitive or superior performance on traditional AR metrics. The authors emphasize that tailoring recourse to user-specific feasibility increases trust, explainability, and adoption.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is defined implicitly as the <strong>viability of taking a suggested action</strong> within the constraints and preferences of the individual. It is user-centered — what is actionable for one person may not be for another — and includes both universal feasibility constraints (e.g., immutable features) and local feasibility constraints (personal reluctances or capacities).  </p>

<blockquote>
  <p>“Actionability… is centered explicitly around individual preferences, and similar recourses… may not necessarily be equally actionable” (p.1)  </p>
</blockquote>

<blockquote>
  <p>“AR aims to provide… a feasible action set which is both actionable by Alice and… as low-cost [as possible]” (p.1)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Alignment with individual user constraints and desires (hard and soft rules)</p></li>
<li><p>Ability to operationalize within user’s own cost and effort parameters</p></li>
<li><p>Feasibility in practice (e.g., avoiding impossible or undesirable feature changes)</p></li>
<li><p>Explainability of why the action is suggested and how it fits user preferences</p></li>
<li><p>Personalization beyond general feasibility rules</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name:</strong> User Preferred Actionable Recourse (UP-AR)  </p></li>
<li><p><strong>Methods/Levers:</strong> Gradient-based iterative optimization weighted by user preference scores; temperature scaling for categorical action frequency; cost correction to remove redundant steps.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Elicit three types of preferences (scoring, bounding, ranking) from the user.  </p>

<p> 2. Embed these as constraints in optimization.  </p>

<p> 3. Generate candidate recourse via stochastic gradient-based updates informed by user preference-weighted costs.  </p>

<p> 4. Apply redundancy and cost correction to finalize recourse.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Percentile shift cost function; pRMSE to evaluate preference adherence; traditional AR metrics (success rate, redundancy, sparsity, proximity).  </p></li>
<li><p><strong>Implementation Context:</strong> Credit lending, income prediction, recidivism risk prediction.  </p></li>
</ul>

<blockquote>
  <p>“We start by enabling Alice to provide three types of user preferences… We embed them into an optimization function to guide the recourse generation mechanism” (p.2)  </p>
</blockquote>

<blockquote>
  <p>“The proposed method minimizes the cost of a recourse weighted by Γᵢ for all actionable features” (p.3)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — user preference scores increase explainability (p.1)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — recourse tailored to individual user profile (p.1–2)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — constraints ensure recommendations are viable for that user (p.1–3)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — efficiency in generation is discussed, but timeliness as a decision-making window is not explicit  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — preference-based reasoning improves explainability (p.1)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — recourse aligned with user’s stated objectives (p.1–2)  </p></li>
<li><p><strong>Other Dimensions:</strong> Diversity only as secondary contrast to preference tailoring  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Builds on <strong>Actionable Recourse (AR)</strong> as per Ustun et al. (2019)  </p></li>
<li><p>Local feasibility concept from Mahajan et al. (2019)  </p></li>
<li><p>Preference elicitation parallels human-in-the-loop approaches (De Toni et al., 2022)  </p></li>
<li><p>Optimization inspired by gradient-based adversarial example generation  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>pRMSE between desired and achieved feature cost proportions  </p></li>
<li><p>Constraint violations (lower is better)  </p></li>
<li><p>Redundancy (steps that don’t affect outcome)  </p></li>
<li><p>Sparsity (number of features changed)  </p></li>
<li><p>Proximity (l2 distance from original point)  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Ignoring user-specific constraints; reliance on universal cost functions; high redundancy; expensive categorical changes  </p></li>
<li><p><strong>Enablers:</strong> Explicit preference capture; flexible optimization accommodating hard/soft constraints; cost correction  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>The authors note most AR literature focuses on universal feasibility and cost minimization, sometimes adding diversity to hedge against unknown preferences. UP-AR directly incorporates known preferences, going beyond diversity-based approaches by personalizing the optimization process.</p>

<hr />

<h2>Summary</h2>

<p>This paper reframes <strong>actionability</strong> in ML recourse as inherently <strong>user-specific</strong> and <strong>preference-driven</strong>, arguing that what is “doable” varies across individuals with identical profiles. The proposed UP-AR framework operationalizes this view by eliciting explicit user preferences in three structured forms and embedding them into a gradient-based recourse generation process. By weighting feature changes according to these preferences and applying redundancy/cost correction, UP-AR improves alignment between suggested actions and user feasibility, outperforming existing methods on preference adherence and maintaining strong results on traditional AR metrics. This personalized approach enhances trust and explainability, and positions actionability as a function of both model mechanics and user-centered feasibility constraints.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 92 — Strong, explicit integration of user-centered definition of actionability with clear features and metrics.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Detailed algorithm and empirical workflow directly aimed at achieving actionability.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Actionability… is centered explicitly around individual preferences…” (p.1)  </p></li>
<li><p>“We start by enabling Alice to provide three types of user preferences… embed them into an optimization function…” (p.2)  </p></li>
<li><p>“Communicating in terms of preference scores… improves the explainability of a recourse generation mechanism” (p.1)  </p></li>
<li><p>“The proposed method minimizes the cost of a recourse weighted by Γᵢ for all actionable features” (p.3)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Ustun et al. (2019) — Actionable Recourse in Linear Classification  </p></li>
<li><p>Mahajan et al. (2019) — Local Feasibility  </p></li>
<li><p>De Toni et al. (2022) — Human-in-the-loop preference elicitation  </p></li>
<li><p>Wachter et al. (2017) — Counterfactual Explanations  </p></li>
<li><p>Poyiadzi et al. (2020) — FACE method</p></li>
</ul>
