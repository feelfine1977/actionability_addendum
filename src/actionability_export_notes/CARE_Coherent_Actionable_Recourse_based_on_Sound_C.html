<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: CARE: Coherent Actionable Recourse based on Sound Counterfactual Explanations  </p>

<p>Authors: Peyman Rasouli, Ingrid Chieh Yu  </p>

<p>DOI: https://doi.org/10.1145/nnnnnnn.nnnnnnn  </p>

<p>Year: 2021  </p>

<p>Publication Type: Conference  </p>

<p>Discipline/Domain: Computer Science / Artificial Intelligence  </p>

<p>Subdomain/Topic: Interpretable Machine Learning, Counterfactual Explanations, Actionable Recourse  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 95  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual with empirical evaluation  </p>

<p>Study Context: Model-agnostic counterfactual and recourse generation for classification and regression on tabular data  </p>

<p>Geographic/Institutional Context: University of Oslo, Norway  </p>

<p>Target Users/Stakeholders: End-users seeking actionable guidance from ML predictions; researchers in explainable AI  </p>

<p>Primary Contribution Type: Modular explanation framework (CARE) integrating model-level and user-level constraints for actionable recourse  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: No  </p>

<p>EX: Partial  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> CARE: Coherent Actionable Recourse based on Sound Counterfactual Explanations  </p>

<p><strong>Authors:</strong> Peyman Rasouli, Ingrid Chieh Yu  </p>

<p><strong>DOI:</strong> https://doi.org/10.1145/nnnnnnn.nnnnnnn  </p>

<p><strong>Year:</strong> 2021  </p>

<p><strong>Publication Type:</strong> Conference  </p>

<p><strong>Discipline/Domain:</strong> Computer Science / Artificial Intelligence  </p>

<p><strong>Subdomain/Topic:</strong> Interpretable Machine Learning, Counterfactual Explanations, Actionable Recourse  </p>

<p><strong>Contextual Background:</strong> The paper addresses the limitations of existing counterfactual explanation methods in machine learning by introducing a modular, multi-objective optimization framework (CARE) that generates actionable recourse grounded in realistic, coherent, and user-specific constraints.  </p>

<p><strong>Geographic/Institutional Context:</strong> University of Oslo, Norway  </p>

<p><strong>Target Users/Stakeholders:</strong> ML end-users needing recourse (e.g., loan applicants), explainable AI researchers  </p>

<p><strong>Primary Methodology:</strong> Conceptual with empirical evaluation  </p>

<p><strong>Primary Contribution Type:</strong> New modular framework for counterfactual and recourse generation</p>

<h2>General Summary of the Paper</h2>

<p>The authors propose CARE, a modular, model-agnostic explanation framework for generating actionable recourse based on sound counterfactual explanations. CARE integrates four modules—Validity, Soundness, Coherency, and Actionability—organized hierarchically to address both model-level and user/domain-level requirements. Validity ensures minimal changes for achieving the desired outcome; Soundness enforces proximity and connectedness to real data; Coherency preserves correlations between features; and Actionability incorporates user-defined constraints with importance weights. Using a multi-objective optimization approach (NSGA-III), CARE generates multiple, diverse counterfactuals for classification and regression tasks with mixed data types. Experiments on standard datasets show CARE’s superior performance in realism, coherency, and user compliance compared to DiCE and CFPrototype.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>Actionability is defined as satisfying global and local user/domain-specific preferences through constraints on features (e.g., immutable/mutable status, value ranges), enabling recourse that is realistic, feasible, and aligned with the user’s circumstances.  </p>

<blockquote>
  <p>“A counterfactual should satisfy some global and local preferences that are domain-specific and defined by the end-user.” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“An actionable explanation… takes into account the user’s preferences containing the name of mutable/immutable features, possible values, and their importance…” (p. 3)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Alignment with user-specified constraints (mutable/immutable features, allowed ranges/values)</p></li>
<li><p>Preservation of feature coherency under constraints</p></li>
<li><p>Feasibility in real-world terms (not recommending impossible changes)</p></li>
<li><p>Respecting constraint importance (prioritizing non-violable constraints)</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> CARE  </p></li>
<li><p><strong>Methods/Levers:</strong> Modular hierarchy with four modules; multi-objective optimization using NSGA-III  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. <strong>VALIDITY:</strong> Enforce minimal, sparse changes to achieve the desired outcome.  </p>

<p> 2. <strong>SOUNDNESS:</strong> Ensure proximity and connectedness to real, same-class data points.  </p>

<p> 3. <strong>COHERENCY:</strong> Use correlation models to preserve feature relationships.  </p>

<p> 4. <strong>ACTIONABILITY:</strong> Apply user-defined constraints with importance weighting.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Gower distance, Local Outlier Factor, HDBSCAN clustering, correlation measures (Pearson’s R, Cramer’s V), constraint satisfaction checks.  </p></li>
<li><p><strong>Implementation Context:</strong> Model-agnostic; applicable to tabular classification/regression; handles mixed features.  </p></li>
</ul>

<blockquote>
  <p>“We propose a constraint language… and the notion of constraint importance to weigh the constraints according to their importance for the user.” (p. 6)  </p>
</blockquote>

<blockquote>
  <p>“CARE… generates actionable recourse by fulfilling the mentioned desiderata through objective functions organized in a modular hierarchy…” (p. 2)</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — minimal, interpretable feature changes improve understandability (p. 3).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — proximity and connectedness ensure alignment with domain data (p. 2).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — coherent changes preserve real-world plausibility (p. 2–3).  </p></li>
<li><p><strong>TI (Timeliness):</strong> No — not explicitly addressed.  </p></li>
<li><p><strong>EX (Explainability):</strong> Partial — explanations are inherent but focus is on actionable counterfactuals, not full causal interpretability.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — constraints ensure user goals/preferences are respected (p. 6).  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Coherency, proximity, connectedness.</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Counterfactual explanations in XAI (Wachter et al., 2017)  </p></li>
<li><p>Proximity and connectedness metrics (Laugel et al., 2019)  </p></li>
<li><p>Actionable recourse frameworks (Ustun et al., 2019; Karimi et al., 2020)  </p></li>
<li><p>Multi-objective optimization (NSGA-III)</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Actionability cost (sum of violated constraint importance values)</p></li>
<li><p>Proximity and connectedness scores to assess plausibility</p></li>
<li><p>Coherency rate (preservation of feature correlations)</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Conflicting constraints; lack of coherent feature changes; artifacts in model space (p. 2–3).  </p></li>
<li><p><strong>Enablers:</strong> Modular structure allowing selective enforcement of properties; weighting of constraints by importance; correlation-based coherency preservation.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>The paper extends prior counterfactual explanation methods by integrating seldom-addressed properties (connectedness, coherency) with actionability. Unlike works that equate proximity with connectedness, CARE treats them as complementary. It also operationalizes coherency, which previous methods neglected.</p>

<h2>Summary</h2>

<p>CARE is a modular, model-agnostic framework for generating actionable recourse grounded in sound counterfactual explanations. It operationalizes four hierarchical properties—Validity, Soundness, Coherency, and Actionability—through specific objective functions optimized via NSGA-III. Validity ensures minimal, sparse changes; Soundness enforces proximity and connectedness to real, same-class data; Coherency preserves correlations between features; and Actionability integrates user-defined constraints with importance weighting. The approach applies to both classification and regression tasks with mixed-feature datasets. Empirical results show CARE outperforms DiCE and CFPrototype in producing coherent, realistic, and user-compliant recourse while maintaining diversity. The framework can serve as a benchmark for future actionable recourse research.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Provides explicit and nuanced definition of actionability with multiple properties tied to it; integrates underexplored aspects like coherency.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Fully details how to implement actionability in practice through constraints, optimization, and evaluation metrics.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“A counterfactual should satisfy some global and local preferences that are domain-specific and defined by the end-user.” (p. 2)  </p></li>
<li><p>“We introduce a novel notion of actionability that can cover various constraints and prioritize different preferences.” (p. 2)  </p></li>
<li><p>“Our proposed objective function… computes the actionability cost… according to the user’s preference.” (p. 6)  </p></li>
<li><p>“An actionable explanation… takes into account the user’s preferences containing the name of mutable/immutable features, possible values, and their importance…” (p. 3)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Ustun, Spangher, Liu (2019) — Actionable recourse in linear classification  </p></li>
<li><p>Karimi et al. (2020) — Algorithmic recourse  </p></li>
<li><p>Wachter et al. (2017) — Counterfactual explanations  </p></li>
<li><p>Laugel et al. (2019) — Proximity and connectedness in counterfactuals  </p></li>
<li><p>Dandl et al. (2020) — Multi-objective counterfactual explanations</p></li>
</ul>
