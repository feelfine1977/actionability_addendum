<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Why-Not Explainable Graph Recommender  </p>

<p>Authors: Herve-Madelein Attolou, Katerina Tzompanaki, Kostas Stefanidis, Dimitris Kotzinos  </p>

<p>DOI: 10.1109/ICDE60146.2024.00178  </p>

<p>Year: 2024  </p>

<p>Publication Type: Conference  </p>

<p>Discipline/Domain: Computer Science / Artificial Intelligence  </p>

<p>Subdomain/Topic: Explainable Recommender Systems, Counterfactual Explanations, Graph-based Recommendations  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 90  </p>

<p>Operationalization Score: 95  </p>

<p>Contains Definition of Actionability: Yes (implicit and explicit through actionable explanation design)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Partial  </p>

<p>Contains Framework/Model: Yes (EMiGRe)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual + Experimental Evaluation  </p>

<p>Study Context: Graph-based recommendation systems with user–item interaction data  </p>

<p>Geographic/Institutional Context: CY Cergy Paris University, Tampere University  </p>

<p>Target Users/Stakeholders: End-users of RS, system developers/debuggers  </p>

<p>Primary Contribution Type: Algorithm/Framework Proposal with Evaluation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: No  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Why-Not Explainable Graph Recommender</p>

<p><strong>Authors:</strong>  </p>

<p>Herve-Madelein Attolou, Katerina Tzompanaki, Kostas Stefanidis, Dimitris Kotzinos</p>

<p><strong>DOI:</strong>  </p>

<p>10.1109/ICDE60146.2024.00178</p>

<p><strong>Year:</strong>  </p>

<p>2024</p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference</p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Computer Science / Artificial Intelligence</p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable Recommender Systems, Counterfactual Explanations, Graph-based Recommendations</p>

<p><strong>Contextual Background:</strong>  </p>

<p>The work is situated in the area of explainable AI for recommendation systems, particularly in addressing <em>Why-Not</em> questions—cases where a user wants to know why a specific, expected item was not recommended. The authors focus on graph-based recommendation systems and adapt counterfactual explanation methods to provide actionable insights either as past actions to remove or new actions to take.</p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>CY Cergy Paris University (France), Tampere University (Finland)</p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>End-users seeking transparency; system developers for debugging and improving recommender performance</p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual framework development + algorithm design + experimental evaluation on Amazon product review data</p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Algorithm/Framework (EMiGRe) and empirical validation</p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This paper introduces <strong>EMiGRe</strong>, a framework for generating <em>Why-Not explanations</em> in graph-based recommender systems, explaining why a desired item was not the top recommendation. Unlike existing explainable RS approaches (e.g., PRINCE), EMiGRe targets missing recommendations and outputs <strong>actionable insights</strong> in the form of counterfactual changes to the user’s interaction graph—either edges to remove (past actions) or edges to add (potential actions). The framework defines a formal problem setting, proposes search strategies (Add/Remove modes) with heuristics (Incremental, Powerset, Exhaustive Comparison), and evaluates performance on a processed Amazon reviews dataset. Results show feasibility, differences in runtime, success rates, and explanation size across methods. The authors highlight challenges such as popular items, cold-start users, and the need for meta-explanations.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as providing explanations that <em>suggest concrete, feasible actions a user can take (or could have taken) to obtain the desired recommendation</em>. This goes beyond interpretability by prescribing <strong>specific edge additions/removals</strong> in the user–item graph.  </p>

<blockquote>
  <p>“We opt for a form of Counterfactual Explanations… proposing a possible world that could have led to the desired outcome” (p. 1)  </p>
</blockquote>

<blockquote>
  <p>“…provides… actionable insights on the source data and their interrelations” (p. 1)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Directly modifiable by the user (edges rooted at the user node)</p></li>
<li><p>Feasibility within privacy constraints (only user’s own actions)</p></li>
<li><p>Causally linked to producing the desired recommendation (must result in WNI being top-1)</p></li>
<li><p>Specificity (identifies exact edges to add or remove)</p></li>
<li><p>Adaptability to system constraints and user preferences</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> EMiGRe (Explainable Missing Graph Recommendation)</p></li>
<li><p><strong>Methods/Levers:</strong> Counterfactual graph modifications via edge addition (Add Mode) or removal (Remove Mode)</p></li>
<li><p><strong>Operational Steps / Workflow:</strong></p>

<p> 1. Define Why-Not item (WNI)</p>

<p> 2. Identify candidate edges (user-rooted) influencing WNI ranking using Personalized PageRank contributions</p>

<p> 3. Search for minimal modification set (Incremental, Powerset, Exhaustive Comparison)</p>

<p> 4. Validate candidate explanations against top-1 constraint</p></li>
<li><p><strong>Data &amp; Measures:</strong> Personalized PageRank scores, contribution metrics, runtime, success rate, explanation size</p></li>
<li><p><strong>Implementation Context:</strong> Post-hoc explanation for graph-based RS, tested on Amazon product reviews</p></li>
</ul>

<blockquote>
  <p>“…set of edges rooted at the user u node… to replace rec by WNI as the recommendation” (p. 5)  </p>
</blockquote>

<blockquote>
  <p>“…propose… missing pertinent edges to be added… or existing edges to be removed” (p. 5)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — explicitly identifies specific, understandable actions (edges)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — actions are user-specific and relevant to target item  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — constrained to actions the user can perform  </p></li>
<li><p><strong>TI (Timeliness):</strong> No — no explicit discussion of time sensitivity  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — method provides causal reasoning via counterfactuals  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — directly tied to achieving WNI recommendation  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Privacy-preserving scope</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Counterfactual explanations (AI interpretability literature)</p></li>
<li><p>Graph-based recommendation and Personalized PageRank</p></li>
<li><p>Why-Not questions in databases and ranking functions</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Success rate (ability to achieve WNI in top-1)</p></li>
<li><p>Size of explanation (fewer edges preferred)</p></li>
<li><p>Runtime efficiency (practicality of producing the explanation)</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong></p>

<p> - Cold start/low activity users (few modifiable edges)</p>

<p> - Highly popular competing items (structurally difficult to displace)</p>

<p> - Out-of-scope cases where only edge additions or removals are insufficient</p></li>
<li><p><strong>Enablers:</strong></p>

<p> - Availability of rich user–item interaction data</p>

<p> - Graph-based structure allowing edge-level manipulation</p>

<p> - Efficient PPR computation methods</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Extends explainable RS literature from <em>Why</em> to <em>Why-Not</em> scenarios, differing from PRINCE by:</p>

<ol>
<li><p>Focusing on missing recommendations</p></li>
<li><p>Providing both past-action and future-action explanations</p></li>
</ol>

<p>Builds on prior Why-Not work in databases and adapts it to graph RS with privacy-preserving constraints.</p>

<hr />

<h2>Summary</h2>

<p>The paper introduces EMiGRe, a novel framework for producing actionable Why-Not explanations in graph-based recommender systems. Actionability is defined through concrete, user-feasible modifications—adding or removing edges in the user–item graph—to achieve a specific desired recommendation. The framework operationalizes this via Personalized PageRank-based influence scoring and search strategies (Incremental, Powerset, Exhaustive Comparison), ensuring that the suggested actions directly cause the Why-Not item to become top-1. The approach is privacy-conscious, focusing only on the user’s own actions. Evaluation on Amazon review data shows varying trade-offs between runtime, explanation size, and success rates across methods. The work makes a strong conceptual and practical contribution to actionable explainability in RS, though timeliness is not addressed and popularity biases remain challenging.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 90 — Strong conceptualization of actionability with explicit operational features; slightly reduced due to lack of temporal considerations.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Detailed algorithms, heuristics, and evaluation directly tied to producing actionable outputs.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“We opt for a form of Counterfactual Explanations… proposing a possible world that could have led to the desired outcome” (p. 1)  </p></li>
<li><p>“…set of edges rooted at the user u node… to replace rec by WNI as the recommendation” (p. 5)  </p></li>
<li><p>“We provide more actionable explanations, by proposing not only existing actions… but also new actions” (p. 5)  </p></li>
<li><p>“This form of explanation provides user-comprehensible and actionable evidence of the trustworthiness of the system” (p. 4)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Ghazimatin et al. (2020) — PRINCE: Provider-side Interpretability with Counterfactual Explanations in RS  </p></li>
<li><p>Miller (2017, 2021) — Contrastive explanation theory  </p></li>
<li><p>Database and IR Why-Not literature (e.g., Bidoit et al. 2014, Chapman &amp; Jagadish 2009)</p></li>
</ul>
