<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP  </p>

<p>Authors: Pieter Delobelle, Giuseppe Attanasio, Debora Nozza, Su Lin Blodgett, Zeerak Talat  </p>

<p>DOI: 10.18653/v1/2024.emnlp-main.1315  </p>

<p>Year: 2024  </p>

<p>Publication Type: Journal/Conference Proceedings (EMNLP 2024)  </p>

<p>Discipline/Domain: Natural Language Processing, AI Ethics  </p>

<p>Subdomain/Topic: Bias evaluation metrics, actionability assessment  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Partial  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (desiderata-based framework)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Review and Conceptual Framework + Qualitative Analysis  </p>

<p>Study Context: NLP bias measures  </p>

<p>Geographic/Institutional Context: International (Authors from KU Leuven, Instituto de Telecomunicações Lisbon, Bocconi, Microsoft Research, MBZUAI)  </p>

<p>Target Users/Stakeholders: NLP researchers, metric developers, practitioners, policymakers, regulators  </p>

<p>Primary Contribution Type: Conceptual framework + systematic literature review  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Partial  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong> Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP  </p>

<p><strong>Authors:</strong> Pieter Delobelle, Giuseppe Attanasio, Debora Nozza, Su Lin Blodgett, Zeerak Talat  </p>

<p><strong>DOI:</strong> 10.18653/v1/2024.emnlp-main.1315  </p>

<p><strong>Year:</strong> 2024  </p>

<p><strong>Publication Type:</strong> Conference Proceedings (EMNLP 2024)  </p>

<p><strong>Discipline/Domain:</strong> Natural Language Processing, Responsible AI  </p>

<p><strong>Subdomain/Topic:</strong> Bias evaluation metrics, actionability, metric design  </p>

<p><strong>Contextual Background:</strong> The paper situates itself in the context of growing use of bias measures in NLP and the need to assess their practical utility (“actionability”). It introduces a conceptual framework with desiderata for making bias metrics actionable.  </p>

<p><strong>Geographic/Institutional Context:</strong> Belgium, Portugal, Italy, Canada, UAE  </p>

<p><strong>Target Users/Stakeholders:</strong> NLP researchers, fairness auditors, AI developers, policymakers, regulators, and potentially impacted communities.  </p>

<p><strong>Primary Methodology:</strong> Conceptual framework + systematic literature review (146 papers)  </p>

<p><strong>Primary Contribution Type:</strong> Definition and framework for “actionability” of bias measures + review-based evidence.</p>

<h2>General Summary of the Paper</h2>

<p>The authors define <em>actionability</em> in bias measures as the degree to which a measure’s results enable informed decision-making or intervention. They propose a set of desiderata (motivation, underlying bias construct, interval/ideal result, intended use, and reliability) drawn from responsible NLP, measurement modeling, and AI auditing literature. They apply these criteria in a systematic review of 146 NLP papers proposing bias measures, revealing widespread under-specification in intended use, theoretical constructs, and reliability assessment. The findings show that such omissions hinder the practical uptake of bias metrics in addressing harms. Recommendations include clear articulation of motivations, linking results to impacts, ensuring reliability assessment, and considering the actions afforded to different stakeholders.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>Actionability is “the degree to which a measure’s results enable decision-making or intervention” — results should help identify who is impacted, the scale and source of bias, and guide potential mitigations, safeguards, redesign, or policy changes. It is related to but distinct from validity, interpretability, transparency, and accountability, focusing on enabling meaningful interventions.</p>

<blockquote>
  <p>“Actionability refers to the degree to which a measure’s results enable decision-making or intervention…” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“…results from actionable bias measures should facilitate informed actions with respect to th</p>
</blockquote>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Actionable Knowledge Discovery and Delivery  </p>

<p>Authors: Longbing Cao  </p>

<p>DOI: 10.1002/widm.1044  </p>

<p>Year: 2012  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Data Mining / Knowledge Discovery  </p>

<p>Subdomain/Topic: Actionable Knowledge Discovery (AKD), Domain-Driven Data Mining  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 98  </p>

<p>Operationalization Score: 95  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual / Framework Development  </p>

<p>Study Context: Knowledge discovery in data mining, focusing on bridging the gap between academic outputs and business decision-making  </p>

<p>Geographic/Institutional Context: University of Technology, Sydney, Australia  </p>

<p>Target Users/Stakeholders: Data mining researchers, practitioners, business decision-makers  </p>

<p>Primary Contribution Type: Conceptual framework and methodological proposition (Domain-Driven Data Mining for AKD)  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Actionable Knowledge Discovery and Delivery  </p>

<p><strong>Authors:</strong>  </p>

<p>Longbing Cao  </p>

<p><strong>DOI:</strong>  </p>

<p>10.1002/widm.1044  </p>

<p><strong>Year:</strong>  </p>

<p>2012  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Data Mining / Knowledge Discovery  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Actionable Knowledge Discovery (AKD), Domain-Driven Data Mining  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the persistent gap between data mining research outputs and the needs of business decision-makers. It proposes a paradigm shift from traditional Knowledge Discovery in Databases (KDD) to Actionable Knowledge Discovery (AKD), emphasizing integration of business context, environmental constraints, and human expertise into data mining processes.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>University of Technology, Sydney, Australia  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Data mining researchers, practitioners, and business decision-makers in domains such as retail, healthcare, intrusion detection, and social network analysis.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual / Framework Development  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Proposal of a structured AKD methodology (Domain-Driven Data Mining) and operational frameworks.  </p>

<h2>General Summary of the Paper</h2>

<p>This paper critiques the inadequacies of traditional KDD, highlighting its inability to produce knowledge directly usable for decision-making. Cao introduces Actionable Knowledge Discovery (AKD) as a multidimensional process integrating problem, data, environment, model, decision, and optimization to produce results that satisfy both technical and business needs. The Domain-Driven Data Mining (D3M) framework operationalizes AKD by embedding domain, human, organizational, and social intelligence into mining processes. The paper outlines systematic features of actionability, proposes mathematical formulations for actionability measurement, and discusses architectures, tools, and delivery mechanisms to make mined knowledge truly usable in operational environments. It also presents examples (e.g., retail basket analysis) to illustrate how additional context transforms technically interesting patterns into actionable business insights.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is defined as “the power to work” — the quality of knowledge that enables direct, effective decision-making without further manipulation. It must satisfy both technical and business perspectives, combining objective and subjective measures, and be integrable into operational environments.  </p>

<blockquote>
  <p>“Actionable knowledge ‘is not only relevant to the world of practice, it is the knowledge that people use to create that world’.” (p. 149)  </p>
</blockquote>

<blockquote>
  <p>“Actionability means the power to work, which is an optimal outcome… through the best integration of six core dimensions.” (p. 154)  </p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Addresses the actual business problem, not just technical interest.  </p></li>
<li><p>Integrates environmental, organizational, and social factors.  </p></li>
<li><p>Is interpretable and explainable to end users.  </p></li>
<li><p>Is feasible and integrable into existing business processes.  </p></li>
<li><p>Produces measurable impact toward business goals.  </p></li>
<li><p>Satisfies both technical and business interestingness thresholds.  </p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Domain-Driven Data Mining (D3M), AKD Framework  </p></li>
<li><p><strong>Methods/Levers:</strong> Integration of domain, human, network, and social intelligence; postanalysis; unified and combined interestingness metrics; interactive and parallel mining; closed-loop refinement.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Problem definition → Data understanding → Environmental/context modeling → Model building → Decision mapping → Optimization → Deliverable transformation into business rules.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Technical and business actionability metrics (objective and subjective), thresholds for technical interestingness (ti) and business interestingness (bi).  </p></li>
<li><p><strong>Implementation Context:</strong> Retail, healthcare, intrusion detection, web analytics, organizational decision-making.  </p></li>
</ul>

<blockquote>
  <p>“AKD is a six-dimension-based optimization process: problem, data, environment, model, decision, optimization.” (p. 152)  </p>
</blockquote>

<blockquote>
  <p>“For a pattern p… Act(p) can be further measured in terms of technical actionability and business actionability.” (p. 153)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — interpretability and understandability emphasized.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — explicit requirement to integrate environmental/business context.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — must be directly usable without major rework.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — addressed indirectly via adaptability to dynamic data and environments.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — must be interpretable in business language and logic.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — deliverables must meet business expectations and objectives.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Autonomy, deliverability, dependability, repeatability, trust, semantics.</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li>System sciences, cybernetics, complex systems theory, metasynthesis, agent-based systems, ubiquitous intelligence integration.</li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Technical interestingness (ti) and business interestingness (bi) with defined thresholds.  </p></li>
<li><p>Objective/subjective measures from technical and business perspectives.  </p></li>
<li><p>Evaluation of business impact (e.g., revenue, efficiency).</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Academic–business goal misalignment; oversimplification of problems; lack of integration of context; passive knowledge formats.  </p></li>
<li><p><strong>Enablers:</strong> Involving domain experts; modeling environmental factors; unified interestingness measures; delivering in business-ready formats.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions itself as extending prior notions of actionable rules and interestingness by embedding them in a systemic, multi-intelligence framework (D3M). It draws on and critiques both technical-focused and business-focused prior work, aiming to balance the two.</p>

<h2>Summary</h2>

<p>Cao (2012) advances the concept of Actionable Knowledge Discovery (AKD) as a shift from conventional KDD to decision-ready insights. Actionability is framed as the “power to work,” requiring integration of problem, data, environment, model, decision, and optimization dimensions, and satisfaction of both technical and business needs. The Domain-Driven Data Mining framework operationalizes AKD through context-rich, human-involved, and adaptive processes. Systematic features include clarity, contextual relevance, feasibility, explainability, and goal alignment, supported by quantifiable actionability metrics. The paper offers architectures, process models, and illustrative cases to demonstrate how embedding domain, human, and social intelligence can transform technically interesting patterns into operationally valuable knowledge.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 98 — Provides a rich, explicit conceptualization of actionability, systematic features, and detailed framework.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Offers a complete methodology with measurable metrics, process models, and examples for achieving actionability.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Actionable knowledge ‘is not only relevant to the world of practice…’” (p. 149)  </p></li>
<li><p>“Actionability means the power to work… through the best integration of six core dimensions.” (p. 154)  </p></li>
<li><p>“AKD is a six-dimension-based optimization process: problem, data, environment, model, decision, optimization.” (p. 152)  </p></li>
<li><p>“Deliverables… must be easily interpretable, convertible into business rules, and linked to decision-making systems.” (p. 151)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Argyris (1993, 1996) on actionable knowledge in organizational contexts.  </p></li>
<li><p>He et al. (2005) on actionable knowledge in data mining.  </p></li>
<li><p>Ras &amp; Wieczorkowska (2000) on action rules.  </p></li>
<li><p>Cao &amp; Zhang (2007, 2010) on knowledge actionability and domain-driven data mining.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Geopolitical Forecasting and Actionable Intelligence</p>

<p>Authors: Ian S. Lustick</p>

<p>DOI: 10.1080/00396338.2022.2032959</p>

<p>Year: 2022</p>

<p>Publication Type: Journal</p>

<p>Discipline/Domain: Political Science / International Relations</p>

<p>Subdomain/Topic: Geopolitical forecasting, intelligence analysis, decision support</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 88</p>

<p>Operationalization Score: 70</p>

<p>Contains Definition of Actionability: Yes (implicit, tied to “actionable intelligence”)</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Yes</p>

<p>Contains Interpretability: Partial</p>

<p>Contains Framework/Model: No formal named framework, but conceptual approach</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Conceptual / Analytical Essay</p>

<p>Study Context: Intelligence analysis for U.S. foreign policy and national security</p>

<p>Geographic/Institutional Context: Primarily U.S. intelligence community</p>

<p>Target Users/Stakeholders: Policymakers, intelligence analysts, national security officials</p>

<p>Primary Contribution Type: Conceptual framework for linking forecasting validity/verification to actionability</p>

<p>CL: Yes</p>

<p>CR: Yes</p>

<p>FE: Partial</p>

<p>TI: No</p>

<p>EX: Yes</p>

<p>GA: Partial</p>

<p>Reason if Not Eligible: N/A</p>

<!--META_END-->

<p><strong>Title:</strong> Geopolitical Forecasting and Actionable Intelligence  </p>

<p><strong>Authors:</strong> Ian S. Lustick  </p>

<p><strong>DOI:</strong> 10.1080/00396338.2022.2032959  </p>

<p><strong>Year:</strong> 2022  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Political Science / International Relations  </p>

<p><strong>Subdomain/Topic:</strong> Geopolitical forecasting, intelligence analysis, decision support  </p>

<p><strong>Contextual Background:</strong> Discusses the evolution of U.S. intelligence forecasting from WWII to the present, focusing on the challenges of making forecasts that are both valid and <em>actionable</em> for policy and decision-making.  </p>

<p><strong>Geographic/Institutional Context:</strong> U.S. intelligence community and policymaking environment  </p>

<p><strong>Target Users/Stakeholders:</strong> Policymakers, intelligence analysts, decision-support tool developers  </p>

<p><strong>Primary Methodology:</strong> Conceptual / Analytical Essay  </p>

<p><strong>Primary Contribution Type:</strong> Conceptual linkage between validation, verification, and actionable intelligence  </p>

<h2>General Summary of the Paper</h2>

<p>The paper examines why geopolitical forecasting, despite technological advances, often fails to produce actionable intelligence for policymakers. Lustick distinguishes between forecasts used for general discussion versus those directly informing policy decisions, arguing that the latter require rigorous <em>validation</em> (accuracy, precision, reliability) and <em>verification</em> (causal explanation). He critiques the dominance of engineers and brute-force empiricism in intelligence R&amp;D, advocating for greater integration of political science, sociology, economics, and cultural expertise. Actionable intelligence, he asserts, must answer not only “what, where, when” but also “why” and “how,” enabling policymakers to select effective interventions. Without this integration of substantive theory, even technically accurate forecasts will have limited practical utility.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is implicitly defined as the capacity of intelligence forecasts to inform and guide concrete policy or operational decisions — requiring both empirical validity and causal explanation. Actionable intelligence is contrasted with forecasts that are merely interesting or discussion-enhancing.</p>

<blockquote>
  <p>“If forecasts are used as actual inputs into a policy- or decision-making process, they do need to be accurate, precise and reliable.” (p. 53)  </p>
</blockquote>

<blockquote>
  <p>“Only models capable of answering why and how questions, not just what, where and when questions, will fit the bill.” (p. 55)  </p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Empirical validity (accuracy, precision, reliability of forecasts)  </p></li>
<li><p>Verification (causal traceability and theoretical grounding)  </p></li>
<li><p>Ability to answer “why” and “how” questions to guide action  </p></li>
<li><p>Integration of domain-specific cultural, political, and economic knowledge  </p></li>
<li><p>Relevance to decision-makers’ context and needs</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Not formalized; dual requirement of validation + verification  </p></li>
<li><p><strong>Methods/Levers:</strong> Brier scoring for validation; causal modeling for verification; integration of substantive theory into computational forecasting tools  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Validate forecasts statistically (probability conformity to outcomes).  </p>

<p> 2. Verify causal soundness of models.  </p>

<p> 3. Integrate social science expertise with computational modeling.  </p>

<p> 4. Tailor models to specific geographic/cultural contexts.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Brier score; qualitative causal traceability  </p></li>
<li><p><strong>Implementation Context:</strong> U.S. intelligence community forecasting for policy use  </p></li>
</ul>

<blockquote>
  <p>“Only streams of outcomes that exhibit the forecasted probability can corroborate the validity of the forecast and the techniques used to produce it.” (p. 54)  </p>
</blockquote>

<blockquote>
  <p>“If outcomes cannot be traced to particular combinations of antecedent variables… decision-makers cannot know how to make use of the forecast.” (p. 54)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — forecasts must be precise and interpretable to decision-makers.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — tied to specific geopolitical/cultural contexts.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Partial — implies that forecasts should inform feasible actions, but not fully elaborated.  </p></li>
<li><p><strong>TI (Timeliness):</strong> No explicit emphasis.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — forecasts must answer “why” and “how” to be useful.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — linked to enhancing desired outcomes and avoiding undesirable ones.  </p></li>
<li><p><strong>Other Dimensions:</strong> Validation, Verification.</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Distinction between validation and verification from modeling literature.  </p></li>
<li><p>Critique of brute-force empiricism in forecasting (Lustick &amp; Tetlock 2021).  </p></li>
<li><p>Decision-support theory in intelligence studies.</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Brier scoring for forecast validity.  </p></li>
<li><p>Presence of causal explanations linking variables to outcomes.  </p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Dominance of engineers over social scientists in intelligence R&D; overreliance on machine learning without substantive theory; lack of integration of domain-specific knowledge.  </p></li>
<li><p><strong>Enablers:</strong> Combining social science expertise with computing power; rigorous validation and verification; tailoring models to cultural/geopolitical specifics.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions itself against purely technical, data-driven forecasting approaches, emphasizing the need for theory-informed, causally explainable models in line with social science insights.</p>

<h2>Summary</h2>

<p>Lustick’s article argues that for geopolitical forecasts to yield <em>actionable intelligence</em>, they must satisfy two conditions: <strong>validation</strong> (accuracy, precision, and reliability) and <strong>verification</strong> (causal explanation and theoretical grounding). Actionability here means enabling decision-makers to understand not just what might happen, but why and how, so they can act to shape outcomes. Current intelligence R&amp;D, dominated by engineering and brute-force empiricism, fails to integrate enough substantive expertise from social sciences and cultural studies. The author proposes combining computational power with deep domain knowledge, using tools like Brier scoring for statistical validation and theoretical modeling for causal verification. Without such integration, even technologically sophisticated forecasting will remain of limited practical utility.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 88 — Strong implicit definition and clear features linked to actionability; robust conceptual discussion of requirements.  </p></li>
<li><p><strong>Operationalization Score:</strong> 70 — Outlines a clear dual-process approach (validation + verification) and suggests methods, but lacks a detailed step-by-step implementation framework.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“If forecasts are used as actual inputs into a policy- or decision-making process, they do need to be accurate, precise and reliable.” (p. 53)  </p></li>
<li><p>“Only models capable of answering why and how questions, not just what, where and when questions, will fit the bill.” (p. 55)  </p></li>
<li><p>“Only streams of outcomes that exhibit the forecasted probability can corroborate the validity of the forecast and the techniques used to produce it.” (p. 54)  </p></li>
<li><p>“If outcomes cannot be traced to particular combinations of antecedent variables… decision-makers cannot know how to make use of the forecast.” (p. 54)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Lustick &amp; Tetlock (2021), <em>The Simulation Manifesto</em>  </p></li>
<li><p>O’Brien (2010), <em>Crisis Early Warning and Decision Support</em>  </p></li>
<li><p>Johnston (2005), <em>Analytic Culture in the U.S. Intelligence Community</em>  </p></li>
<li><p>Halberstam (1972), <em>The Best and the Brightest</em></p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Knowledge and Policy: research – information – intervention  </p>

<p>Authors: Ingrid Gogolin, Edwin Keiner, Gita Steiner-Khamsi, Jenny Ozga, Lyn Yates  </p>

<p>DOI: n/a  </p>

<p>Year: 2007  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Educational Policy, Educational Research  </p>

<p>Subdomain/Topic: Policy Analysis, Research Governance, Knowledge Transfer  </p>

<p>Eligibility: Yes  </p>

<p>Overall Relevance Score: 85  </p>

<p>Operationalization Score: 70  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual and Review  </p>

<p>Study Context: Education Policy, International Comparisons  </p>

<p>Geographic/Institutional Context: International (Switzerland, UK, Germany, USA, Australia)  </p>

<p>Target Users/Stakeholders: Educational Policymakers, Researchers, Educators  </p>

<p>Primary Contribution Type: Conceptual Exploration, Policy Implications  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> Knowledge and Policy: research – information – intervention  </p>

<p><strong>Authors:</strong> Ingrid Gogolin, Edwin Keiner, Gita Steiner-Khamsi, Jenny Ozga, Lyn Yates  </p>

<p><strong>DOI:</strong> n/a  </p>

<p><strong>Year:</strong> 2007  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Educational Policy, Educational Research  </p>

<p><strong>Subdomain/Topic:</strong> Policy Analysis, Research Governance, Knowledge Transfer  </p>

<p><strong>Contextual Background:</strong> The paper discusses the evolving relationships between research, information, and policy interventions in the context of educational research. The authors explore how educational research is influenced by political agendas, particularly focusing on the role of international knowledge banks, evidence-based policy, and research governance. They address the implications for the knowledge production process and the role of researchers in shaping educational reforms.  </p>

<p><strong>Geographic/Institutional Context:</strong> The paper draws on international perspectives, including the UK, Germany, Switzerland, the USA, and Australia.  </p>

<p><strong>Target Users/Stakeholders:</strong> Educational policymakers, researchers, practitioners in education  </p>

<p><strong>Primary Methodology:</strong> Conceptual analysis, review of educational policy trends  </p>

<p><strong>Primary Contribution Type:</strong> Conceptual exploration of policy-research interactions and implications for the educational field  </p>

<h2>General Summary of the Paper</h2>

<p>This paper explores the interplay between research, information, and policy interventions in educational systems, emphasizing how knowledge is shaped by political agendas. The authors critically examine international knowledge banks and their role in shaping policy through data-driven comparisons, such as PISA and TIMSS. The paper also discusses how research governance has evolved, with a shift toward evidence-based policy-making, and explores the tensions between academic autonomy and policy influence in the production of educational knowledge.  </p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<p>Reason if Not Eligible: n/a  </p>

<h2>How Actionability is Understood</h2>

<p>The authors define actionability in the context of research-policy relationships as the process by which research outputs are transformed into policy decisions. This involves translating research findings into actionable knowledge that policymakers can use to drive reforms, often shaped by political agendas and the externalization of policy needs.  </p>

<blockquote>
  <p>“Research knowledge is not just a tool for solving problems but becomes a resource for governance, facilitating policy actions” (p. 13).  </p>
</blockquote>

<blockquote>
  <p>“Policy-making increasingly relies on research that is ‘actionable,’ a process that is mediated by political needs and external pressures” (p. 14).</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<p>The authors argue that for research to be actionable, it must:</p>

<ul>
<li><p>Be clearly translated into policy-relevant knowledge  </p></li>
<li><p>Align with political and economic needs, particularly in the context of global benchmarking and educational reforms  </p></li>
<li><p>Be produced with a view toward achieving practical outcomes, often under the constraints of governance frameworks  </p></li>
</ul>

<blockquote>
  <p>“Actionable knowledge must be framed to meet both the practical needs of policymakers and the strategic goals of educational reform” (p. 16).  </p>
</blockquote>

<blockquote>
  <p>“The shift toward evidence-based policy-making demands that research be oriented toward measurable outcomes” (p. 17).</p>
</blockquote>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<p>The paper proposes that actionability is achieved through mechanisms like international knowledge banks and the growing reliance on data-driven policy interventions. The authors highlight the role of global benchmarking (e.g., OECD’s PISA) in creating externally validated measures of educational performance that become the basis for policy interventions.  </p>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Evidence-based Policy, Knowledge Transfer  </p></li>
<li><p><strong>Methods/Levers:</strong> International knowledge banks, benchmarking, cross-national comparisons  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Researchers produce data-driven reports that become policy tools; these tools are used by governments to justify or guide educational reforms  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Standardized assessments (PISA, TIMSS), national rankings, educational benchmarks  </p></li>
<li><p><strong>Implementation Context:</strong> Primarily in global educational reform initiatives, influenced by international organizations like the World Bank and OECD  </p></li>
</ul>

<blockquote>
  <p>“International comparisons, like those of PISA and TIMSS, provide the evidence that policymakers need to justify reform and secure funding” (p. 19).  </p>
</blockquote>

<blockquote>
  <p>“The creation of knowledge banks is a deliberate attempt to shape the policy landscape by providing evidence that fits political agendas” (p. 20).</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – Actionable knowledge must be clear and understandable to policymakers.  </p>

<p> &gt; “Actionable research must be accessible to those making policy decisions, as clarity is essential to ensure it is implemented effectively” (p. 15).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – Knowledge must be relevant to the specific political and educational contexts.  </p>

<p> &gt; “Research needs to be contextualized to fit the political, social, and economic conditions of the country” (p. 16).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – Research should be practical and feasible to implement in policy.  </p>

<p> &gt; “Feasibility is a key attribute for research to be considered actionable, particularly when framed within the realities of national governance” (p. 14).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes – Actionability also depends on the timeliness of the research in relation to policy needs.  </p>

<p> &gt; “Timely interventions are necessary to ensure that research can be translated into action during critical policy windows” (p. 17).  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – The ability to explain research findings in a way that informs decision-making is vital for actionability.  </p>

<p> &gt; “The explanation of research results in an understandable way is critical for influencing policy decisions” (p. 14).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – Research must align with the goals and agendas of policymakers.  </p>

<p> &gt; “Alignment with national or international educational goals is crucial for ensuring that research is actionable in policy” (p. 19).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<p>The paper draws on the concept of Mode 1 and Mode 2 knowledge production (Gibbons et al., 1994), highlighting a shift from traditional, discipline-based research to a more collaborative, policy-oriented approach that involves multiple stakeholders, including government bodies and international organizations.  </p>

<blockquote>
  <p>“Mode 2 knowledge production is marked by its transdisciplinary approach, involving collaboration between researchers, policymakers, and practitioners” (p. 13).</p>
</blockquote>

<h2>Indicators or Metrics for Actionability</h2>

<p>The paper implies that actionable knowledge is measured through indicators such as educational rankings, standardized test results, and performance benchmarks. These metrics are used to guide policy decisions and justify educational reforms.  </p>

<blockquote>
  <p>“International rankings and benchmarks act as primary indicators of the quality and impact of educational systems” (p. 18).</p>
</blockquote>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><strong>Barriers:</strong> Political agendas that shape the research questions and the framing of evidence, resistance to non-quantitative research, lack of researcher engagement with policy needs.  </li>
</ul>

<blockquote>
  <p>“Political pressures can skew the research agenda, prioritizing data that aligns with predetermined policy goals” (p. 14).  </p>
</blockquote>

<ul>
<li><strong>Enablers:</strong> Collaboration between researchers and policymakers, the rise of evidence-based policy-making, international comparative studies.  </li>
</ul>

<blockquote>
  <p>“The growth of knowledge banks and international policy networks has enhanced the ability to translate research into action” (p. 16).</p>
</blockquote>

<h2>Relation to Existing Literature</h2>

<p>The paper critiques the linear model of research-to-policy transfer, which assumes a direct link from research to practical intervention. It aligns with literature on policy transfer and knowledge governance, emphasizing the complexity of how knowledge is adapted and used in policy contexts.  </p>

<blockquote>
  <p>“The relationship between research and policy is more complex than the simple transmission of knowledge to action; it is shaped by political and economic forces” (p. 17).</p>
</blockquote>

<h2>Summary</h2>

<p>This paper critically examines the relationship between research, information, and policy interventions in education, focusing on how research is framed and transformed into actionable knowledge that informs policy. The authors highlight the growing reliance on international knowledge banks and benchmarking systems as key tools for shaping educational reforms, arguing that actionability in education research is increasingly driven by political agendas and the need for quantifiable outcomes. They explore how this shift affects researchers' roles and the types of knowledge that are valued in policy-making contexts.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 85 – The paper offers valuable insights into the complexities of making research actionable in the context of educational policy and governance, providing both theoretical and practical perspectives.  </p></li>
<li><p><strong>Operationalization Score:</strong> 70 – While the paper discusses mechanisms for achieving actionability, it lacks detailed operational steps or frameworks that could be directly applied in practice.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li>“Research knowledge is not just a tool for solvin</li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Actionable Insights in Urban Multivariate Time-series  </p>

<p>Authors: Anika Tabassum, Supriya Chinthavali, Varisara Tansakul, B. Aditya Prakash  </p>

<p>DOI: https://doi.org/10.1145/3459637.3482410  </p>

<p>Year: 2021  </p>

<p>Publication Type: Conference (ACM CIKM ’21)  </p>

<p>Discipline/Domain: Computer Science / Urban Analytics  </p>

<p>Subdomain/Topic: Multivariate Time-series Segmentation, Explainability, Rationalization  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 90  </p>

<p>Operationalization Score: 95  </p>

<p>Contains Definition of Actionability: Yes (implicit, formalized in RaTSS problem)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (RaTSS, Find-RaTSS)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual + Quantitative Evaluation (Algorithm design, experiments)  </p>

<p>Study Context: Urban analytics applications in disasters, public health, epidemiology, and general high-dimensional datasets  </p>

<p>Geographic/Institutional Context: US (Oak Ridge National Laboratory, Virginia Tech, Georgia Tech)  </p>

<p>Target Users/Stakeholders: Urban domain experts (emergency management authorities, epidemiologists, planners)  </p>

<p>Primary Contribution Type: Novel problem formulation + algorithmic solution  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Actionable Insights in Urban Multivariate Time-series  </p>

<p><strong>Authors:</strong>  </p>

<p>Anika Tabassum, Supriya Chinthavali, Varisara Tansakul, B. Aditya Prakash  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1145/3459637.3482410  </p>

<p><strong>Year:</strong>  </p>

<p>2021  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference Paper (CIKM 2021)  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Computer Science / Data Mining  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Urban Analytics, Time-series Segmentation, Explainable AI  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the difficulty urban domain experts face in extracting <strong>actionable</strong> time-series of interest (TOIs) from complex multivariate time-series segmentation outputs. Contexts include hurricanes, pandemics, epidemiology, and web analytics, where quick identification of non-obvious but important series can direct interventions.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>US, collaboration between Virginia Tech, Oak Ridge National Laboratory, Georgia Tech.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Emergency management authorities, epidemiologists, public health planners, infrastructure operators.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual framework + Algorithm design (RaTSS &amp; Find-RaTSS) + Empirical evaluation on synthetic, real, and domain-specific datasets.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Novel problem definition + algorithm to produce actionable insights for any black-box segmentation algorithm.  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The authors introduce <strong>RaTSS</strong> (Rationalization for Time-series Segmentation), a framework for identifying <em>actionable</em> Time-series of Interest (TOIs) from multivariate segmentation results in urban applications. They argue that existing segmentation methods produce accurate phase boundaries but lack human-friendly, decision-ready outputs. RaTSS treats segmentation as a black box and derives importance weights for each series at each cutpoint via a graph optimization problem over a <em>segment graph</em>. They implement <strong>Find-RaTSS</strong>, an algorithm to solve this problem efficiently for large datasets. Empirical evaluation on synthetic, real-world, and domain-specific urban datasets (hurricanes, COVID-19, epidemiology, Wikipedia traffic) shows Find-RaTSS consistently outperforms baselines in identifying ground-truth TOIs and generates non-obvious but operationally relevant insights for decision-making.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as identifying TOIs whose changes across segmentation cutpoints are <strong>most relevant to operational decisions</strong> in the target domain — for example, counties to prioritize for hurricane recovery or states likely affected by policy interventions.  </p>

<blockquote>
  <p>“… actionable insights, i.e., which time-series/counties are the most important with respect to an event, they can send personnel to fix damage and alert local authorities to reduce further loss” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“… human-friendly and actionable TOIs (rationalizations) for the urban experts across the associated events” (p. 2)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>High relative importance across a cutpoint (based on learned weights)  </p></li>
<li><p>Potential to influence direct interventions or decisions  </p></li>
<li><p>Inclusion of <em>non-obvious</em> series not apparent from visual inspection  </p></li>
<li><p>Contextual linkage to events (e.g., weather, policy changes, epidemiological outbreaks)  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> RaTSS (problem), Find-RaTSS (algorithm)  </p></li>
<li><p><strong>Methods/Levers:</strong> Segment graph representation; optimization of global latent weights (α) to maximize separation of chosen segmentation path over all alternatives.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Build segment graph for multivariate time series.  </p>

<p> 2. Calculate edge weights using basic statistical features across segments.  </p>

<p> 3. Compute Δπ (difference in path lengths between chosen and alternative segmentations).  </p>

<p> 4. Optimize α under sparsity and norm constraints.  </p>

<p> 5. Derive r_j (importance weights) per cutpoint and select top TOIs.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Mean, variance, min, max features per segment; importance weights; F1-scores for evaluation.  </p></li>
<li><p><strong>Implementation Context:</strong> Works for any black-box segmentation algorithm, regardless of internal mechanics.  </p></li>
</ul>

<blockquote>
  <p>“We propose an algorithm Find-RaTSS to automatically capture the TOIs in a way that is flexible and works for any black-box segmentation that a DE may use” (p. 2)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — output is simplified, interpretable list of TOIs with weights.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — TOIs tied to specific events and domain context.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — outputs can be operationalized into concrete actions (e.g., send crews, investigate policy).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — method processes historical data; potential for near real-time with optimization.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — weight-based rationalizations with clear link to cutpoints.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — TOIs are selected to match decision-makers’ objectives.  </p></li>
<li><p><strong>Other Dimensions:</strong> Non-obviousness (ability to surface hidden but important cases).  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Graph-based representation of segmentation paths (segment graph)  </p></li>
<li><p>Optimization under sparsity and norm constraints  </p></li>
<li><p>Basic statistical change detection (mean, variance, min, max features)  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Importance weight (r_j) per series at each cutpoint  </p></li>
<li><p>F1-score comparing predicted TOIs to ground truth  </p></li>
<li><p>Fraction of total rationalization weight captured by top-k TOIs  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong>  </p>

<p> - If segmentation is meaningless (e.g., constant series), rationalizations may not be meaningful.  </p>

<p> - Some actionable groups may consist of combinations of series, not individual ones (not yet implemented).  </p></li>
<li><p><strong>Enablers:</strong>  </p>

<p> - Algorithm’s independence from segmentation model details  </p>

<p> - Works with any multivariate time-series data  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Positions itself as the first method to identify actionable TOIs for any black-box segmentation. Builds on work in urban analytics, change point detection, and interpretable AI, but moves beyond model-internal explanations to produce human-friendly rationalizations.</p>

<hr />

<h2>Summary</h2>

<p>This paper presents RaTSS, a formal problem framing for deriving actionable Time-series of Interest (TOIs) from multivariate time-series segmentation, and Find-RaTSS, an algorithm that computes these TOIs without requiring access to the internal workings of the segmentation algorithm. By representing all possible segmentations in a segment graph and optimizing a global weight vector to distinguish the chosen segmentation from alternatives, the method outputs ranked TOIs per cutpoint. The approach is validated on synthetic, real-world, and domain-specific datasets, outperforming baselines and surfacing non-obvious but operationally relevant insights, such as unexpected counties affected during hurricanes or states with COVID-19 interventions. Actionability here is operationalized as relevance to decision-making, contextual interpretability, feasibility, and alignment with domain objectives.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 90 — Strong implicit and operational definition of actionability, clear identification of relevant features.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Detailed algorithm and workflow for achieving actionable outputs, tested in varied contexts.  </p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“... actionable insights, i.e., which time-series/counties are the most important with respect to an event…” (p. 2)  </p></li>
<li><p>“We introduce and formalize a novel problem Rationalization for Time-series Segmentations (RaTSS)…” (p. 2)  </p></li>
<li><p>“r<em>j = |α ⊙ w</em>ijk|” (p. 4)  </p></li>
<li><p>“Remark 1: … when the time-series is constant, then rationalizations (TOIs) found by RaTSS may not be meaningful” (p. 4)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>[23] Cut-n-Reveal: Time Series Segmentations with Explanations — related explanation approach but model-dependent.  </p></li>
<li><p>[6] ORNL EARSS — situational awareness in disaster response.  </p></li>
<li><p>[19] Dynammo — handling missing values in time-series.  </p></li>
<li><p>[21] Autoplait — segmentation with HMMs.  </p></li>
<li><p>[12] TICC — segmentation with multilayer Markov Random Fields.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Communication of Actionable Information  </p>

<p>Authors: Giles W. Boland, Richard Duszak Jr, Paul A. Larson  </p>

<p>DOI: http://dx.doi.org/10.1016/j.jacr.2014.08.003  </p>

<p>Year: 2014  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Radiology / Medical Imaging  </p>

<p>Subdomain/Topic: Communication of actionable radiology findings  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 83  </p>

<p>Operationalization Score: 75  </p>

<p>Contains Definition of Actionability: Yes (implicit and partially explicit)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: No  </p>

<p>Contains Interpretability: No  </p>

<p>Contains Framework/Model: Yes (ACR categories)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual / Practice guidance  </p>

<p>Study Context: Communication of actionable radiology information in clinical workflows  </p>

<p>Geographic/Institutional Context: U.S. radiology practices, hospitals, and teleradiology services  </p>

<p>Target Users/Stakeholders: Radiologists, referring physicians, patients, hospital administrators  </p>

<p>Primary Contribution Type: Practice recommendations and framework adaptation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: No  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Communication of Actionable Information  </p>

<p><strong>Authors:</strong>  </p>

<p>Giles W. Boland, Richard Duszak Jr, Paul A. Larson  </p>

<p><strong>DOI:</strong>  </p>

<p>http://dx.doi.org/10.1016/j.jacr.2014.08.003  </p>

<p><strong>Year:</strong>  </p>

<p>2014  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Radiology / Medical Imaging  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Communication of actionable radiology findings  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the challenge of ensuring that radiology reports—especially those containing actionable findings—are effectively and efficiently communicated to relevant stakeholders. It emphasizes operational workflows, integration of information systems, and policies for timely reporting.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>U.S. radiology practices, including academic centers, private groups, and teleradiology services.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Radiologists, referring physicians, patients, hospital administrators.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual / Practice guidance.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Practice recommendations and operational framework.</p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This article outlines the critical role radiologists play in not only producing timely, meaningful, and actionable reports, but also ensuring these reports are communicated effectively to the appropriate stakeholders. It frames this communication as the final link in the “imaging value chain,” which is often the weakest. The authors highlight barriers such as fragmented IT systems, lack of standardized critical findings policies, and challenges in teleradiology. They reference the American College of Radiology (ACR) framework that categorizes findings based on urgency, offering clear timelines for communication. Proposed solutions include integrated electronic systems, closed-loop communication protocols, embedding radiologists in clinical teams, and providing patients direct access to their reports. The paper stresses that radiologists operate in the information business, and actionable value is only realized when reports are both delivered and understood.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<hr />

<h2>How Actionability is Understood</h2>

<p>The paper defines actionable information in radiology as findings that, once communicated, can influence patient management and outcomes, requiring delivery that is timely, clear, and directed to the correct stakeholders.</p>

<blockquote>
  <p>“A report creates little value until it is delivered, read, and correctly understood… Only then can information be used to have an impact on patient outcomes.” (p. 1)  </p>
</blockquote>

<blockquote>
  <p>“Effective communication of actionable information” is described as the final step in the imaging value chain, essential for delivering appropriateness, quality, safety, efficiency, and patient satisfaction. (p. 1)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Clear identification of findings with clinical significance.  </p></li>
<li><p>Timeliness in delivering the report relative to urgency.  </p></li>
<li><p>Delivery to the right recipient(s) with confirmation.  </p></li>
<li><p>Documentation of communication.  </p></li>
<li><p>Use of standardized categories (ACR Category 1–3) tied to urgency.</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> ACR Actionable Reporting categories.  </p></li>
<li><p><strong>Methods/Levers:</strong> Standardized timelines for Category 1 (minutes), Category 2 (hours), Category 3 (days); integrated IT and EMR systems; closed-loop communication; embedding radiologists in clinical teams; direct patient portals.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Interpret findings → classify urgency → communicate via appropriate channel (verbal, electronic alert, direct message) → document delivery and recipient acknowledgment.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Time from report finalization to communication; confirmation logs; audit trails.  </p></li>
<li><p><strong>Implementation Context:</strong> Hospital radiology, teleradiology, academic centers, multidisciplinary clinics.  </p></li>
</ul>

<blockquote>
  <p>“Category-1 findings require communication within minutes, usually by direct verbal communication…” (p. 1)  </p>
</blockquote>

<blockquote>
  <p>“Electronic text and e-mail alerts… confirm whether referrers have reviewed such reports… close the communication loop…” (p. 2)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><strong>CL (Clarity):</strong> Yes — Reports must be concise and precisely structured to be understood by stakeholders.  </li>
</ul>

<blockquote>
  <p>“…synthesize all relevant clinical information into a concise and precisely structured document.” (p. 1)  </p>
</blockquote>

<ul>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Findings must be relevant to the patient’s condition and clinical management.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Communication processes must be operationally possible within institutional constraints.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes — Strong emphasis on rapid delivery based on urgency category.  </p></li>
<li><p><strong>EX (Explainability):</strong> No — Paper does not explicitly frame explainability as part of actionability.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — Aligns communication with patient outcomes but not framed as explicit dimension.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Integration with IT systems; closed-loop communication; documentation.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Imaging Value Chain model.  </p></li>
<li><p>ACR Actionable Reporting framework.</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Time-to-communication metrics by urgency category.  </p></li>
<li><p>Audit logs of communication events.  </p></li>
<li><p>Confirmation of recipient acknowledgment.</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Fragmented IT systems; lack of integrated EMR; variability in preliminary/final report workflows; teleradiology delays; ad hoc communication methods.  </p></li>
<li><p><strong>Enablers:</strong> Integrated IT solutions; standardized critical findings policies; embedding radiologists in care teams; electronic alerts; patient portals.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Builds directly on ACR Actionable Reporting Work Group recommendations, situating them within broader workflow and technology integration strategies.</p>

<hr />

<h2>Summary</h2>

<p>Boland et al. (2014) conceptualize actionability in radiology as the combination of meaningful findings, timeliness, targeted delivery, and confirmation that recipients understand the information. They operationalize this via the ACR’s three-category urgency system, advocating for integrated IT solutions, closed-loop communication, and embedding radiologists within clinical teams to facilitate real-time exchange. The paper situates actionable reporting as the final and often weakest link in the imaging value chain, emphasizing that value is realized only when reports are delivered and understood by relevant stakeholders. It extends prior frameworks by addressing institutional barriers (e.g., fragmented systems, teleradiology inefficiencies) and proposing specific operational strategies for different clinical contexts, including patient-facing transparency initiatives.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 83 — Strong implicit definition, tied to explicit features and urgency framework, though not a formal theoretical model.  </p></li>
<li><p><strong>Operationalization Score:</strong> 75 — Provides concrete steps and workflow recommendations linked to actionability; primarily conceptual rather than empirical.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“A report creates little value until it is delivered, read, and correctly understood… Only then can information be used to have an impact on patient outcomes.” (p. 1)  </p></li>
<li><p>“Category-1 findings require communication within minutes, usually by direct verbal communication…” (p. 1)  </p></li>
<li><p>“Electronic text and e-mail alerts… close the communication loop…” (p. 2)  </p></li>
<li><p>“Radiologists need to remember that they serve primarily in an information business and recognize that value will be created only when actionable reports are delivered…” (p. 3)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li>Larson PA, Berland LL, Kahn CE, Liebscher LA. <em>Actionable findings and the role of IT support: report of the ACR Actionable Reporting Work Group</em>. J Am Coll Radiol. 2014;11:552–8.</li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Generating Actionable Insights from Patient Medical Records and Structured Clinical Knowledge  </p>

<p>Authors: Natasha Trajkovska, Michael Roiss, Sophie Bauernfeind, Mohammad Alnajdawi, Simone Sandler, Daniel Herzmanek, Matthias Winkler, Michael Haider, Oliver Krauss  </p>

<p>DOI: 10.3233/SHTI240015  </p>

<p>Year: 2024  </p>

<p>Publication Type: Conference  </p>

<p>Discipline/Domain: Health Informatics / Medical Data Science  </p>

<p>Subdomain/Topic: Clinical decision support, medical NLP, structured knowledge integration  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 85  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (implicit)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Partial  </p>

<p>Contains Interpretability: Yes (implicitly through process mining and ontology mapping)  </p>

<p>Contains Framework/Model: Yes (Treetop treatment pathways &amp; disease models)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Mixed Methods (technical implementation with evaluation)  </p>

<p>Study Context: Extraction and structuring of unstructured patient records for clinical decision support  </p>

<p>Geographic/Institutional Context: Austria; University of Applied Sciences Upper Austria, Treetop Medical, Medical University of Vienna  </p>

<p>Target Users/Stakeholders: Clinicians, medical decision support developers, healthcare institutions  </p>

<p>Primary Contribution Type: Technical method and evaluation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Partial  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Generating Actionable Insights from Patient Medical Records and Structured Clinical Knowledge  </p>

<p><strong>Authors:</strong>  </p>

<p>Natasha Trajkovska, Michael Roiss, Sophie Bauernfeind, Mohammad Alnajdawi, Simone Sandler, Daniel Herzmanek, Matthias Winkler, Michael Haider, Oliver Krauss  </p>

<p><strong>DOI:</strong>  </p>

<p>10.3233/SHTI240015  </p>

<p><strong>Year:</strong>  </p>

<p>2024  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Health Informatics / Medical Data Science  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Clinical decision support, medical NLP, structured knowledge integration  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>This work addresses the challenge of converting unstructured medical text (e.g., patient letters, lab reports) into structured, encoded, and contextualized data to reconstruct a patient’s treatment course. The aim is to integrate this with predefined treatment pathways and disease models for improved clinical decision-making.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Austria; University of Applied Sciences Upper Austria, Treetop Medical, Medical University of Vienna.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Clinicians, health IT specialists, hospital administrators, AI developers in healthcare.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Mixed methods—technical pipeline development, natural language processing, process mining, comparative evaluation.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Technical method and evaluation.  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The paper proposes a method to transform unstructured patient medical records into structured, encoded, and contextually interpreted data, enabling the generation of actionable clinical insights. The approach uses LLM-based extraction (LLM and GuidedLLM variants), enriched by structured medical knowledge from treatment pathways and disease models, to improve accuracy. Extracted data are encoded with SNOMED CT, organized chronologically using process mining, and compared against standard treatment pathways to identify deviations or missing steps. Evaluation with four chronic myeloid leukemia patient cases shows that GuidedLLM outperforms a generic LLM in detecting relevant diagnoses and medications, achieving 100% detection in targeted evaluation and high Jaccard similarity for detail accuracy. The method supports better clinical decision-making and patient safety by highlighting deviations, resource bottlenecks, and required follow-ups.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is implicitly defined as the transformation of raw, unstructured medical data into structured, encoded, and contextually interpreted knowledge that can directly inform clinical decision-making and patient management.</p>

<blockquote>
  <p>“...transform unstructured data into a cascade of progressively refined stages: structured data, encoded data, interpreted data, and ultimately, actionable knowledge.” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“...identify relevant findings in the treatment course that might be relevant for upcoming treatments or procedures.” (p. 4)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Accurate extraction of relevant clinical events from unstructured data.  </p></li>
<li><p>Encoding with standardized clinical terminologies (e.g., SNOMED CT).  </p></li>
<li><p>Chronological reconstruction of treatment history.  </p></li>
<li><p>Contextual comparison with evidence-based treatment pathways.  </p></li>
<li><p>Identification of deviations, missing steps, or bottlenecks in care.  </p></li>
<li><p>Alignment with patient-specific disease models and upcoming care needs.</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Treetop Medical treatment pathways and disease models; GuidedLLM extraction pipeline.  </p></li>
<li><p><strong>Methods/Levers:</strong> NLP with Llama-2-70b-orca-200k, medical knowledge-infused prompting, ontology mapping, process mining.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Convert unstructured PDFs to plain text.  </p>

<p> 2. Classify document type.  </p>

<p> 3. Section segmentation (diagnosis, medication, etc.).  </p>

<p> 4. Extract structured data using LLM or GuidedLLM.  </p>

<p> 5. Map extracted data to SNOMED CT codes via hybrid lexical-semantic matching.  </p>

<p> 6. Construct treatment timeline using process mining.  </p>

<p> 7. Compare with predefined treatment pathways and detect deviations/missing steps.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Sensitivity, Jaccard similarity coefficient.  </p></li>
<li><p><strong>Implementation Context:</strong> Chronic myeloid leukemia patient letters and lab reports.</p></li>
</ul>

<blockquote>
  <p>“...construct a chronological treatment timeline... can then be automatically compared to the treatment plan that should be followed...” (p. 3)  </p>
</blockquote>

<blockquote>
  <p>“...identify relevant findings... and deviations between predefined treatment pathways and actual treatment courses...” (p. 4)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Data is structured, encoded, and clearly organized in JSON for interpretability.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Aligned with disease models and treatment pathways.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Implemented with existing EHR data and ontology standards.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes — Designed to highlight upcoming procedures and overdue checks.  </p></li>
<li><p><strong>EX (Explainability):</strong> Partial — Process is interpretable, but LLM outputs may have limited transparency.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — Directly aligned with clinical guidelines and personalized patient management.  </p></li>
<li><p><strong>Other Dimensions:</strong> Safety relevance, deviation detection.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Evidence-based clinical pathways and guidelines.  </p></li>
<li><p>Ontology-based data encoding (SNOMED CT).  </p></li>
<li><p>Process mining for event timeline reconstruction.</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Sensitivity (diagnosis and medication detection).  </p></li>
<li><p>Jaccard similarity coefficient for extraction detail accuracy.  </p></li>
<li><p>Deviation detection between actual and standard treatment timelines.</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong>  </p>

<p> - Unstructured and heterogeneous medical data formats.  </p>

<p> - LLM hallucinations.  </p>

<p> - Limited initial dataset size.  </p></li>
<li><p><strong>Enablers:</strong>  </p>

<p> - Integration of structured medical knowledge in LLM prompting.  </p>

<p> - Use of standard clinical ontologies.  </p>

<p> - Automated process mining.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Positions itself among LLM applications in medicine, highlighting mixed results without domain-specific knowledge, and shows improvement via expert knowledge integration. Builds on work in NLP for EHRs and structured data extraction.</p>

<hr />

<h2>Summary</h2>

<p>The paper presents a practical, technically grounded approach to making medical data actionable by systematically transforming unstructured patient records into structured, encoded, and clinically contextualized information. Through the GuidedLLM method, enriched by disease models and treatment pathways, the authors achieve high detection and detail accuracy in extracting relevant diagnoses and medications. Actionability here is tied to enabling precise clinical decision support: contextual relevance, adherence to treatment pathways, detection of deviations, and alignment with patient safety. Operationalization is robust, combining NLP, ontology mapping, and process mining to produce outputs directly comparable to clinical guidelines. While explainability of the LLM components remains partial, the methodology is well-aligned with goal-driven, evidence-based care.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 85 — Strong implicit conceptualization of actionability with explicit features tied to clinical decision-making.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Clear, multi-step pipeline from raw data to actionable knowledge, evaluated with concrete metrics.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“...transform unstructured data into a cascade of progressively refined stages: structured data, encoded data, interpreted data, and ultimately, actionable knowledge.” (p. 2)  </p></li>
<li><p>“...identify relevant findings in the treatment course that might be relevant for upcoming treatments or procedures.” (p. 4)  </p></li>
<li><p>“...construct a chronological treatment timeline... can then be automatically compared to the treatment plan that should be followed...” (p. 3)  </p></li>
<li><p>“...identify deviations between predefined treatment pathways and actual treatment courses...” (p. 4)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Cellina et al. (2023) on personalized medicine and digital twins.  </p></li>
<li><p>Sugandh et al. (2024) on personalized diabetes care.  </p></li>
<li><p>Packer &amp; Metra (2021) on guideline adherence in heart failure.  </p></li>
<li><p>Jarjour et al. (2020) on care gaps in guideline adherence.  </p></li>
<li><p>Vaismoradi et al. (2020) on patient safety principles.  </p></li>
<li><p>Koleck et al. (2019) and Sheikhalishahi et al. (2019) on NLP for clinical notes.  </p></li>
<li><p>Thirunavukarasu et al. (2023) on LLMs in medicine.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Actionable Intelligence  </p>

<p>Authors: Eugene McMahon  </p>

<p>DOI: n/a  </p>

<p>Year: 2010  </p>

<p>Publication Type: Journal Commentary  </p>

<p>Discipline/Domain: Education / Special Education  </p>

<p>Subdomain/Topic: Education of students with blindness and visual impairments; data-driven program improvement  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 72  </p>

<p>Operationalization Score: 65  </p>

<p>Contains Definition of Actionability: Yes (explicit and contextualized to field)  </p>

<p>Contains Systematic Features/Dimensions: Yes (sample size, comparability, relevance to program change)  </p>

<p>Contains Explainability: Partial  </p>

<p>Contains Interpretability: Partial  </p>

<p>Contains Framework/Model: Yes (COSB outcome data collection infrastructure)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual with applied data infrastructure design  </p>

<p>Study Context: Council of Schools for the Blind (COSB) initiative to collect and use longitudinal student data for program improvement  </p>

<p>Geographic/Institutional Context: United States; COSB member schools  </p>

<p>Target Users/Stakeholders: Superintendents, educators, administrators in schools for the blind  </p>

<p>Primary Contribution Type: Conceptual framework with applied data collection process  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Partial  </p>

<p>TI: No  </p>

<p>EX: Partial  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Actionable Intelligence  </p>

<p><strong>Authors:</strong>  </p>

<p>Eugene McMahon  </p>

<p><strong>DOI:</strong>  </p>

<p>n/a  </p>

<p><strong>Year:</strong>  </p>

<p>2010  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Commentary  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Education / Special Education  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Education of students with blindness and visual impairments; data-driven program improvement  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The piece addresses the lack of “actionable intelligence” in the education of students with visual impairments, defined as information that drives meaningful changes in instructional strategies, program design, funding priorities, and professional development. It introduces the COSB initiative to systematically collect and analyze longitudinal outcome data to overcome barriers like low incidence and high diversity of the student population.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>United States; Council of Schools for the Blind (COSB)  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Superintendents, school administrators, educators of students with blindness/visual impairment  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual framework with applied data infrastructure and descriptive reporting  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Conceptual and practical model for collecting and using outcome data to enable actionable decision-making in a low-incidence educational field  </p>

<h2>General Summary of the Paper</h2>

<p>This commentary introduces the COSB’s long-term project to collect outcome data on graduates of schools for the blind, aiming to generate “actionable intelligence” that can inform program improvements. McMahon defines actionable intelligence as information sufficient to prompt changes in practice. The project uses a structured Base Data Set (school years) and Post-Graduation Data Set (post-school life) to allow for disaggregated, comparable analysis by student characteristics. The commentary highlights two main barriers to actionable intelligence in this field: low incidence of visual impairment and diversity of learning characteristics. While the data collection has limitations, the design allows superintendents to compare inputs and outcomes across schools with similar student profiles, supporting informed program adjustments.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>Actionable intelligence is explicitly defined as <em>“information sufficient to allow the government to take some action for the protection of the American people”</em>, adapted here as <em>information that will cause those of us in the blindness field to change instructional strategies, program offerings, funding priorities, professional development, and the like</em> (p. 1).  </p>

<blockquote>
  <p>“Given the dearth of actionable intelligence, professionals are often left relying only on their past experiences and clinical judgments.” (p. 1)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Sufficient to prompt meaningful changes in educational practice  </p></li>
<li><p>Comparable across similar populations (“apple to apple” comparisons)  </p></li>
<li><p>Based on adequate sample sizes to justify practice changes  </p></li>
<li><p>Sensitive to diversity of learning characteristics in the target population  </p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> COSB Outcome Data Collection Infrastructure  </p></li>
<li><p><strong>Methods/Levers:</strong> Longitudinal data on student demographics, program activities, exit outcomes/satisfaction, and post-graduation outcomes  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Annual data submission from member schools; categorization into Base and Post-Graduation Data Sets; disaggregation by variables for meaningful comparison  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Demographics, reading level, disability status, employment, education, independence, satisfaction  </p></li>
<li><p><strong>Implementation Context:</strong> COSB schools in the U.S.  </p></li>
</ul>

<blockquote>
  <p>“Outcome data can be disaggregated… to arrive at meaningful ‘apple to apple’ comparisons.” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“Such comparisons might then result in professionals making meaningful, generalizeable changes to interventions.” (p. 2)</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Actionability depends on clear, comparable data for interpreting results.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Data must be relevant to specific program improvement contexts.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Partial — Acknowledges sample size and diversity constraints affecting practical application.  </p></li>
<li><p><strong>TI (Timeliness):</strong> No — Timeliness not explicitly discussed.  </p></li>
<li><p><strong>EX (Explainability):</strong> Partial — Comparisons aim to explain variations in outcomes.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — Data collection designed to support COSB’s goal of improving programs.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Comparability, generalizability, meaningfulness.</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Adaptation of “actionable intelligence” from national security discourse to education  </p></li>
<li><p>Emphasis on data-driven decision-making in special education contexts  </p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Ability to disaggregate and compare outcomes by relevant student characteristics  </p></li>
<li><p>Sufficient sample size to justify generalizable changes  </p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Low incidence of visual impairment; diversity of student characteristics; small research sample sizes.  </p></li>
<li><p><strong>Enablers:</strong> Systematic, longitudinal data collection; commitment of COSB superintendents; structured datasets enabling comparability.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions itself against a backdrop of limited empirical data in the field of visual impairment education, reframing the term “actionable intelligence” to fit the sector’s needs, and proposing structured outcome measurement as a remedy.</p>

<h2>Summary</h2>

<p>McMahon’s commentary redefines “actionable intelligence” from national security to the education of visually impaired students, emphasizing the need for information that directly prompts changes in practice. The COSB’s longitudinal data collection framework, split into Base and Post-Graduation Data Sets, is designed to overcome challenges of low incidence and population diversity by enabling meaningful comparisons across similar student groups. While limitations remain—particularly small sample sizes and potential overinterpretation—the approach provides a structured, replicable means for superintendents to align program changes with measured outcomes. Actionability here is tied to clarity, contextual relevance, comparability, and goal alignment, with feasibility partially addressed. This framework offers a practical path toward evidence-informed program improvement in a field historically lacking robust outcome data.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 72 — Clear, adapted definition of actionability with identified features (comparability, sufficient evidence, contextual relevance); less emphasis on certain dimensions like timeliness.  </p></li>
<li><p><strong>Operationalization Score:</strong> 65 — Provides a concrete data collection and comparison infrastructure linked to actionability, but lacks detailed procedural guidelines for converting data insights into implemented changes.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Information that will cause those of us in the blindness field to change instructional strategies, program offerings, funding priorities, professional development, and the like.” (p. 1)  </p></li>
<li><p>“Outcome data can be disaggregated… to arrive at meaningful ‘apple to apple’ comparisons.” (p. 2)  </p></li>
<li><p>“Such comparisons might then result in professionals making meaningful, generalizeable changes to interventions.” (p. 2)  </p></li>
<li><p>“The first purpose of the project is to give superintendents the ability to compare inputs and outcomes of their students to students with similar learning characteristics across the other COSB schools.” (p. 2)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li>None explicitly cited for defining/operationalizing actionability.</li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Explainability: Actionable Information Extraction</p>

<p>Authors: Catarina Silva, Jorge Henriques, Bernardete Ribeiro</p>

<p>DOI: https://doi.org/10.1007/978-3-031-59216-4_11</p>

<p>Year: 2024</p>

<p>Publication Type: Conference</p>

<p>Discipline/Domain: Artificial Intelligence / Machine Learning</p>

<p>Subdomain/Topic: Explainability, Actionable Information Extraction, Knowledge Distillation</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 78</p>

<p>Operationalization Score: 85</p>

<p>Contains Definition of Actionability: Implicit</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Yes</p>

<p>Contains Interpretability: Yes</p>

<p>Contains Framework/Model: Yes</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Conceptual with empirical demonstration</p>

<p>Study Context: Credit scoring (German credit dataset), adaptable to other domains</p>

<p>Geographic/Institutional Context: University of Coimbra, Portugal</p>

<p>Target Users/Stakeholders: AI practitioners, decision-makers in finance/healthcare, researchers in interpretability/actionability</p>

<p>Primary Contribution Type: Methodological approach for explainable and actionable AI</p>

<p>CL: Yes — “visualization of decision-trees is also human-friendly making them better for explanation and interpretation” (p. 108)</p>

<p>CR: Partial — implied via “adaptable to different setup… health prognosis… predictive maintenance” (p. 110)</p>

<p>FE: Yes — “training a model with the support of a neural net’s dark knowledge might be beneficial to get better performance on less complex models” (p. 111)</p>

<p>TI: No — timeliness not explicitly linked to actionability</p>

<p>EX: Yes — “importance of each feature… example of the set of rules extracted… for actionability” (p. 111–112)</p>

<p>GA: Partial — goal alignment implied via problem-specific feature importance</p>

<p>Reason if Not Eligible: n/a</p>

<!--META_END-->

<p><strong>Title:</strong> Explainability: Actionable Information Extraction  </p>

<p><strong>Authors:</strong> Catarina Silva, Jorge Henriques, Bernardete Ribeiro  </p>

<p><strong>DOI:</strong> https://doi.org/10.1007/978-3-031-59216-4_11  </p>

<p><strong>Year:</strong> 2024  </p>

<p><strong>Publication Type:</strong> Conference  </p>

<p><strong>Discipline/Domain:</strong> Artificial Intelligence / Machine Learning  </p>

<p><strong>Subdomain/Topic:</strong> Explainability, Actionable Information Extraction, Knowledge Distillation  </p>

<p><strong>Contextual Background:</strong> The paper addresses the challenge of making black-box AI models interpretable and actionable by transferring knowledge to interpretable decision-tree models. Demonstrated in credit scoring, the approach is intended for broader decision-support applications where explanation and actionable rules are critical.  </p>

<p><strong>Geographic/Institutional Context:</strong> University of Coimbra, Portugal  </p>

<p><strong>Target Users/Stakeholders:</strong> AI/ML practitioners, data scientists, domain experts in finance/healthcare, policy-makers needing interpretable decision rationale.  </p>

<p><strong>Primary Methodology:</strong> Conceptual proposal with empirical validation  </p>

<p><strong>Primary Contribution Type:</strong> Methodological — interpretable surrogate modeling for actionable insights  </p>

<h2>General Summary of the Paper</h2>

<p>This work proposes a method for extracting actionable information from black-box machine learning models by distilling their decision patterns into interpretable decision-tree surrogates. The approach uses logits from a trained deep neural network (“Teacher”) as soft targets to train a decision-tree model (“Student”), thereby mimicking the black-box model’s decision process while maintaining interpretability. Experiments on the German credit dataset show that the student model closely matches the teacher’s performance, especially in recall, which is important for credit risk contexts. Feature importance visualizations and explicit decision rules are extracted from the surrogate, enabling actionable insights for decision support. The authors argue that this approach can generalize to other domains and outline future work towards a general-purpose framework.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is implicitly defined as providing interpretable decision patterns from AI models that can support human decision-making through explicit rules and feature importance that guide interventions.</p>

<blockquote>
  <p>“provide actionable information that can be used to support decisions” (p. 105)  </p>
</blockquote>

<blockquote>
  <p>“Rules extracted for actionability” (p. 112)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Interpretability through human-friendly visualization (decision trees)</p></li>
<li><p>Ability to reveal feature interactions and their role in decision-making</p></li>
<li><p>Extraction of explicit rules that map conditions to outcomes</p></li>
<li><p>Alignment of model logic with domain-specific decision needs</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Decision-tree surrogate via knowledge distillation  </p></li>
<li><p><strong>Methods/Levers:</strong> Transfer logits from a deep neural net to a gradient-boosted decision-tree  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Train a black-box deep neural network (Teacher)  </p>

<p> 2. Extract logits (soft targets) from its final layer  </p>

<p> 3. Train a decision-tree model (Student) on these soft targets  </p>

<p> 4. Compare Student’s performance with Teacher’s to validate fidelity  </p>

<p> 5. Extract interpretable rules and feature importance from the Student  </p></li>
<li><p><strong>Data &amp; Measures:</strong> German credit dataset; metrics include accuracy, precision, recall, F1-score; special focus on recall due to domain priorities  </p></li>
<li><p><strong>Implementation Context:</strong> Credit risk classification, adaptable to health prognosis and predictive maintenance  </p></li>
</ul>

<blockquote>
  <p>“visualization of decision-trees is… human-friendly making them better for explanation and interpretation” (p. 108)  </p>
</blockquote>

<blockquote>
  <p>“Rules extracted for actionability” (p. 112)</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — decision-tree visualization explicitly linked to explanation (p. 108)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Partial — adaptation to multiple domains suggested (p. 110)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — method improves performance of less complex models (p. 111)  </p></li>
<li><p><strong>TI (Timeliness):</strong> No — timeliness not discussed  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — feature importance and rule extraction for decision understanding (p. 111–112)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — alignment implied via feature targeting and decision context  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Fidelity to original model’s decision-making</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Knowledge distillation (Hinton et al., 2015)  </p></li>
<li><p>Surrogate model interpretability (Ribeiro et al., 2016 — LIME)  </p></li>
<li><p>Model compression (Bucila et al., 2006)</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Fidelity between surrogate and original model’s predictions  </p></li>
<li><p>Recall and F1-score improvements in decision-critical contexts  </p></li>
<li><p>Feature importance scores  </p></li>
<li><p>Explicit decision rules</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Black-box nature of high-performance models; complexity trade-offs with interpretability  </p></li>
<li><p><strong>Enablers:</strong> Surrogate modeling; human-friendly rule extraction; high fidelity between models</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions itself within interpretability research, particularly model-agnostic surrogate modeling and knowledge distillation, extending these techniques to emphasize rule extraction for actionable decision-making.</p>

<h2>Summary</h2>

<p>The paper contributes a method for making AI models both interpretable and actionable by distilling a black-box neural network into a decision-tree surrogate. This enables extraction of explicit, human-readable rules that preserve the predictive behavior of the original model while providing actionable insights for practitioners. The method is validated in credit scoring, showing that surrogate models can achieve comparable or better performance than the original black-box while revealing feature importance and decision logic. Actionability is achieved through clarity, explainability, and feasibility, though timeliness and explicit goal alignment are less developed. The approach is positioned as domain-agnostic and adaptable to various decision-support scenarios.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 78 — Strong implicit definition of actionability and clear identification of actionable features; could be improved with an explicit, formal definition and broader coverage of contextual relevance and goal alignment.  </p></li>
<li><p><strong>Operationalization Score:</strong> 85 — Detailed, replicable workflow with specific implementation steps; directly tied to producing actionable outputs from black-box models.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“provide actionable information that can be used to support decisions” (p. 105)  </p></li>
<li><p>“visualization of decision-trees is… human-friendly making them better for explanation and interpretation” (p. 108)  </p></li>
<li><p>“training a model with the support of a neural net’s dark knowledge might be beneficial to get better performance on less complex models” (p. 111)  </p></li>
<li><p>“Rules extracted for actionability” (p. 112)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Hinton et al., 2015 — Knowledge distillation  </p></li>
<li><p>Ribeiro et al., 2016 — LIME  </p></li>
<li><p>Bucila et al., 2006 — Model compression  </p></li>
<li><p>Che et al., 2015 — Interpretable mimic learning  </p></li>
<li><p>Xu et al., 2018 — DarkSight visualization</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Domain-Driven, Actionable Knowledge Discovery  </p>

<p>Authors: Longbing Cao, Chengqi Zhang  </p>

<p>DOI: 10.1109/MIS.2007.75  </p>

<p>Year: 2007  </p>

<p>Publication Type: Journal Article  </p>

<p>Discipline/Domain: Computer Science / Data Mining  </p>

<p>Subdomain/Topic: Domain-Driven Data Mining (D3M), Actionable Knowledge Discovery  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 92  </p>

<p>Operationalization Score: 88  </p>

<p>Contains Definition of Actionability: Yes (explicit and implicit)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Partial  </p>

<p>Contains Framework/Model: Yes (Domain-Driven Data Mining framework)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual with applied case studies  </p>

<p>Study Context: Complex domain problems in business and government (e.g., trade support, social security debt detection)  </p>

<p>Geographic/Institutional Context: University of Technology Sydney; case studies in Australian government and business domains  </p>

<p>Target Users/Stakeholders: Business decision-makers, data scientists, government analysts  </p>

<p>Primary Contribution Type: Conceptual framework with operational guidance  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> Domain-Driven, Actionable Knowledge Discovery  </p>

<p><strong>Authors:</strong> Longbing Cao, Chengqi Zhang  </p>

<p><strong>DOI:</strong> 10.1109/MIS.2007.75  </p>

<p><strong>Year:</strong> 2007  </p>

<p><strong>Publication Type:</strong> Journal Article  </p>

<p><strong>Discipline/Domain:</strong> Computer Science / Data Mining  </p>

<p><strong>Subdomain/Topic:</strong> Domain-Driven Data Mining (D3M), Actionable Knowledge Discovery  </p>

<p><strong>Contextual Background:</strong> Focuses on bridging the gap between data-mining research outputs and actionable results that meet real-world business and government decision-making needs. Argues for integrating domain knowledge, human intelligence, and context constraints into all stages of the KDD process.  </p>

<p><strong>Geographic/Institutional Context:</strong> University of Technology Sydney; case applications in Australia.  </p>

<p><strong>Target Users/Stakeholders:</strong> Business managers, policy-makers, domain experts, data analysts.  </p>

<p><strong>Primary Methodology:</strong> Conceptual with applied case examples.  </p>

<p><strong>Primary Contribution Type:</strong> Framework and methodology proposal with operational examples.</p>

<h2>General Summary of the Paper</h2>

<p>The paper introduces the Domain-Driven Data Mining (D3M) paradigm as a shift from traditional, purely data-driven KDD toward a methodology that explicitly incorporates domain knowledge, human expertise, and business context to produce actionable knowledge. The authors define actionable knowledge as patterns that hold both technical and business interestingness from objective and subjective perspectives. They present a formal framework, key differentiators between traditional and domain-driven approaches, and case studies in trade support and government debt analysis. The methodology emphasizes integrating multiple intelligence sources, balancing technical performance with business impact, and embedding human decision-making in the mining process.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as knowledge that is not only technically valid but also meaningful and implementable in a business or operational context.</p>

<blockquote>
  <p>“Domain-driven data mining generally targets actionable knowledge discovery in complex domain problems.” (p. 1)  </p>
</blockquote>

<blockquote>
  <p>“Actionable knowledge discovery should fit the following framework… from not only technological and business viewpoints but also objective and subjective perspectives.” (p. 1)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Meets both technical and business interestingness criteria.  </p></li>
<li><p>Balances objective (quantitative) and subjective (expert judgment) measures.  </p></li>
<li><p>Fits within business rules, policies, and operational constraints.  </p></li>
<li><p>Supports decision-making by delivering trustworthy, relevant, and context-sensitive results.  </p></li>
<li><p>Is derived through integration of multiple intelligence sources (data, domain, human, social, environmental).  </p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Domain-Driven Data Mining (D3M)  </p></li>
<li><p><strong>Methods/Levers:</strong> Integration of domain expertise, metasynthesis of multiple intelligence sources, business-oriented interestingness measures, runtime model customization.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify business and technical objectives → integrate domain knowledge and human input → apply adaptive mining processes → evaluate patterns by both technical and business measures → deliver decision-support-ready insights.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Technical metrics (support, confidence, lift); business metrics (impact on debt amount/duration).  </p></li>
<li><p><strong>Implementation Context:</strong> Applied in Australian government social security debt detection; trade support systems.  </p></li>
</ul>

<blockquote>
  <p>“We developed both technical and business measures for patterns relevant to these issues in real, unbalanced social security data.” (p. 2)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – patterns must be understandable to decision-makers.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – must reflect complex, real-world context.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – must be implementable under operational constraints.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial – timeliness is implied via runtime/adaptive processes but not a major focus.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – human involvement in interpretation is key.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – explicitly aligned with business goals and problem-solving.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Reliability, trustworthiness, cost-effectiveness.</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Pattern interestingness theory (Silberschatz &amp; Tuzhilin, 1996)  </p></li>
<li><p>Metasynthesis approach in complex systems  </p></li>
<li><p>Evolution of KDD toward domain-driven paradigms  </p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Technical: support, confidence, lift.  </p></li>
<li><p>Business: average debt amount, debt duration, business impact scores.  </p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Data constraints (heterogeneity, imbalance), evolving scenarios, technical–business conflicts.  </p></li>
<li><p><strong>Enablers:</strong> Human–machine collaboration, domain expert involvement, integration of contextual knowledge.  </p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions itself as an evolution of KDD beyond method-centric research to a business-impact-oriented practice. Builds on pattern interestingness and decision-support literature, extending with domain-specific operational frameworks.</p>

<h2>Summary</h2>

<p>Cao and Zhang (2007) present Domain-Driven Data Mining as a framework to make knowledge discovery truly actionable by integrating domain knowledge, human input, and contextual constraints into all stages of the process. Actionability is defined as knowledge that is both technically valid and business-relevant, balancing objective measures (e.g., statistical significance) with subjective expert judgment. The framework operationalizes actionability via metasynthesis of intelligence sources, runtime customization, and dual evaluation metrics. Case studies in government debt prevention illustrate how technical and business measures jointly determine a pattern’s value. This approach reframes KDD as a decision-support tool rather than a purely academic exercise, with clear methodological and evaluative criteria.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 92 – Strong, explicit definition of actionability; detailed conceptualization and criteria.  </p></li>
<li><p><strong>Operationalization Score:</strong> 88 – Clear framework and concrete operational examples, though some implementation details remain high-level.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Domain-driven data mining generally targets actionable knowledge discovery in complex domain problems.” (p. 1)  </p></li>
<li><p>“Actionable knowledge discovery should fit the following framework… from not only technological and business viewpoints but also objective and subjective perspectives.” (p. 1)  </p></li>
<li><p>“We developed both technical and business measures for patterns relevant to these issues in real, unbalanced social security data.” (p. 2)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Silberschatz, A., &amp; Tuzhilin, A. (1996). <em>What Makes Patterns Interesting in Knowledge Discovery Systems?</em> IEEE TKDE.  </p></li>
<li><p>Cao, L., &amp; Zhang, C. (2006). <em>Domain-Driven Data Mining: A Practical Methodology</em>. IJ Data Warehousing and Mining.  </p></li>
<li><p>Fayyad, U., Shapiro, G., &amp; Uthurusamy, R. (2003). <em>Data Mining: The Next 10 Years</em>. ACM SIGKDD Explorations.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Delivering actionable information  </p>

<p>Authors: Nathalie Colineau, Cécile Paris, Mingfang Wu  </p>

<p>DOI: 10.3166/ria.18.549-576  </p>

<p>Year: 2004  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Information Retrieval, Natural Language Generation  </p>

<p>Subdomain/Topic: Information Delivery, Document Generation, User Models  </p>

<p>Eligibility: Yes  </p>

<p>Overall Relevance Score: 85  </p>

<p>Operationalization Score: 80  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual and Empirical Analysis  </p>

<p>Study Context: Tailored Information Delivery in Knowledge Management  </p>

<p>Geographic/Institutional Context: CSIRO, Monash University  </p>

<p>Target Users/Stakeholders: Information Retrieval Practitioners, Knowledge Management Professionals  </p>

<p>Primary Contribution Type: Conceptual framework, platform development  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> Delivering actionable information  </p>

<p><strong>Authors:</strong> Nathalie Colineau, Cécile Paris, Mingfang Wu  </p>

<p><strong>DOI:</strong> 10.3166/ria.18.549-576  </p>

<p><strong>Year:</strong> 2004  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Information Retrieval, Natural Language Generation  </p>

<p><strong>Subdomain/Topic:</strong> Information Delivery, Document Generation, User Models  </p>

<p><strong>Contextual Background:</strong> The paper discusses the need for delivering information in a way that answers users' information needs and is presented in a way that facilitates understanding and use. The authors propose the Virtual Document Planner (VDP), a platform designed to support tailored information delivery. The platform integrates user models, information retrieval, and natural language generation to create coherent and useful documents for users. The paper focuses on improving the traditional search-and-browse mechanisms and investigates how delivering coherent, tailored documents can enhance user comprehension and engagement with the information.  </p>

<p><strong>Geographic/Institutional Context:</strong> CSIRO - ICT Centre, Monash University, Australia  </p>

<p><strong>Target Users/Stakeholders:</strong> Researchers, practitioners in information retrieval, knowledge management  </p>

<p><strong>Primary Methodology:</strong> Conceptual framework, empirical evaluation  </p>

<p><strong>Primary Contribution Type:</strong> Platform development, case study, evaluation  </p>

<h2>General Summary of the Paper</h2>

<p>The paper presents the Virtual Document Planner (VDP), a platform designed to generate tailored information delivery for users based on their specific needs. The VDP integrates techniques from user modeling, information retrieval, and natural language generation. The platform is aimed at producing coherent and useful documents that help users easily make sense of complex information, such as scientific or business-related content. The paper also describes a case study, PERCY, which generates customized corporate brochures for users, demonstrating the effectiveness of the VDP. A preliminary study shows that the tailored and coherent delivery of information is more useful than conventional search-and-browse systems.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<p>Reason if Not Eligible: n/a  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability in this context is understood as the delivery of information in a form that is not only relevant to the user's needs but also structured in a way that allows users to easily understand and apply the information. The VDP aims to provide actionable information by delivering tailored content that addresses the specific tasks or questions of the user, facilitating decision-making and understanding.  </p>

<blockquote>
  <p>“Actionable information is delivered when it addresses the user’s needs in a coherent and structured manner, enabling effective use and decision-making” (p. 3).  </p>
</blockquote>

<blockquote>
  <p>“Tailored information delivery ensures that the content is relevant, structured, and easy to use for the specific user context” (p. 5).</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<p>The paper identifies several key factors that make information actionable:</p>

<ul>
<li><p><strong>Relevance:</strong> The content must be specifically relevant to the user's needs and goals.  </p></li>
<li><p><strong>Clarity:</strong> The information should be presented in a clear and organized manner to enhance comprehension.  </p></li>
<li><p><strong>Tailoring:</strong> The information must be customized to the user's context, such as their role or task.  </p></li>
<li><p><strong>Coherence:</strong> The content must be logically structured to ensure that the relationships between information elements are clear.  </p></li>
</ul>

<blockquote>
  <p>“Tailored and coherent presentation of information makes it actionable by aligning the content with the user’s task and preferences” (p. 7).  </p>
</blockquote>

<blockquote>
  <p>“Actionability is achieved when the information is not only relevant but also easy to understand and apply” (p. 6).</p>
</blockquote>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<p>Actionability is operationalized through the use of the VDP platform, which applies discourse planning and user modeling to generate coherent documents. The VDP platform uses three main steps:</p>

<ol>
<li><p><strong>Content Planning:</strong> Determines the relevant information based on the user's query and profile.  </p></li>
<li><p><strong>Presentation Planning:</strong> Organizes the content and formats it according to the user’s delivery medium (e.g., web or paper).  </p></li>
<li><p><strong>Surface Realization:</strong> Generates the final document by assembling the content and formatting it for the specified medium.  </p></li>
</ol>

<blockquote>
  <p>“The VDP generates actionable information by selecting and organizing content based on the user’s needs and preferences, and presenting it in an appropriate format” (p. 7).  </p>
</blockquote>

<blockquote>
  <p>“By using discourse planning, the VDP ensures that the content is logically organized and relevant to the user’s context, which facilitates actionability” (p. 8).</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – Clear, coherent presentation of information is essential for actionability.  </p>

<p> &gt; “Coherent presentation is key to ensuring the information is easily understood and actionable” (p. 6).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – The information must be relevant to the user’s specific task or role.  </p>

<p> &gt; “Contextual relevance ensures that the information meets the user’s needs, making it actionable” (p. 5).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – The information should be easy to access and apply.  </p>

<p> &gt; “Feasibility is a key factor in actionability, as the information should be easily applied to the user’s task” (p. 7).  </p></li>
<li><p><strong>TI (Timeliness):</strong> No – The paper does not directly address timeliness, but it is implied through the relevance and organization of content.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – The clarity and structure of the information enable explainability and make it actionable.  </p>

<p> &gt; “Explainability is crucial for actionability, as the user must understand how to apply the information” (p. 7).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – The information should align with the user’s goals to ensure that it is actionable.  </p>

<p> &gt; “Aligning the content with the user’s goals ensures that the information is actionable and leads to meaningful outcomes” (p. 6).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<p>The authors base their approach on established theories in natural language generation and discourse planning, particularly Rhetorical Structure Theory (RST). They also draw from research in user modeling and information retrieval to create a system that tailors information delivery to users’ needs.  </p>

<blockquote>
  <p>“The VDP platform is built on discourse planning, using Rhetorical Structure Theory to ensure coherence and facilitate the delivery of actionable information” (p. 7).</p>
</blockquote>

<h2>Indicators or Metrics for Actionability</h2>

<p>The paper suggests that actionability can be evaluated by the relevance, clarity, and coherence of the delivered information. These factors determine how well the user can understand and use the information.  </p>

<blockquote>
  <p>“Actionability is measured by how well the information supports the user’s task and how easily it can be applied” (p. 7).</p>
</blockquote>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of coherence, irrelevant information, and poor organization of content can hinder actionability.  </p></li>
<li><p><strong>Enablers:</strong> Tailoring the information to the user’s needs, ensuring clarity, and maintaining a coherent structure are key enablers of actionability.  </p></li>
</ul>

<blockquote>
  <p>“Barriers to actionability arise when information is not tailored to the user’s needs or when it is presented in a disorganized manner” (p. 6).  </p>
</blockquote>

<blockquote>
  <p>“Tailoring and structuring the information according to the user’s context are key enablers of actionability” (p. 7).</p>
</blockquote>

<h2>Relation to Existing Literature</h2>

<p>The paper builds on existing work in information retrieval, natural language generation, and discourse theory, particularly Rhetorical Structure Theory (RST). It extends these ideas by applying them to the problem of tailored information delivery for users, specifically in a corporate context.  </p>

<blockquote>
  <p>“This work extends previous research on information retrieval and discourse planning by focusing on delivering tailored, coherent information that is actionable for users” (p. 8).</p>
</blockquote>

<h2>Summary</h2>

<p>This paper introduces the Virtual Document Planner (VDP), a platform designed to deliver actionable, tailored information to users. The VDP uses discourse planning, user modeling, and natural language generation techniques to organize and present information in a coherent and contextually relevant manner. The authors demonstrate the effectiveness of this approach through a case study involving the generation of customized corporate brochures, showing that users prefer this tailored delivery method over traditional search-and-browse systems.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 85 – The paper provides valuable insights into tailored information delivery and its impact on actionable information.  </p></li>
<li><p><strong>Operationalization Score:</strong> 80 – The VDP platform is well-described, though practical implementation details for wider applications could be expanded.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Coherent presentation is key to ensuring the information is easily understood and actionable” (p. 6).  </p></li>
<li><p>“Tailoring and structuring the information according to the user’s context are key enablers of actionability” (p. 7).  </p></li>
<li><p>“Actionability is measured by how well the information supports the user’s task and how easily it can be applied” (p. 7).  </p></li>
<li><p>“This work extends previous research on information retrieval and discourse planning by focusing on delivering tailored, coherent information that is actionable for users” (p. 8).</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Moore, J.D., &amp; Paris, C.L. (1993). Planning Text for Advisory Dialogues: Capturing Intentional and Rhetorical Information. Computational Linguistics.  </p></li>
<li><p>André, E., &amp; Rist, T. (1995). Generating Coherent Presentations Employing Textual and Visual Material. Artificial Intelligence Review.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Grand Challenges in Visual Analytics Applications  </p>

<p>Authors: Aoyu Wu, Dazhen Deng, Min Chen, Shixia Liu, Daniel Keim, Ross Maciejewski, Silvia Miksch, Hendrik Strobelt, Fernanda Viegas, Martin Wattenberg  </p>

<p>DOI: 10.1109/MCG.2023.3284620  </p>

<p>Year: 2023  </p>

<p>Publication Type: Journal Article  </p>

<p>Discipline/Domain: Visualization / Visual Analytics  </p>

<p>Subdomain/Topic: Research rigor and value in VA application research  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 80  </p>

<p>Operationalization Score: 70  </p>

<p>Contains Definition of Actionability: Yes (implicit, framed through "rigor" and "value" in VA applications)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Partial  </p>

<p>Contains Framework/Model: Yes (VA application research ecosystem)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual / Review with expert interviews and panel synthesis  </p>

<p>Study Context: Visual analytics research ecosystem and practice  </p>

<p>Geographic/Institutional Context: International (authors from USA, China, UK, Germany, Austria)  </p>

<p>Target Users/Stakeholders: Visual analytics researchers, practitioners, tool developers, interdisciplinary collaborators  </p>

<p>Primary Contribution Type: Conceptual framework and agenda-setting  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Grand Challenges in Visual Analytics Applications</p>

<p><strong>Authors:</strong>  </p>

<p>Aoyu Wu, Dazhen Deng, Min Chen, Shixia Liu, Daniel Keim, Ross Maciejewski, Silvia Miksch, Hendrik Strobelt, Fernanda Viegas, Martin Wattenberg</p>

<p><strong>DOI:</strong>  </p>

<p>10.1109/MCG.2023.3284620</p>

<p><strong>Year:</strong>  </p>

<p>2023</p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Article</p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Visualization / Visual Analytics</p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Research rigor and value in VA application research</p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses long-standing concerns about the rigor, value, and generalizability of visual analytics (VA) application research, synthesizing insights from interviews and a panel discussion to produce a conceptual research ecosystem and a set of 12 grand challenges.</p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>International (authors affiliated with Harvard University, Zhejiang University, University of Oxford, Tsinghua University, University of Konstanz, Arizona State University, TU Wien, MIT-IBM Watson AI Lab, Google)</p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>VA researchers, application developers, interdisciplinary research collaborators, practitioners in domains like bioinformatics, urban analytics, explainable AI.</p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual/review, based on synthesis of expert interviews and conference panel discussion.</p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Conceptual framework and strategic research agenda.</p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This article identifies and analyzes fundamental dilemmas in VA application research, particularly the tension between domain-specific solutions and the drive for generalizable knowledge, the subjectivity of design study methodologies, lack of documentation standards, and limited open science practices. The authors propose a conceptual ecosystem connecting academia, VA research, and practice through cycles of "rigor" and "value," and map 12 grand challenges across four thematic areas: foundation, methodology, application, and community. They call for better theoretical grounding, integration of AI and VA, improved evaluation frameworks, explainable VA, open-source contributions, deployment strategies, documentation standards, and expanded educational materials.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<hr />

<h2>How Actionability is Understood</h2>

<p>The paper conceptualizes "actionability" in VA research implicitly through two linked constructs: <em>rigor</em> (scientific validity, methodological soundness, generalizability) and <em>value</em> (practical relevance, societal impact, adoption). Actionability emerges from research that both advances academic knowledge and is practically deployable.</p>

<blockquote>
  <p>“VA application research is driven by real-world application problems, and successful solutions… generate socio-technical impact and value.” (p. 85)  </p>
</blockquote>

<blockquote>
  <p>“We advocate for an inclusive perspective to derive combined benefits from promoting the research value and rigor of VA applications.” (p. 85)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Connection between domain-specific problems and generalizable knowledge.</p></li>
<li><p>Use of both qualitative and quantitative methodologies to enhance validity.</p></li>
<li><p>Comprehensive documentation enabling reuse.</p></li>
<li><p>Openness (data, code) for replication and extension.</p></li>
<li><p>Integration with broader data science workflows.</p></li>
<li><p>Deployment and sustained community engagement.</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> VA Application Research Ecosystem (rigor and value cycles).</p></li>
<li><p><strong>Methods/Levers:</strong> Construction of knowledge bases, shared vocabularies, integration of VA and AI, guidance mechanisms, modular system design, open-source release.</p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify domain-specific problem → Design/build VA system → Justify/validate with mixed evaluation → Deploy and document for reuse.</p></li>
<li><p><strong>Data &amp; Measures:</strong> Real-world case collections, evaluation metrics tailored to complex analytical tasks, provenance data for explainability.</p></li>
<li><p><strong>Implementation Context:</strong> Academic-industry collaborations, interdisciplinary research projects.</p></li>
</ul>

<blockquote>
  <p>“We propose a research ecosystem that connects VA application research with academia and practice through the rigor circle and the value circle…” (p. 85)  </p>
</blockquote>

<blockquote>
  <p>“Open software is key to facilitating comparison and improvement…” (p. 88)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — through shared vocabularies, documentation standards.</p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — driven by domain-specific application needs.</p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — focus on deployable, sustainable open-source tools.</p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — mentions guidance that is “timely” but not a core recurring theme.</p></li>
<li><p><strong>EX (Explainability):</strong> Yes — goal of constructing explainable VA and capturing analytical processes.</p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — aligning academic rigor with real-world value.</p></li>
</ul>

<p><strong>Other Dimensions Named by Authors:</strong> Sustainability, modularity, interdisciplinarity, openness.</p>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Chen &amp; Ebert’s ontological framework for VA workflows.</p></li>
<li><p>Thomas &amp; Cook’s early VA theory work.</p></li>
<li><p>Design study methodology (Sedlmair et al.).</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Extent of code/data openness.</p></li>
<li><p>Adoption beyond original domain.</p></li>
<li><p>Evaluation metrics linked to cognitive functions and decision outcomes.</p></li>
<li><p>Reuse/modularization success.</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of shared vocabularies; subjective evaluations; limited deployment; closed systems.</p></li>
<li><p><strong>Enablers:</strong> Open-source release; shared knowledge bases; deployment tracks; guidance tools.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Positions itself in line with foundational VA definitions (Keim et al., 2008) and theoretical calls (Thomas &amp; Cook, 2006), extending them to emphasize the balance between scientific rigor and practical value.</p>

<hr />

<h2>Summary</h2>

<p>This paper reframes actionability in VA application research as the dual pursuit of <em>rigor</em> and <em>value</em>. The authors identify four persistent dilemmas — domain specificity vs. generalizability, subjective design study methods vs. quantitative rigor, documentation gaps, and limited open science — as barriers to actionability. They propose an ecosystem model linking academia, VA research, and practice, and outline 12 grand challenges spanning foundational theory, methodological innovation, deployment, community building, and education. Actionable VA research, in their view, produces reusable, explainable, well-documented systems that address real-world problems while advancing general knowledge. Operationalization strategies include constructing VA knowledge bases, integrating VA with AI, refining evaluation frameworks, and fostering open-source contributions.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 80 — Strong conceptual linkage between actionability, rigor, and value; explicit identification of features; lacks a formal definition of “actionability.”</p></li>
<li><p><strong>Operationalization Score:</strong> 70 — Provides a conceptual model and concrete levers for achieving actionability, but operational detail remains high-level.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“[VA research]… driven by real-world application problems, and successful solutions… generate socio-technical impact and value.” (p. 85)  </p></li>
<li><p>“We advocate for an inclusive perspective to derive combined benefits from promoting the research value and rigor…” (p. 85)  </p></li>
<li><p>“Open software is key to facilitating comparison and improvement…” (p. 88)  </p></li>
<li><p>“Another larger goal is to build explainable VA—how can we capture one’s analytical process and explain it to someone else?” (p. 87)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Chen &amp; Ebert (2019) — Ontological framework for VA.</p></li>
<li><p>Thomas &amp; Cook (2006) — Theory of VA.</p></li>
<li><p>Sedlmair et al. (2012) — Design study methodology.</p></li>
<li><p>Ceneda et al. (2017) — Guidance in VA.</p></li>
<li><p>Khayat et al. (2020) — Evaluation methods in VA.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Improving Process Mining Maturity – From Intentions to Actions  </p>

<p>Authors: Jonathan Brock, Katharina Brennig, Bernd Löhr, Christian Bartelheimer, Sebastian von Enzberg, Roman Dumitrescu  </p>

<p>DOI: 10.1007/s12599-024-00882-7  </p>

<p>Year: 2024  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Business Process Management, Process Mining  </p>

<p>Subdomain/Topic: Maturity Models, Process Mining Adoption  </p>

<p>Eligibility: Yes  </p>

<p>Overall Relevance Score: 90  </p>

<p>Operationalization Score: 85  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Mixed Methods (Interviews and Model Development)  </p>

<p>Study Context: Business Process Management, Process Mining in Organizations  </p>

<p>Geographic/Institutional Context: International (Multiple Organizations)  </p>

<p>Target Users/Stakeholders: Process Mining Practitioners, Business Process Managers, BPM Consultants  </p>

<p>Primary Contribution Type: Conceptual Model Development, Practical Guidelines  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> Improving Process Mining Maturity – From Intentions to Actions  </p>

<p><strong>Authors:</strong> Jonathan Brock, Katharina Brennig, Bernd Löhr, Christian Bartelheimer, Sebastian von Enzberg, Roman Dumitrescu  </p>

<p><strong>DOI:</strong> 10.1007/s12599-024-00882-7  </p>

<p><strong>Year:</strong> 2024  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Business Process Management, Process Mining  </p>

<p><strong>Subdomain/Topic:</strong> Maturity Models, Process Mining Adoption  </p>

<p><strong>Contextual Background:</strong> The paper addresses the growing need for a comprehensive framework to bridge the intention-action gap in process mining adoption within organizations. It develops a Process Mining Maturity Model (P3M) to assess and improve process mining readiness, based on factors like data foundation, organizational embedding, and people's knowledge. The model aims to guide organizations in optimizing their process mining activities by providing actionable insights for improvement.  </p>

<p><strong>Geographic/Institutional Context:</strong> The paper involves multiple organizations across various industries, with practical insights derived from eleven interviews.  </p>

<p><strong>Target Users/Stakeholders:</strong> Process mining practitioners, BPM consultants, business process managers  </p>

<p><strong>Primary Methodology:</strong> Mixed Methods (Interviews and Model Development)  </p>

<p><strong>Primary Contribution Type:</strong> Conceptual Model Development, Practical Guidelines  </p>

<h2>General Summary of the Paper</h2>

<p>The paper develops a multi-factor maturity model for process mining (P3M) to assist organizations in increasing their process mining readiness. The model consists of five factors and 23 elements, with five maturity stages. The authors conducted interviews with practitioners to identify actions that organizations can take to improve their process mining maturity. The research highlights how organizations can overcome common implementation challenges, such as lack of management support, poor data quality, and ineffective integration of process mining tools into existing processes. It provides both a theoretical framework and practical steps for organizations to enhance their process mining maturity and effectively manage process dynamics.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<p>Reason if Not Eligible: n/a  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is understood as the ability of organizations to successfully implement process mining through structured readiness assessments and the adoption of actionable improvement steps derived from the maturity model. The paper emphasizes that closing the gap between the intention to adopt process mining and the actual implementation requires practical guidance on how to enhance organizational capabilities and resources.  </p>

<blockquote>
  <p>“The process mining maturity model provides a roadmap for organizations to turn their intentions into actionable steps, ensuring that process mining can be implemented sustainably” (p. 3).  </p>
</blockquote>

<blockquote>
  <p>“Organizations need concrete actions that they can take to improve their readiness for process mining across the different maturity factors” (p. 11).</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<p>For process mining to be actionable, the authors identify several key factors that organizations must address:</p>

<ul>
<li><p>Organizational embedding of process mining initiatives  </p></li>
<li><p>Access to high-quality process and event data  </p></li>
<li><p>Knowledge and training for individuals involved in process mining  </p></li>
<li><p>Governance structures that support process mining implementation and scaling  </p></li>
</ul>

<blockquote>
  <p>“The organizational embedding of process mining is a key success factor that enables actionability within organizations” (p. 11).  </p>
</blockquote>

<blockquote>
  <p>“Data accessibility, quality, and governance are fundamental prerequisites for actionable process mining” (p. 14).</p>
</blockquote>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<p>The actionability of process mining is achieved by:</p>

<ul>
<li><p>Developing a maturity model (P3M) that organizations can use to assess and improve their process mining readiness  </p></li>
<li><p>Providing a structured process to guide organizations through various stages of process mining adoption  </p></li>
<li><p>Conducting workshops and surveys to help organizations identify gaps and improvement areas in their process mining activities  </p></li>
<li><p>Offering practical actions, based on real-world interviews, that organizations can implement to increase their process mining maturity  </p></li>
</ul>

<blockquote>
  <p>“The maturity model is applied through an online survey and workshop, which helps organizations identify their current maturity and set a roadmap for improvement” (p. 12).  </p>
</blockquote>

<blockquote>
  <p>“The maturity model helps organizations understand where they are in their process mining journey and provides tailored actions for advancing their maturity” (p. 12).</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – Actionable knowledge must be clearly communicated, especially in terms of the steps needed to improve process mining maturity.  </p>

<p> &gt; “Clarity in the purpose and scope of the process mining initiative is essential for ensuring that actions are understandable and actionable” (p. 9).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – Actions must be relevant to the specific organizational context and tailored to the challenges each organization faces.  </p>

<p> &gt; “Actions should be customized based on the organization’s specific needs and readiness” (p. 14).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – The actions identified must be feasible given the resources and capabilities of the organization.  </p>

<p> &gt; “Organizations should prioritize actions that are both feasible and have the potential to create significant improvements in process mining maturity” (p. 11).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes – Timely action is necessary to ensure that process mining initiatives can evolve as needed to address dynamic business process changes.  </p>

<p> &gt; “Organizations should not wait for perfect data or conditions but should take pragmatic steps to begin improving their process mining maturity” (p. 13).  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – Actionable steps must be easily understandable and justifiable for stakeholders across the organization.  </p>

<p> &gt; “Providing clear documentation and communication about process mining initiatives ensures that they are understood by all stakeholders” (p. 12).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – Actions must be aligned with organizational goals and strategies, ensuring that process mining efforts contribute to broader business objectives.  </p>

<p> &gt; “Process mining initiatives must align with organizational goals, such as improving efficiency and flexibility, to be truly actionable” (p. 10).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<p>The development of the P3M is based on existing BPM maturity models, particularly those focused on capability maturity in business process management (Rosemann and De Bruin, 2005b; Kerpedzhiev et al., 2021). The authors also draw on insights from process mining and data science to create a comprehensive model that includes both technical and organizational factors.  </p>

<blockquote>
  <p>“The P3M model synthesizes best practices from BPM maturity models, adapting them to the context of process mining and addressing both technical and organizational aspects” (p. 9).</p>
</blockquote>

<h2>Indicators or Metrics for Actionability</h2>

<p>The P3M model provides five maturity stages (Initial, Rudimentary, Standalone, Systematic, Optimizing) for each of the 23 elements across the five factors, enabling organizations to assess their current state and identify specific actions to improve their process mining readiness.  </p>

<blockquote>
  <p>“The maturity stages offer clear indicators of progress, allowing organizations to track their improvements and identify areas where action is needed” (p. 10).</p>
</blockquote>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of management support, poor data quality, insufficient organizational embedding of process mining initiatives  </p></li>
<li><p><strong>Enablers:</strong> Dedicated process mining teams, collaboration across business units, management support, structured maturity assessments  </p></li>
</ul>

<blockquote>
  <p>“Organizational embedding is a key enabler for process mining success, but without management buy-in, the initiative may struggle to gain traction” (p. 12).  </p>
</blockquote>

<blockquote>
  <p>“Data quality and accessibility are frequent barriers that need to be addressed early in the process mining implementation” (p. 13).</p>
</blockquote>

<h2>Relation to Existing Literature</h2>

<p>The paper builds on prior work in process mining and BPM maturity models, extending these frameworks by incorporating elements specific to the challenges of process mining adoption. It positions the P3M model as a solution to the intention-action gap identified in previous research, offering actionable steps for organizations to improve their process mining maturity.  </p>

<blockquote>
  <p>“This paper extends the concept of BPM maturity by integrating process mining-specific factors and providing actionable insights for organizations” (p. 14).</p>
</blockquote>

<h2>Summary</h2>

<p>The paper introduces a Process Mining Maturity Model (P3M), a framework that helps organizations assess and improve their process mining readiness. Through a detailed analysis of 23 elements across five factors, the model provides a structured approach for organizations to close the intention-action gap in process mining adoption. The authors also identify 30 actions, derived from real-world interviews, that organizations can take to improve their maturity and effectively manage process dynamics. The model and actions presented provide valuable guidance for organizations looking to enhance their process mining capabilities.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 90 – The paper provides a comprehensive, actionable framework for improving process mining maturity, addressing a significant gap in the current literature.  </p></li>
<li><p><strong>Operationalization Score:</strong> 85 – The maturity model is clearly operationalized through real-world actions and assessment tools, offering practitioners tangible steps for improvement.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“The process mining maturity model provides a roadmap for organizations to turn their intentions into actionable steps” (p. 3).  </p></li>
<li><p>“Organizations should prioritize actions that are both feasible and have the potential to create significant improvements in process mining maturity” (p. 11).  </p></li>
<li><p>“Actions should be customized based on the organization’s specific needs and readiness” (p. 14).  </p></li>
<li><p>“Clarity in the purpose and scope of the process mining initiative is essential for ensuring that actions are understandable and actionable” (p. 9).</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Reinkemeyer L, Grindemann P, Egli V, et al. (2022). Accelerating business transformation with a process mining center of excellence.  </p></li>
<li><p>Rosemann M, De Bruin T (2005b). Towards a business process management maturity model.  </p></li>
<li><p>Kerpedzhiev GD, Ko¨nig UM, Ro¨glinger M, Rosemann M (2021). An exploration into future business process management capabilities in view of digitalization.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: From Process Mining Insights to Process Improvement: All Talk and No Action?  </p>

<p>Authors: Vinicius Stein Dani, Henrik Leopold, Jan Martijn E. M. van der Werf, Iris Beerepoot, Hajo A. Reijers  </p>

<p>DOI: https://doi.org/10.1007/978-3-031-46846-9_15  </p>

<p>Year: 2024  </p>

<p>Publication Type: Conference (LNCS, CoopIS 2023)  </p>

<p>Discipline/Domain: Computer Science / Business Process Management  </p>

<p>Subdomain/Topic: Process Mining; Insights-to-Action; Process Improvement  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 88  </p>

<p>Operationalization Score: 75  </p>

<p>Contains Definition of Actionability: Implicit  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Partial  </p>

<p>Contains Interpretability: No  </p>

<p>Contains Framework/Model: No formal model, but taxonomy of actions  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Systematic Literature Review  </p>

<p>Study Context: Cross-domain, multiple sectors from reviewed case studies  </p>

<p>Geographic/Institutional Context: Global literature base; authors from Netherlands and Germany  </p>

<p>Target Users/Stakeholders: Process mining practitioners, managers, consultants, researchers  </p>

<p>Primary Contribution Type: Taxonomy of actions and insights-to-action links in process mining  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Partial  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>From Process Mining Insights to Process Improvement: All Talk and No Action?  </p>

<p><strong>Authors:</strong>  </p>

<p>Vinicius Stein Dani, Henrik Leopold, Jan Martijn E. M. van der Werf, Iris Beerepoot, Hajo A. Reijers  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1007/978-3-031-46846-9_15  </p>

<p><strong>Year:</strong>  </p>

<p>2024  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference Paper (LNCS, CoopIS 2023)  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Computer Science / Business Process Management  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Process mining; insights-to-action; process improvement  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>This paper investigates the gap between obtaining insights from process mining and translating them into actual process improvements. Drawing on a systematic literature review of 57 case-based studies, the authors categorize actions taken (or recommended) after insights are acquired, linking them to process, system, documentation, communication, and training changes. The intended users are practitioners and researchers aiming to improve process mining methodologies beyond insight generation.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Global study base; research institutions in the Netherlands (Utrecht University) and Germany (Kühne Logistics University).  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Process mining practitioners, managers, process analysts, organizational change agents, academic researchers.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Systematic Literature Review  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Taxonomy and mapping of actions to process mining insights.  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The paper addresses the underexplored area of how organizations act upon insights derived from process mining. Through a systematic literature review of 57 case studies, the authors identify and categorize 156 quotes leading to 226 coded actions. These actions are grouped into three main themes: (1) Supporting process understanding and documentation, (2) Improving involved information systems, and (3) Improving the investigated process. They reveal a many-to-many relationship between insights and actions, with common insights including low data quality, high wait times, high rework, problematic process models, and non-compliant behavior. The paper provides a structured taxonomy of intervention spaces and action types, highlighting that many interventions target supporting systems, documentation, and communication—not just the process itself.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is implicitly conceptualized as the capacity for process mining insights to lead to concrete, feasible, and targeted organizational changes. This includes the diversity of “intervention spaces” such as documentation, training, IT systems, and the process itself, with an emphasis on linking specific insights to tailored actions.  </p>

<blockquote>
  <p>“Understanding the diversity of the actions triggered by process mining insights is important to instigate future research on… translating process mining insights into process improvement” (p. 1)  </p>
</blockquote>

<blockquote>
  <p>“Making these actions explicit can help organizations… complement existing process mining methodologies” (p. 278)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Connection to specific, identifiable insights from process mining (e.g., bottlenecks, rework, non-compliance).</p></li>
<li><p>Clear target object in the intervention space (e.g., process, documentation, system).</p></li>
<li><p>Feasibility and plausibility (drawn from actions reported/recommended by practitioners).</p></li>
<li><p>Potential for measurable improvement in performance, compliance, or understanding.</p></li>
<li><p>Contextual fit with organizational structures, systems, and processes.</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> None formalized; outcome is a taxonomy of actions.</p></li>
<li><p><strong>Methods/Levers:</strong> Inductive coding of literature; categorization into themes and intervention spaces.</p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Insight acquisition → Identification of intervention space and object → Selection/implementation of action (performed or recommended).</p></li>
<li><p><strong>Data &amp; Measures:</strong> Qualitative coding of 226 supporting quotes from 57 papers.</p></li>
<li><p><strong>Implementation Context:</strong> Cross-domain applications in healthcare, manufacturing, logistics, IT, finance.</p></li>
</ul>

<blockquote>
  <p>“Three main themes of actions: i) supporting process understanding and documentation; ii) improving the involved information system…; iii) improving the investigated process.” (p. 281)  </p>
</blockquote>

<blockquote>
  <p>“We identified a clear pattern pointing out the important role the objects target of the actions themselves play in understanding… translation into process improvement.” (p. 280)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Clarity in documentation, communication, and visualization aids actionability.  </p>

<p> &gt; “Creating an ad-hoc custom visualization for communicating findings” (p. 283)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Actions are matched to the specific process context and insight type.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Only performed or practitioner-recommended actions included.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — Implied in discussions of delays/wait times, but not treated as core dimension.  </p></li>
<li><p><strong>EX (Explainability):</strong> Partial — Some focus on root cause investigation and justification of conduct.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — Many actions tied to organizational improvement goals, but not explicitly framed as alignment.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> None explicitly named beyond thematic categorization.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Draws on process mining methodologies (van der Aalst 2011, PM2, L*) and improvement literature.</p></li>
<li><p>Inspired by coding techniques for qualitative research (Saldana 2015).</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<p>No formal KPIs, but action triggers often relate to:</p>

<ul>
<li><p>Data quality metrics (missing fields, incomplete traces).</p></li>
<li><p>Performance metrics (wait time, activity execution time).</p></li>
<li><p>Compliance rates.</p></li>
<li><p>Rework frequency.</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong>  </p>

<p> - Gap between recommended and taken actions.  </p>

<p> - High effort or organizational resistance to certain actions.  </p>

<p> - Poor data quality undermining insight validity.</p></li>
<li><p><strong>Enablers:</strong>  </p>

<p> - Clear link between insight and targeted intervention.  </p>

<p> - Holistic consideration of process-related artefacts.  </p>

<p> - Integration between technical and organizational teams.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>The work extends process mining methodologies by explicitly detailing the “last mile” from insight to improvement, filling a gap where existing frameworks stop after insight generation. It complements works on process improvement, root cause analysis, and performance management.</p>

<hr />

<h2>Summary</h2>

<p>This paper provides a systematic, evidence-based taxonomy of actions organizations take after acquiring process mining insights, based on 57 case-based studies. Actionability is framed implicitly as the ability of an insight to lead to feasible, targeted organizational change, with intervention spaces spanning documentation, communication, training, IT systems, and the process itself. The authors identify a complex, many-to-many mapping between insights (e.g., low data quality, high wait time) and actions (e.g., adjust documentation, reallocate resources). Operationalization is achieved through inductive coding of reported/recommended actions, yielding three main themes. The findings highlight that process improvement extends beyond the process into systems, human factors, and organizational artefacts.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 88 — Strong implicit conceptualization of actionability, clear features, direct mapping between insights and actions.  </p></li>
<li><p><strong>Operationalization Score:</strong> 75 — Robust taxonomy and mapping, but lacks a prescriptive step-by-step method for achieving actionability in new contexts.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“We identify the intervention space, i.e., the aspects of the organization that are affected by the actions, since process improvement actions may not only concern the process itself.” (p. 276)  </p></li>
<li><p>“Three main themes of actions: supporting process understanding and documentation; improving the involved information system; improving the investigated process.” (p. 281)  </p></li>
<li><p>“There is a many-to-many relation between insights and actions… one insight can trigger several actions and… one action can be triggered by several insights.” (p. 287)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>van der Aalst (2011, 2016) on process mining.</p></li>
<li><p>Bozkaya et al. (2009) Process Diagnostics Methodology.</p></li>
<li><p>van Eck et al. (2015) PM2 methodology.</p></li>
<li><p>Lashkevich et al. (2023) on analysis templates for improvement.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Action-oriented process mining: bridging the gap between insights and actions</p>

<p>Authors: Gyunam Park, Wil M. P. van der Aalst</p>

<p>DOI: https://doi.org/10.1007/s13748-022-00281-7</p>

<p>Year: 2022</p>

<p>Publication Type: Journal</p>

<p>Discipline/Domain: Process Mining / Business Process Management</p>

<p>Subdomain/Topic: Action-oriented process mining, Continuous process improvement</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 93</p>

<p>Operationalization Score: 95</p>

<p>Contains Definition of Actionability: Yes</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Yes</p>

<p>Contains Interpretability: Partial</p>

<p>Contains Framework/Model: Yes</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Conceptual + Experimental</p>

<p>Study Context: Artificial IS + SAP ERP system (Order handling / Order-to-Cash)</p>

<p>Geographic/Institutional Context: RWTH Aachen University, Germany</p>

<p>Target Users/Stakeholders: Process managers, business analysts, ERP system users</p>

<p>Primary Contribution Type: Conceptual framework + implementation + empirical validation</p>

<p>CL: Yes</p>

<p>CR: Yes</p>

<p>FE: Yes</p>

<p>TI: Yes</p>

<p>EX: Yes</p>

<p>GA: Yes</p>

<p>Reason if Not Eligible: n/a</p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Action-oriented process mining: bridging the gap between insights and actions</p>

<p><strong>Authors:</strong>  </p>

<p>Gyunam Park, Wil M. P. van der Aalst</p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1007/s13748-022-00281-7</p>

<p><strong>Year:</strong>  </p>

<p>2022</p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal</p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Process Mining / Business Process Management</p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Action-oriented process mining, Continuous process improvement</p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the missing link between process mining insights (diagnostics, monitoring) and concrete, automated management actions for operational improvement. It proposes a framework that integrates continuous monitoring of event streams, constraint evaluation, and automated action generation. Targeted at dynamic business environments, the approach combines process mining, domain knowledge, and automated execution capabilities.</p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>RWTH Aachen University, Germany</p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Process managers, business analysts, ERP system users</p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual framework development with empirical validation (artificial IS + real-life ERP)</p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Framework + Implementation + Experimental Evaluation</p>

<h2>General Summary of the Paper</h2>

<p>The authors propose a <strong>general framework for action-oriented process mining</strong> designed to close the gap between diagnostics from process mining and the concrete actions needed for improvement. The framework comprises a <strong>constraint monitor</strong> (detects/predicts violations in real time) and an <strong>action engine</strong> (generates and executes management actions automatically). They instantiate the action engine as a <strong>cube-based model</strong> that aggregates violations along multiple dimensions for analysis using OLAP operations. Implementation is provided as a ProM plug-in and evaluated on (1) an artificial order handling process and (2) a real-life SAP ERP Order-to-Cash process. Results show improved operational performance, validating the framework’s ability to turn insights into timely, effective actions.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>Actionability is understood as the <strong>systematic conversion of process mining insights into automated, targeted management actions</strong> that improve operational performance.</p>

<blockquote>
  <p>“Action-oriented process mining aims at… systematically combining process mining results and domain knowledge, and also automating management actions to improve business processes.” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“…turn the insights from process mining diagnostics to management actions.” (p. 2)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Must be grounded in <strong>objective monitoring results</strong> (diagnostics + predictions).</p></li>
<li><p>Must be linked to <strong>clear operational goals</strong> (risk reduction, performance improvement).</p></li>
<li><p>Must be <strong>context-aware</strong> (relevant to process, activity, resource, object).</p></li>
<li><p>Must be <strong>timely</strong> in relation to process execution.</p></li>
<li><p>Must be <strong>feasible</strong> for automatic execution in the IS environment.</p></li>
<li><p>Must be <strong>explainable</strong> and based on transparent criteria.</p></li>
</ul>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> General framework for action-oriented process mining; Cube-based action engine.</p></li>
<li><p><strong>Methods/Levers:</strong> Continuous constraint monitoring, OLAP-based multi-dimensional analysis, automated transaction execution.</p></li>
<li><p><strong>Operational Steps / Workflow:</strong></p>

<p> 1. Define constraints from diagnostics &amp; domain knowledge.</p>

<p> 2. Monitor event streams to detect/predict violations.</p>

<p> 3. Generate constraint instances.</p>

<p> 4. Analyze violations via cube-based OLAP views.</p>

<p> 5. Map conditions to predefined management actions.</p>

<p> 6. Trigger and execute actions in source systems.</p></li>
<li><p><strong>Data &amp; Measures:</strong> Event logs (OCEL format), constraint categories, violation counts, response times, throughput measures.</p></li>
<li><p><strong>Implementation Context:</strong> ProM plug-in integrated with artificial IS and SAP ERP O2C process.</p></li>
</ul>

<blockquote>
  <p>“The action engine analyzes the constraint instances and produces action instances… automatically triggered in the underlying information system…” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“…cube-based action engine… generates actions by analyzing monitoring results in a multi-dimensional way.” (p. 9)</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Actions are explicitly defined via formulas and parameter mappings.  </p>

<p> &gt; “…the action formula specifies which transactions to generate in which conditions…” (p. 8)</p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Context dimension explicitly models processes, activities, resources, objects.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Actions mapped to executable IS transactions (SAP ERP, artificial IS).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes — Time dimension in cube; monitoring scheduled multiple times daily.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — Violations linked to transparent constraints; explainable predictions referenced.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — Actions linked to operational goals like reducing delivery failures or response delays.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Severity levels (priority), constraint categories (cost, time, quality).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Process mining diagnostics (discovery, conformance, enhancement)  </p></li>
<li><p>Predictive process monitoring  </p></li>
<li><p>OLAP multi-dimensional analysis  </p></li>
<li><p>Action recommender systems</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Number of violations detected  </p></li>
<li><p>Violation frequency ratios (e.g., &gt;10% late responses)  </p></li>
<li><p>Throughput time reduction  </p></li>
<li><p>Change frequency in orders</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of systematic action mapping in existing tools; subjective decision-making; possible unintended consequences; organizational frictions.  </p></li>
<li><p><strong>Enablers:</strong> Formalized framework; integration with ERP; cube-based structured analysis; automated execution.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions itself as extending operational support literature by formalizing the <strong>recommendation-to-action</strong> phase, which is underdeveloped compared to detect/predict phases. Builds on Celonis Action Engine but adds systematic, multi-dimensional analysis.</p>

<h2>Summary</h2>

<p>The paper defines actionability as the <strong>ability to convert process insights into timely, context-aware, feasible, and goal-aligned automated actions</strong>. It operationalizes this through a formal framework with two components: a constraint monitor (detects/predicts violations) and an action engine (maps violations to transactions). The cube-based instantiation allows structured, OLAP-style analysis of violations across dimensions (time, context, constraint). Implementation in ProM demonstrates integration with artificial and real ERP environments, showing measurable improvements in operational KPIs. The work contributes a systematic, executable link between process mining diagnostics and management actions, addressing a gap in both literature and practice.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 93 — Provides explicit conceptualization of actionability with clear features (context, timeliness, feasibility, explainability, goal alignment) integrated in framework.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Offers detailed, formalized operationalization with methods, workflow, and empirical validation in two contexts.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“[Action-oriented process mining] aims at… systematically combining process mining results and domain knowledge, and also automating management actions…” (p. 2)  </p></li>
<li><p>“The action formula specifies which transactions to generate in which conditions…” (p. 8)  </p></li>
<li><p>“The cube-based action engine… generates actions by analyzing monitoring results in a multi-dimensional way.” (p. 9)  </p></li>
<li><p>“We propose a general framework… to support continuous monitoring… and the automated execution of actions…” (p. 2)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Celonis Action Engine [41]  </p></li>
<li><p>Digital twin interface model [42]  </p></li>
<li><p>Predictive monitoring frameworks [22–26]  </p></li>
<li><p>Prescriptive alarm systems [37]  </p></li>
<li><p>Process-aware recommender systems [40]</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: A General Framework for Action-Oriented Process Mining  </p>

<p>Authors: Gyunam Park, Wil M.P. van der Aalst  </p>

<p>DOI: 10.1007/978-3-030-66498-5_16  </p>

<p>Year: 2020  </p>

<p>Publication Type: Conference Paper  </p>

<p>Discipline/Domain: Computer Science / Information Systems  </p>

<p>Subdomain/Topic: Process Mining, Action-Oriented Process Improvement  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 92  </p>

<p>Operationalization Score: 95  </p>

<p>Contains Definition of Actionability: Yes (explicit and implicit)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Partial  </p>

<p>Contains Interpretability: No  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual + Proof-of-Concept Experiment  </p>

<p>Study Context: Continuous operational process management, simulated order-handling system  </p>

<p>Geographic/Institutional Context: RWTH Aachen University (Germany)  </p>

<p>Target Users/Stakeholders: Operations managers, process analysts, decision-makers using process mining tools  </p>

<p>Primary Contribution Type: Conceptual framework + tool implementation in ProM  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Partial  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>A General Framework for Action-Oriented Process Mining  </p>

<p><strong>Authors:</strong>  </p>

<p>Gyunam Park, Wil M.P. van der Aalst  </p>

<p><strong>DOI:</strong>  </p>

<p>10.1007/978-3-030-66498-5_16  </p>

<p><strong>Year:</strong>  </p>

<p>2020  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference Paper  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Computer Science / Information Systems  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Process Mining, Action-Oriented Process Improvement  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>This work addresses the gap in process mining where insights from diagnostics are often not transformed into concrete actions. It proposes a general framework integrating continuous monitoring and automated action execution, applicable to operational process management in dynamic environments.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>RWTH Aachen University, Germany  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Operations managers, process analysts, decision-makers leveraging process mining tools  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual framework development with proof-of-concept implementation and simulated experiments  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Framework proposal + implementation in ProM + experimental evaluation  </p>

<h2>General Summary of the Paper</h2>

<p>The paper proposes a general framework for action-oriented process mining, aiming to bridge the gap between process mining diagnostics and actionable interventions. It comprises two key components: a <em>constraint monitor</em> that continuously evaluates event streams against predefined constraints, and an <em>action engine</em> that translates detected (or predicted) violations into actionable transactions for information systems. Implemented as a ProM plug-in, the framework was tested on a simulated order-handling process, showing effective detection of violations and improvement of operations through automated actions such as alerts, priority changes, and resource reassignments. The approach supports continuous process management and can handle complex, object-centric process data.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>Actionability is conceptualized as the ability to transform process diagnostics into concrete, automated actions that mitigate risks and improve operational performance in real time.  </p>

<blockquote>
  <p>“It is necessary to convert the insights from process mining diagnostics to management actions.” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“…the automated execution of actions to improve the processes based on the monitoring results (i.e., diagnostics).” (p. 3)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Linkage of diagnostics to specific operational changes  </p></li>
<li><p>Contextual relevance to ongoing process conditions  </p></li>
<li><p>Defined triggering conditions (constraints)  </p></li>
<li><p>Feasibility of execution by the information system  </p></li>
<li><p>Timeliness in detecting and acting on violations  </p></li>
<li><p>Alignment with operational goals (process improvement)</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> General Framework for Action-Oriented Process Mining  </p></li>
<li><p><strong>Methods/Levers:</strong> Continuous event stream monitoring, constraint definition, automated transaction execution  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Define constraints reflecting operational goals or rules.  </p>

<p> 2. Monitor event streams to detect/predict violations.  </p>

<p> 3. Trigger action formulas that map violations to transactions.  </p>

<p> 4. Execute transactions in the source system via an action instance stream.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Object-centric event logs, constraint evaluation results, action instance streams  </p></li>
<li><p><strong>Implementation Context:</strong> Simulated order-handling system via ProM plug-in  </p></li>
</ul>

<blockquote>
  <p>“By analyzing this constraint instance stream, the action engine assesses the necessity of actions and generates the actions ranging from process-level valves, instance-level adaptors, and alerts…” (p. 3)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – Constraints and action rules are explicitly defined in formal languages.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – Constraints tied to specific process contexts and object classes.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – Actions are executable via existing information systems.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes – Continuous monitoring and defined time windows.  </p></li>
<li><p><strong>EX (Explainability):</strong> Partial – Context of violations provided, but no deep causal modeling.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – Actions aim directly at process improvement.  </p></li>
<li><p><strong>Other Dimensions:</strong> Proactivity (predictive triggering), Scalability (enterprise-level deployment).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Object-centric process mining principles  </p></li>
<li><p>Conformance checking, Petri-net patterns, Linear Temporal Logic  </p></li>
<li><p>Predictive process monitoring literature</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Number of constraint violations detected  </p></li>
<li><p>Number of proactive actions executed  </p></li>
<li><p>Reduction in violation count over time</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of link between diagnostics and actions in traditional tools; inability to process streaming data in some commercial solutions.  </p></li>
<li><p><strong>Enablers:</strong> Formalized constraint/action languages (CFL, AFL), integration into operational systems, continuous data feeds.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>The authors position their work as advancing beyond descriptive and predictive process mining to a prescriptive, action-oriented paradigm, differentiating from tools like Celonis Action Engine by supporting streaming data and broader action contexts.</p>

<h2>Summary</h2>

<p>This paper defines actionability as the ability to continuously translate process diagnostics into automated, context-specific actions that improve operations. The proposed framework operationalizes this through a <em>constraint monitor</em> and <em>action engine</em>, enabling real-time detection and mitigation of process issues. Explicit features linked to actionability include clarity in rule definition, contextual relevance, feasibility of execution, timeliness of intervention, and alignment with operational goals. The ProM-based implementation and simulated experiments demonstrate effectiveness in reducing violations and improving process performance. The work represents a shift towards prescriptive process mining, integrating diagnostics with direct interventions.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 92 — Strong conceptual clarity, explicit linkage of features to actionability, and practical examples.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Detailed technical framework, formal definitions, and implemented proof-of-concept directly addressing “how to achieve” actionability.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“It is necessary to convert the insights from process mining diagnostics to management actions.” (p. 2)  </p></li>
<li><p>“…the automated execution of actions to improve the processes based on the monitoring results (i.e., diagnostics).” (p. 3)  </p></li>
<li><p>“If there exist more than 10 (possibly) violated items, send an e-mail to the case manager to warn for bad consequences.” (p. 5)  </p></li>
<li><p>“The constraint monitor evaluates a set of constraints… the action engine assesses the necessity of actions and generates the actions…” (p. 3)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Conformance checking [6]  </p></li>
<li><p>Petri-net patterns [7]  </p></li>
<li><p>Linear Temporal Logic [8]  </p></li>
<li><p>Predictive monitoring [3,4]  </p></li>
<li><p>Celonis Action Engine [11]  </p></li>
<li><p>Prescriptive alarm systems [13]</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Even If Explanations: Prior Work, Desiderata &amp; Benchmarks for Semi-Factual XAI  </p>

<p>Authors: Saugat Aryal, Mark T. Keane  </p>

<p>DOI: 10.24963/ijcai.2023/732  </p>

<p>Year: 2023  </p>

<p>Publication Type: Conference Paper  </p>

<p>Discipline/Domain: Artificial Intelligence, Explainable AI  </p>

<p>Subdomain/Topic: Semi-factual explanations, counterfactual reasoning, XAI benchmarking  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 92  </p>

<p>Operationalization Score: 88  </p>

<p>Contains Definition of Actionability: Yes (implicit, through desiderata)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual + Quantitative benchmarking  </p>

<p>Study Context: Survey and benchmarking of semi-factual explanation methods  </p>

<p>Geographic/Institutional Context: University College Dublin, Ireland  </p>

<p>Target Users/Stakeholders: AI researchers, XAI practitioners, policymakers, domain experts in decision-support systems  </p>

<p>Primary Contribution Type: Conceptual framework + empirical benchmarking  </p>

<p>CL: Yes — clarity is implied as important for convincingness (desiderata b, d)  </p>

<p>CR: Yes — contextual relevance via plausible/mutable/actionable changes within data manifold (desiderata c)  </p>

<p>FE: Yes — feasibility tied to plausibility and robustness of changes (desiderata f)  </p>

<p>TI: Partial — timeliness not a primary focus, but relevance in immediate interpretability  </p>

<p>EX: Yes — convincingness, surprise, and causal model change imply explainability (desiderata d, e)  </p>

<p>GA: Partial — goal alignment implied in fairness/ethical criteria (desiderata f)  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Even If Explanations: Prior Work, Desiderata &amp; Benchmarks for Semi-Factual XAI  </p>

<p><strong>Authors:</strong>  </p>

<p>Saugat Aryal, Mark T. Keane  </p>

<p><strong>DOI:</strong>  </p>

<p>10.24963/ijcai.2023/732  </p>

<p><strong>Year:</strong>  </p>

<p>2023  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference Paper  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Artificial Intelligence, Explainable AI  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Semi-factual explanations, counterfactual reasoning, XAI benchmarking  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the underexplored concept of semi-factual explanations (“even if” statements) in XAI. Unlike counterfactuals, semi-factuals describe changes to input features that do <strong>not</strong> alter the model’s decision outcome. The authors connect philosophical and psychological origins of the concept to computational implementations, propose desiderata, and benchmark historical and novel methods.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>University College Dublin, Ireland  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>AI researchers, XAI practitioners, policymakers, and domain experts in decision-support contexts such as medicine, finance, and agriculture  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual analysis and quantitative benchmarking  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Conceptual framework (desiderata), historical survey, and empirical benchmarking  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The paper surveys the philosophical, psychological, and AI literature on semi-factuals — “even if” explanations that assert changes to inputs without affecting the model’s output. Building on insights from prior work, the authors define computational and psychological desiderata for effective semi-factual explanations. They classify historical approaches, introduce a new “Most Distant Neighbor” (MDN) benchmark, and conduct a comparative study against four existing methods (three KLEOR variants and a Local Region method) across seven tabular datasets. The evaluation uses multiple metrics, including distance measures, sparsity, and proximity to decision boundaries. Results show that MDN and Local Region methods offer strong baselines, though each has trade-offs in sparsity versus distance. The paper calls for further user studies and cautions about ethical use.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is implicitly understood through the <strong>desiderata for semi-factuals</strong>, which define the properties necessary for them to be meaningful and persuasive to end-users. An actionable semi-factual must be:</p>

<ul>
<li><p>Plausible and within the data manifold</p></li>
<li><p>Sparse in changes, ideally affecting key mutable features</p></li>
<li><p>Convincing, even if counterintuitive</p></li>
<li><p>Robust and fair, avoiding misleading proxy variables  </p></li>
</ul>

<blockquote>
  <p>“The key-feature(s) changed should be plausible/mutable/actionable; that is, the SF produced by the change should be within the data-manifold.” (p. 4)  </p>
</blockquote>

<blockquote>
  <p>“If people accept SF, it will change their perception of the causal role of the key-feature(s)… causes may be updated/deleted/refined.” (p. 4)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Change to key features without altering the outcome (desiderata a)  </p></li>
<li><p>Sparse and targeted feature changes, ideally one feature (b)  </p></li>
<li><p>Plausibility and mutability within domain constraints (c)  </p></li>
<li><p>Convincingness, even if surprising (d)  </p></li>
<li><p>Ability to alter user’s causal understanding (e)  </p></li>
<li><p>Ethical robustness, avoiding proxies, maintaining domain causality, and adhering to fairness (f)  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Desiderata framework; benchmark methods including KLEOR variants, Local Region, MDN  </p></li>
<li><p><strong>Methods/Levers:</strong> Nearest unlike neighbors, feature-utility ranking, local logistic regression, most distant same-class instances  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Identify query instance and class  </p>

<p> 2. Search same-class instances meeting sparse-change and plausibility criteria  </p>

<p> 3. Rank candidates based on distance, convincingness, and domain constraints  </p>

<p> 4. Output semi-factual with maximum persuasive potential  </p></li>
<li><p><strong>Data &amp; Measures:</strong> L2 and Mahalanobis distances, kNN separation, sparsity (L0-norm)  </p></li>
<li><p><strong>Implementation Context:</strong> Benchmarked on seven binary-class tabular datasets  </p></li>
</ul>

<blockquote>
  <p>“SF will be a good explanation of Q if… diff(x, x′) with no outcome change, y = y′.” (p. 4)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — “Sparse changes to key-feature(s) … fewer is assumed to be better for psychological reasons.” (p. 4)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Plausible and within data-manifold (p. 4)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Changes must be plausible/mutable and robust (p. 4)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — Implied in providing immediate interpretability, but not explicitly stated  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — Convincingness and causal model updating (p. 4)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — Ethical and fairness constraints (p. 4)  </p></li>
<li><p><strong>Other Dimensions:</strong> Surprise/counter-intuitiveness as an explanatory asset (p. 4)  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Philosophy of conditionals (Bennett, Goodman)  </p></li>
<li><p>Psychology of counterfactual/semi-factual thinking (Byrne, McCloy)  </p></li>
<li><p>Case-Based Reasoning and Nearest Neighbor methods  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Query-to-SF distance (L2)  </p></li>
<li><p>Query-to-SF kNN percentage  </p></li>
<li><p>SF-to-query-class Mahalanobis distance  </p></li>
<li><p>SF-to-NUN distance  </p></li>
<li><p>MDN distance score  </p></li>
<li><p>Sparsity (1-, 2-, &gt;3-diff features)  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> High knowledge-engineering costs (feature-utility methods), lack of user studies, ethical risks of misleading explanations  </p></li>
<li><p><strong>Enablers:</strong> Plausible feature selection, distance-based search, benchmarking for standardized comparison  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Links psychological effects of semi-factuals to their potential in AI explanations, extending counterfactual literature with a novel operational class and providing the first systematic desiderata.</p>

<hr />

<h2>Summary</h2>

<p>This paper systematically defines and operationalizes semi-factual explanations in XAI, drawing from philosophy, psychology, and AI research. The authors establish computational and psychological desiderata that frame actionability as requiring plausibility, sparsity, convincingness, and robustness. They survey prior work, categorize historical approaches, introduce the MDN benchmark, and compare it to existing methods on multiple datasets. The results highlight trade-offs between distance and sparsity, with MDN and Local Region emerging as strong baselines. While the paper provides concrete operational metrics and a public repository, it stresses the need for user studies and ethical safeguards to ensure actionability is responsibly achieved.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 92 — Clear implicit definition of actionability through desiderata; strong linkage of features to actionability  </p></li>
<li><p><strong>Operationalization Score:</strong> 88 — Detailed benchmarking and algorithmic procedures directly tied to achieving actionability in semi-factual XAI  </p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“SF will be a good explanation of Q if… diff(x, x′) with no outcome change, y = y′.” (p. 4)  </p></li>
<li><p>“The key-feature(s) changed should be plausible/mutable/actionable; that is, the SF… should be within the data-manifold.” (p. 4)  </p></li>
<li><p>“If people accept SF, it will change their perception of the causal role of the key-feature(s)… causes may be updated/deleted/refined.” (p. 4)  </p></li>
<li><p>“For fairness and ethical reasons, the asserted differences… should not be misleading.” (p. 4)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Bennett (1982, 2003); Goodman (1947) — Philosophy of conditionals  </p></li>
<li><p>McCloy &amp; Byrne (2002); Parkinson &amp; Byrne (2017) — Psychology of semi-factual reasoning  </p></li>
<li><p>Doyle et al. (2004, 2006); Cummins &amp; Bridge (2006) — AI semi-factual algorithms  </p></li>
<li><p>Kenny &amp; Keane (2021) — GAN-based semi-factual generation  </p></li>
<li><p>Artelt &amp; Hammer (2022); Mertes et al. (2022) — Modern semi-factual applications</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Decomposing Counterfactual Explanations for Consequential Decision Making  </p>

<p>Authors: Martin Pawelczyk, Lea Tiyavorabun, Gjergji Kasneci  </p>

<p>DOI: arXiv:2211.02151  </p>

<p>Year: 2022  </p>

<p>Publication Type: Conference/Preprint (arXiv)  </p>

<p>Discipline/Domain: Machine Learning / Explainable AI  </p>

<p>Subdomain/Topic: Algorithmic Recourse, Counterfactual Explanations, Feature Dependencies  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 92  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (implicit and explicit in recourse context)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (DEAR)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual + Quantitative experiments  </p>

<p>Study Context: Automated decision-making systems (e.g., credit scoring, recidivism prediction)  </p>

<p>Geographic/Institutional Context: Not geographically bounded; datasets from U.S. contexts  </p>

<p>Target Users/Stakeholders: Affected individuals seeking recourse; developers of ML systems  </p>

<p>Primary Contribution Type: Framework + empirical evaluation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: No explicit (timeliness not discussed as requirement)  </p>

<p>EX: Partial (mechanistic explainability of direct/indirect costs)  </p>

<p>GA: Partial (goal is implicitly favorable outcome alignment)  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Decomposing Counterfactual Explanations for Consequential Decision Making  </p>

<p><strong>Authors:</strong>  </p>

<p>Martin Pawelczyk, Lea Tiyavorabun, Gjergji Kasneci  </p>

<p><strong>DOI:</strong>  </p>

<p>arXiv:2211.02151  </p>

<p><strong>Year:</strong>  </p>

<p>2022  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference/Preprint (arXiv)  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Machine Learning / Explainable AI  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Algorithmic Recourse, Counterfactual Explanations, Feature Dependencies  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses limitations of existing algorithmic recourse methods—especially their reliance on either the Independently Manipulable Feature (IMF) assumption or strong causal models—by introducing a framework that models feature dependencies without causal graphs. It focuses on consequential domains like credit scoring and criminal justice risk assessments.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Methods tested on datasets from the U.S. (Adult Income, COMPAS, Give Me Credit).  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Individuals affected by automated decisions; AI practitioners seeking to implement actionable recourse.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual framework development + empirical experiments with benchmarks.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Framework proposal (DEAR) + quantitative and qualitative evaluation.  </p>

<h2>General Summary of the Paper</h2>

<p>The authors present DEAR (DisEntangling Algorithmic Recourse), a novel framework for generating actionable counterfactual explanations in the presence of feature dependencies without relying on full causal models. DEAR separates features into directly actionable ones and those indirectly affected, using a disentangled latent representation to model dependencies. This allows realistic, low-cost recourses that remain in dense regions of the data distribution. The paper derives theoretical cost decompositions (direct vs. indirect), proposes training with Hessian penalties to enforce disentanglement, and provides an optimization procedure for minimal-cost recourse. Empirical tests on three real-world datasets show DEAR reduces costs and improves reliability compared to manifold-based and graph-based recourse methods.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as the ability to reverse unfavorable decisions by providing <strong>realistic, feasible, and low-cost changes</strong> to input features that can be implemented by the affected individual, while respecting dependencies between features.</p>

<blockquote>
  <p>“Counterfactual explanations provide a means for actionable model explanations at feature level… an instruction on how to act to arrive at a desirable outcome.” (p. 1)  </p>
</blockquote>

<blockquote>
  <p>“…generate recourses by disentangling the latent representation of co-varying features from a subset of promising recourse features to capture the main practical desiderata…” (p. 1)  </p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Adheres to <strong>feature dependencies</strong> (avoids unrealistic independence assumptions)  </p></li>
<li><p>Lies in <strong>dense regions</strong> of the data distribution  </p></li>
<li><p>Is <strong>attainable at low and controllable cost</strong> for the individual  </p></li>
<li><p>Produces <strong>interpretable direct and indirect actions</strong>  </p></li>
<li><p>Avoids reliance on strong causal assumptions  </p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name:</strong> DEAR (DisEntangling Algorithmic Recourse)  </p></li>
<li><p><strong>Methods/Levers:</strong> Disentangled latent-variable generative modeling; cost decomposition; Hessian penalty regularization; ResNet for identity mapping; constrained optimization for recourse search  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Train conditional autoencoder with disentanglement via Hessian penalty to separate direct features (xS) from latent v  </p>

<p> 2. Ensure identity mapping for xS to allow controllable direct actions  </p>

<p> 3. Optimize direct actions dS to flip prediction with minimal cost, tracking indirect changes  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Adult, COMPAS, Give Me Credit datasets; evaluation via recourse cost (<code>L1</code>), success rate (SR), constraint violations (CV), neighborhood support (YNN)  </p></li>
<li><p><strong>Implementation Context:</strong> Black-box or differentiable classifiers; tabular decision-making tasks  </p></li>
</ul>

<blockquote>
  <p>“Our framework generates recourses by disentangling the latent representation of co-varying features… to capture the main practical desiderata.” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“DEAR requires two steps: first… obtain a latent space representation v independent of xS… second… identify the nearest counterfactual.” (p. 5)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — actions are expressed in original feature space (interpretable direct actions).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — recourses adhere to actual feature dependencies.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — costs decomposed to ensure attainable low-cost changes.  </p></li>
<li><p><strong>TI (Timeliness):</strong> No explicit discussion.  </p></li>
<li><p><strong>EX (Explainability):</strong> Partial — direct/indirect cost split provides mechanistic explanation.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — implicit aim to achieve favorable classification outcome.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Reliability (success rate), proximity to data manifold.  </p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Counterfactual explanations literature (Wachter et al., causal recourse approaches)  </p></li>
<li><p>Disentangled representation learning (Hessian penalty; ResNet identity mapping)  </p></li>
<li><p>Cost decomposition into direct/indirect effects  </p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Recourse cost (<code>L1</code> norm)  </p></li>
<li><p>Success rate (SR) of flipping prediction  </p></li>
<li><p>Constraint violations (CV) for immutable features  </p></li>
<li><p>Neighborhood support (YNN) from positive-class instances  </p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong>  </p>

<p> - Strong causal assumptions in prior methods hinder practical deployment  </p>

<p> - IMF assumption yields unrealistic recommendations in dependent-feature settings  </p></li>
<li><p><strong>Enablers:</strong>  </p>

<p> - Disentanglement to reduce indirect costs  </p>

<p> - Explicit modeling of dependencies without causal graphs  </p>

<p> - Search in interpretable input space  </p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions DEAR as bridging manifold-based recourse (realistic but ignores dependencies) and causal recourse (dependency-aware but assumption-heavy), offering dependency modeling without causal graphs.</p>

<h2>Summary</h2>

<p>The paper proposes DEAR, a framework for generating actionable counterfactual explanations that handle feature dependencies without requiring causal models. It does this by disentangling direct-action features from dependent features in a generative model, enabling interpretable, low-cost recourses that stay in dense data regions. Actionability here entails feasibility, contextual realism, and interpretability. The authors provide theoretical cost decomposition, a training method enforcing disentanglement, and an optimization routine for recourse search. Experiments on real-world datasets show DEAR reduces median costs by up to 50% and achieves perfect success rates, outperforming both manifold-based and graph-based baselines.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 92 — Strong conceptualization of actionability in recourse setting, explicit criteria (dependencies, realism, cost), clear link between features and actionability.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Detailed algorithmic steps, optimization objectives, disentanglement training procedure; strong empirical validation.  </p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Counterfactual explanations provide a means for actionable model explanations at feature level… an instruction on how to act to arrive at a desirable outcome.” (p. 1)  </p></li>
<li><p>“Our framework generates recourses by disentangling the latent representation of co-varying features from a subset of promising recourse features…” (p. 2)  </p></li>
<li><p>“The framework should allow recourses to adhere to feature dependencies… lie in dense regions… ensure attainable low and controllable cost.” (p. 2)  </p></li>
<li><p>“DEAR requires two steps: first… obtain a latent space representation v independent of xS… second… identify the nearest counterfactual.” (p. 5)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Wachter et al. (2018) — IMF assumption recourse  </p></li>
<li><p>Karimi et al. (2021) — causal recourse approaches  </p></li>
<li><p>Antorán et al. (2021), Joshi et al. (2019), Pawelczyk et al. (2020) — manifold-based recourse  </p></li>
<li><p>Peebles et al. (2020) — Hessian penalty for disentanglement</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: An Actionability Assessment Tool for Explainable AI</p>

<p>Authors: Ronal Singh, Tim Miller, Liz Sonenberg, Eduardo Velloso, Frank Vetere, Piers Howe, Paul Dourish</p>

<p>DOI: arXiv:2407.09516</p>

<p>Year: 2024</p>

<p>Publication Type: Journal Article (Preprint on arXiv)</p>

<p>Discipline/Domain: Artificial Intelligence / Human-Computer Interaction</p>

<p>Subdomain/Topic: Explainable AI (XAI), Algorithmic Recourse, Human-Centred Design</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 95</p>

<p>Operationalization Score: 90</p>

<p>Contains Definition of Actionability: Yes</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Yes</p>

<p>Contains Interpretability: Partial</p>

<p>Contains Framework/Model: Yes</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Mixed Methods (Conceptual development + Empirical user studies)</p>

<p>Study Context: Credit scoring and employee turnover prediction scenarios</p>

<p>Geographic/Institutional Context: Australia (with online MTurk participants from the US)</p>

<p>Target Users/Stakeholders: AI researchers, practitioners, system designers, end-users seeking recourse</p>

<p>Primary Contribution Type: Tool development and validation</p>

<p>CL: Yes</p>

<p>CR: Yes</p>

<p>FE: Yes</p>

<p>TI: Partial</p>

<p>EX: Yes</p>

<p>GA: Yes</p>

<p>Reason if Not Eligible: N/A</p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>An Actionability Assessment Tool for Explainable AI</p>

<p><strong>Authors:</strong>  </p>

<p>Ronal Singh, Tim Miller, Liz Sonenberg, Eduardo Velloso, Frank Vetere, Piers Howe, Paul Dourish</p>

<p><strong>DOI:</strong>  </p>

<p>arXiv:2407.09516</p>

<p><strong>Year:</strong>  </p>

<p>2024</p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Article (arXiv preprint)</p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Artificial Intelligence / Human-Computer Interaction</p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable AI, Algorithmic Recourse, Actionable Explanations</p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the lack of a clear, human-centred definition and measurement of "actionability" in explainable AI (XAI), specifically for algorithmic recourse. It develops and validates a seven-question Actionability Assessment Tool inspired by frameworks from patient education, management research, and cybersecurity advice evaluation.</p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Developed by Australian researchers (CSIRO, University of Queensland, University of Melbourne, University of Sydney) with experimental participants from the US via MTurk.</p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>AI system designers, XAI researchers, practitioners providing explanations for algorithmic decisions, and end-users seeking actionable guidance.</p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Mixed methods — conceptual synthesis of existing tools and empirical validation through two user studies.</p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Practical tool for assessing actionability in XAI explanations, validated via empirical studies.</p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This work introduces a seven-question Actionability Assessment Tool for Explainable AI (XAI), aiming to measure whether an explanation enables algorithmic recourse — concrete steps a user can take to change an unfavourable decision. Drawing from instruments in patient education, management research, and cybersecurity, the authors distilled five core topics: clarity, decision understanding, personal relevance, correction of misunderstandings, and actionable steps. They evaluated the tool in two domains (credit scoring and employee turnover) using three explanation types — prototypical, counterfactual, and directive. Two user studies compared human judgements of actionability with tool-based ratings, showing strong alignment. The tool effectively distinguished between explanation types, with directive explanations consistently rated as most actionable. Findings underscore the importance of clarity, contextual relevance, and explicit step-by-step guidance, while also revealing how user roles affect perceived actionability.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is defined as:  </p>

<blockquote>
  <p>“An explanation of a decision is actionable if people can use the information to identify actions to take to change the decision.” (p. 1)</p>
</blockquote>

<p>The authors emphasise a <strong>human-centred</strong> rather than purely technical definition, focusing on the recipient’s ability to derive feasible steps from the explanation.</p>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Clarity and understandability</p></li>
<li><p>Explanation of the decision’s reasoning</p></li>
<li><p>Personal relevance and contextual fit</p></li>
<li><p>Social appropriateness of recommendations</p></li>
<li><p>Ability to correct misunderstandings</p></li>
<li><p>Identification of at least one feasible action</p></li>
<li><p>Breakdown of actions into explicit steps</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Actionability Assessment Tool for XAI</p></li>
<li><p><strong>Methods/Levers:</strong> Seven-question survey instrument across five dimensions.</p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Present explanation to participant/user.  </p>

<p> 2. Rate it using Q1–Q7.  </p>

<p> 3. Analyse item-level scores rather than aggregated totals.</p></li>
<li><p><strong>Data &amp; Measures:</strong> Likert-scale ratings per question; statistical tests (Friedman, Nemenyi) for discrimination.</p></li>
<li><p><strong>Implementation Context:</strong> Tested in credit scoring and employee turnover with three explanation types.</p></li>
</ul>

<blockquote>
  <p>“The information allows me to identify at least one feasible action to achieve my desired outcome.” (Q6, p. 3)  </p>
</blockquote>

<blockquote>
  <p>“The information allows me to break down any action into explicit steps.” (Q7, p. 3)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — “The information is clear and easy to understand.” (Q1)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — “The information is relevant to my personal circumstances.” (Q3)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — “The information allows me to identify at least one feasible action…” (Q6)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — Timeliness not explicitly measured but implied via domain specificity.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — Includes decision reasoning clarity (Q2).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — Embedded in contextual relevance and social appropriateness (Q4).  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Social appropriateness, correction of misunderstandings.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Patient Education Materials Assessment Tool (PEMAT)  </p></li>
<li><p>Actionability frameworks from management research  </p></li>
<li><p>Shared decision-making instruments  </p></li>
<li><p>Cybersecurity advice evaluation frameworks</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Seven-item Likert-scale instrument  </p></li>
<li><p>Dimension-level discrimination between explanation types  </p></li>
<li><p>Median ratings per item across explanation types and contexts</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of clarity, irrelevance to user context, absence of explicit action steps, role misalignment between recipient and intended actor.  </p></li>
<li><p><strong>Enablers:</strong> Direct, step-by-step directives; personal relevance; ability to identify misunderstandings; contextual tailoring.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>The tool draws directly from validated assessment instruments in other fields, translating them into XAI. It responds to critiques that actionability claims in XAI are often intuitive and unfalsifiable.</p>

<hr />

<h2>Summary</h2>

<p>Singh et al. (2024) present the first empirically validated, human-centred tool for assessing the actionability of explanations in XAI. By synthesising prior instruments from multiple domains, they identify seven core questions across five dimensions that capture essential features of actionable information. Tested in two domains and with three explanation types, the tool discriminates effectively between levels of perceived actionability, aligning with human judgements. Directive explanations consistently rank highest, especially when providing explicit steps, while prototypical explanations rate lowest. Contextual and role-related factors influence perceived actionability, suggesting the need for domain-specific adaptations. The tool offers a systematic, falsifiable way to assess and design actionable explanations for algorithmic recourse.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Provides explicit, clear definition of actionability; identifies systematic, multi-dimensional criteria directly tied to concept; grounded in literature.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Fully operationalises actionability into a validated 7-question tool; tested in multiple contexts with statistical analysis; slight deduction as timeliness is only implicit.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“An explanation of a decision is actionable if people can use the information to identify actions to take to change the decision.” (p. 1)  </p></li>
<li><p>“The information is clear and easy to understand.” (Q1, p. 3)  </p></li>
<li><p>“The information allows me to break down any action into explicit steps.” (Q7, p. 3)  </p></li>
<li><p>“Directive explanations… clearly outlined specific steps… most actionable.” (p. 1–2)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Shoemaker et al. (2014) — PEMAT tool  </p></li>
<li><p>HakemZadeh &amp; Baba (2016) — Actionability in management research  </p></li>
<li><p>Redmiles et al. (2020) — Cybersecurity advice evaluation  </p></li>
<li><p>Scholl et al. (2011) — Shared decision-making measures  </p></li>
<li><p>Russell (2019), Singh et al. (2023) — Counterfactual and directive explanations in XAI</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: The Art and Science of Cause and Effect (Epilogue to <em>Causality: Models, Reasoning, and Inference</em>, 2nd ed.)</p>

<p>Authors: Judea Pearl</p>

<p>DOI: https://doi.org/10.1017/CBO9780511803161.014</p>

<p>Year: 2009 (lecture delivered 1996)</p>

<p>Publication Type: Book Chapter</p>

<p>Discipline/Domain: Statistics, Artificial Intelligence, Philosophy of Science</p>

<p>Subdomain/Topic: Causal Inference, Structural Models, Graphical Models</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 95</p>

<p>Operationalization Score: 95</p>

<p>Contains Definition of Actionability: Yes (framed as the ability to predict consequences under interventions)</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Yes</p>

<p>Contains Interpretability: Yes</p>

<p>Contains Framework/Model: Yes (causal diagrams, do-calculus, intervention-as-surgery model)</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Conceptual &amp; Applied Methodological</p>

<p>Study Context: General scientific reasoning across disciplines; illustrated with examples from engineering, statistics, economics, AI, epidemiology</p>

<p>Geographic/Institutional Context: UCLA Faculty Research Lecture</p>

<p>Target Users/Stakeholders: Researchers in statistics, economics, social sciences, epidemiology, AI, philosophy of science</p>

<p>Primary Contribution Type: Conceptual framework + practical tools for causal analysis</p>

<p>CL: Yes — clarity is essential to express causation in a formal language</p>

<p>CR: Yes — contextual relevance is explicitly tied to usefulness of causal models in domains</p>

<p>FE: Yes — feasibility addressed through computational tools and graphical methods</p>

<p>TI: Yes — timeliness via real-time applicability in policy analysis and epidemiology</p>

<p>EX: Yes — explainability tied to “deep understanding” and prediction under hypothetical scenarios</p>

<p>GA: Yes — goal alignment linked to ability to answer “what if” and “how to” questions for decision-making</p>

<p>Reason if Not Eligible: N/A</p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>The Art and Science of Cause and Effect  </p>

<p><strong>Authors:</strong>  </p>

<p>Judea Pearl  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1017/CBO9780511803161.014  </p>

<p><strong>Year:</strong>  </p>

<p>2009 (lecture delivered 1996)  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Book Chapter  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Statistics, Artificial Intelligence, Philosophy of Science  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Causal Inference, Structural Models, Graphical Models  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>Pearl addresses causality as a universal concern across disciplines, focusing on how to formally represent, reason about, and operationalize cause–effect relations. His examples span engineering systems, statistical inference, economic policy, epidemiology, and AI, showing how a formal “mathematical language” for causation allows scientists to design interventions and predict outcomes.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>University of California, Los Angeles (UCLA Faculty Research Lectureship Program)  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Researchers, statisticians, economists, epidemiologists, social scientists, AI practitioners, philosophers of science  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual and applied methodological exposition  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Integration of historical, philosophical, and technical perspectives into a unified operational framework for causal inference  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This epilogue presents Judea Pearl’s synthesis of centuries of debate on causality and his solution: a formal, graphical, and algebraic framework that enables prediction under hypothetical interventions. After a historical overview from ancient attributions of causality to gods, through Galileo’s mathematical empiricism and Hume’s skepticism, Pearl identifies the limitations of traditional probability theory in expressing causation. He introduces “intervention-as-surgery” on structural equations, represented in causal diagrams, to preserve autonomy of mechanisms and enable computation of causal effects. The lecture demonstrates do-calculus as an “algebra of doing,” solving practical problems such as confounding adjustment, policy evaluation, and mediating variables. Examples include randomized trials, tax policy effects, smoking–lung cancer debates, and Simpson’s paradox. The overarching message: causality is neither mystical nor subjective—it can be expressed in a precise, computable, and transferable language.  </p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Pearl frames actionability as the <strong>capacity to predict the consequences of interventions</strong>—whether natural, experimental, or hypothetical—on a system, given a causal model.</p>

<blockquote>
  <p>“The very essence of causation – the ability to predict the consequences of abnormal eventualities and new manipulations” (p. 415)  </p>
</blockquote>

<blockquote>
  <p>“Causation means predicting the consequences of such a surgery [on equations]” (p. 417)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Ability to predict outcomes under new, possibly unobserved, scenarios  </p></li>
<li><p>Representation of independent mechanisms (autonomy) to allow localized changes without altering the whole system  </p></li>
<li><p>Clear mapping from intervention to altered model (“surgery” on equations)  </p></li>
<li><p>Capability to distinguish causal from purely correlational relationships  </p></li>
<li><p>Formal language enabling precise computation and communication across studies  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Structural Causal Models (SCMs), Causal Diagrams, do-Calculus, Intervention-as-Surgery  </p></li>
<li><p><strong>Methods/Levers:</strong> Graph-based representation of causal mechanisms, algebra of interventions (do-operator), back-door/front-door criteria, graphical adjustment rules  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Represent system as a causal diagram with autonomous mechanisms  </p>

<p> 2. Specify intervention by removing/replacing mechanism(s) (surgery)  </p>

<p> 3. Use graphical criteria to identify adjustment sets or mediation paths  </p>

<p> 4. Apply do-calculus rules to transform interventional queries into observational ones when possible  </p>

<p> 5. Compute quantities from data under the transformed model  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Observational data, experimental data, and auxiliary variables (mediators, covariates)  </p></li>
<li><p><strong>Implementation Context:</strong> Demonstrated in epidemiology (smoking/cancer), economics (tax policy), and social science (Simpson’s paradox cases)  </p></li>
</ul>

<blockquote>
  <p>“Intervention amounts to a surgery on equations (guided by a diagram)” (p. 417)  </p>
</blockquote>

<blockquote>
  <p>“The door is open for deduction, and the result is given in the… rules of causal calculus” (p. 422)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — causality must be expressed in a formal, diagrammatic, and algebraic language (p. 412)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — models are tied to specific domains and interventions (p. 418–419)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — computational procedures (do-calculus, graphical tests) make implementation possible (p. 422–425)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes — applicable to real-time decision problems (e.g., policy analysis) (p. 418)  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — causal models offer “deep understanding” by predicting under hypothetical scenarios (p. 415)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — explicitly linked to “what if” and “how to” queries central to decision-making (p. 405)  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Autonomy of mechanisms; capacity for counterfactual reasoning  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Hume’s problem of induction and spurious correlations  </p></li>
<li><p>Russell’s critique of causality in physics  </p></li>
<li><p>Structural equation modeling (S. Wright)  </p></li>
<li><p>Herman Wold’s “surgery” idea in econometrics  </p></li>
<li><p>Graph theory and Bayesian networks  </p></li>
<li><p>Galileo’s “description before explanation” principle  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Identifiability of causal effect given a model and data  </p></li>
<li><p>Graphical criteria (back-door, front-door) satisfied  </p></li>
<li><p>Ability to eliminate the “do” operator from expressions using do-calculus rules  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong>  </p>

<p> - Lack of formal language for causation in mainstream statistics (p. 412)  </p>

<p> - Historical skepticism and avoidance of causal vocabulary  </p>

<p> - Endogeneity in observational data without clear intervention modeling  </p></li>
<li><p><strong>Enablers:</strong>  </p>

<p> - Adoption of causal diagrams in model specification  </p>

<p> - Computational rules for interventions (do-calculus)  </p>

<p> - Combining domain expertise with graphical structure  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Pearl contrasts his operationalization with:  </p>

<ul>
<li><p>Classical philosophy (Aristotle, Hume, Russell)  </p></li>
<li><p>Correlation-based statistics (Galton, Pearson)  </p></li>
<li><p>SEM traditions in social sciences  </p></li>
</ul>

<p>He positions causal diagrams and do-calculus as bridging the gap between probabilistic and structural approaches, unifying manipulation-based and counterfactual definitions.  </p>

<hr />

<h2>Summary</h2>

<p>Judea Pearl’s epilogue reframes causality as a fully operational, mathematically tractable concept. He defines actionability in terms of a model’s ability to predict consequences under interventions, and he provides the structural and graphical machinery to achieve it. His “intervention-as-surgery” approach, combined with the do-calculus, enables translation of interventional questions into observational data analysis, overcoming limitations of correlation-based reasoning. The framework is domain-independent, allowing application in diverse fields from epidemiology to AI, and is built to be clear, computationally feasible, and aligned with decision-making goals.  </p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Strong, explicit conceptualization of actionability and detailed enumeration of its structural features.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Fully worked-out procedural tools (graphs, algebra, examples) to implement actionability in practice.  </p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“The very essence of causation – the ability to predict the consequences of abnormal eventualities and new manipulations” (p. 415)  </p></li>
<li><p>“Causation means predicting the consequences of such a surgery” (p. 417)  </p></li>
<li><p>“Viewing causality this way explains why scientists pursue causal explanations with such zeal” (p. 415)  </p></li>
<li><p>“The door is open for deduction… rules of causal calculus” (p. 422)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Wright, S. (1920) <em>Proceedings of the National Academy of Sciences</em> — path diagrams  </p></li>
<li><p>Wold, H. (1960) — econometric intervention-as-surgery concept  </p></li>
<li><p>Galton, F.; Pearson, K. — correlation vs causation debate  </p></li>
<li><p>Russell, B. (1913); Suppes, P. — philosophical positions on causality  </p></li>
<li><p>Fisher, R.A. — randomized experiments</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI  </p>

<p>Authors: Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera  </p>

<p>DOI: 10.1016/j.inffus.2019.12.012  </p>

<p>Year: 2020  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Artificial Intelligence / Machine Learning  </p>

<p>Subdomain/Topic: Explainable Artificial Intelligence (XAI), Responsible AI  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (implicit as “explainability” and audience-specific usefulness)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (taxonomy of XAI methods; Responsible AI conceptual model)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Review / Conceptual Analysis  </p>

<p>Study Context: Broad AI/ML application domains, including critical sectors (health, finance, transport)  </p>

<p>Geographic/Institutional Context: Multi-institutional (Europe-based with international perspective)  </p>

<p>Target Users/Stakeholders: AI researchers, developers, policy-makers, domain experts, affected users  </p>

<p>Primary Contribution Type: Conceptual framework and taxonomy with operational guidelines toward Responsible AI  </p>

<p>CL: Yes – “...explanations should make the model’s functioning clear or easy to understand to the audience...” (p.7)  </p>

<p>CR: Yes – “...clarity targeted by XAI techniques...reverts on different application purposes such as trustworthiness...” (p.8)  </p>

<p>FE: Yes – Feasibility implied via implementability and robustness as necessary for practical deployment (p.3)  </p>

<p>TI: Partial – Timeliness is not a main dimension but is relevant in regulatory/audit contexts (p.8, Figure 2)  </p>

<p>EX: Yes – Explainability explicitly defined (p.7)  </p>

<p>GA: Yes – Goal alignment implied in audience-specific and purpose-driven explainability (p.7–9)  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI  </p>

<p><strong>Authors:</strong>  </p>

<p>Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera  </p>

<p><strong>DOI:</strong>  </p>

<p>10.1016/j.inffus.2019.12.012  </p>

<p><strong>Year:</strong>  </p>

<p>2020  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Artificial Intelligence / Machine Learning  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable Artificial Intelligence (XAI), Responsible AI  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper surveys and systematizes the state of research in explainable AI, especially in machine learning and deep learning, and links it to the broader concept of Responsible AI, which integrates fairness, accountability, and privacy. It addresses multi-stakeholder contexts, from regulatory agencies to domain experts and affected users.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Multi-institutional, with primary affiliations in Spain and France; international scope.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Researchers, ML engineers, policy-makers, regulatory bodies, domain experts, end users affected by AI decisions.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Review / Conceptual Analysis  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Comprehensive conceptual framework and taxonomies for XAI, linked to operational challenges and Responsible AI principles.  </p>

<h2>General Summary of the Paper</h2>

<p>This paper delivers a comprehensive review of Explainable Artificial Intelligence (XAI), defining its core concepts, clarifying terminology (interpretability, transparency, comprehensibility), and stressing the role of the audience in determining explainability. It proposes a novel audience-centric definition of XAI, outlines purposes (trustworthiness, causality, transferability, informativeness, fairness, accessibility, privacy awareness), and develops two taxonomies: one distinguishing transparent models from post-hoc techniques, and another tailored to deep learning. The authors discuss challenges, including the interpretability-performance trade-off, metric development, and integration with fairness, privacy, and accountability under Responsible AI. Operationalization examples span model simplification, feature relevance, visualization, and hybrid symbolic–subsymbolic approaches.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>The authors conceptualize actionability implicitly through “explainability” as the model’s ability to provide clear, audience-appropriate details and reasons for its functioning, enabling trust, informed decision-making, regulatory compliance, and ethical use.  </p>

<blockquote>
  <p>“Given an audience, an explainable Artificial Intelligence is one that produces details or reasons to make its functioning clear or easy to understand.” (p.7)  </p>
</blockquote>

<blockquote>
  <p>“Ease of understanding and clarity…reverts on different application purposes, such as better trustworthiness of the model’s output by the audience.” (p.8)  </p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Clarity of functioning for the intended audience  </p></li>
<li><p>Contextual relevance to stakeholder goals  </p></li>
<li><p>Feasibility and robustness in implementation  </p></li>
<li><p>Alignment with regulatory, ethical, and operational objectives  </p></li>
<li><p>Support for trustworthiness, causality analysis, and fairness audits  </p></li>
<li><p>Ability to inform decisions and provide accessible, understandable outputs  </p></li>
</ul>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Taxonomy of XAI methods; Responsible AI framework  </p></li>
<li><p><strong>Methods/Levers:</strong> Transparent model design, post-hoc techniques (model simplification, feature relevance, visualization), hybrid symbolic–subsymbolic methods  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Select model type based on interpretability needs; apply post-hoc methods if necessary; tailor explanations to audience; integrate fairness, privacy, and accountability checks  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Model parameters, feature importance scores, saliency maps, counterfactual examples, rule sets, user feedback  </p></li>
<li><p><strong>Implementation Context:</strong> Applicable across ML/DL models in sectors such as health, finance, autonomous systems  </p></li>
</ul>

<blockquote>
  <p>“XAI proposes creating…techniques that…enable humans to understand, appropriately trust, and effectively manage…” (p.3)  </p>
</blockquote>

<blockquote>
  <p>“Target audience…as the cognitive skills and pursued goal of the users…must be taken into account…” (p.6–7)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — audience-specific clarity is central (p.7)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — explanations linked to purposes like trust, compliance (p.8–9)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — tied to implementability, robustness, and meaningful variable use (p.3)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — relevant in compliance/audit timelines (p.8, Figure 2)  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — explicit definition provided (p.7)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — alignment with stakeholder goals and Responsible AI principles (p.8–9)  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Trustworthiness, causality, transferability, informativeness, confidence, fairness, accessibility, interactivity, privacy awareness  </p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>DARPA XAI definition (Gunning, 2017)  </p></li>
<li><p>Social sciences of explanation (Miller, 2019)  </p></li>
<li><p>Michalski’s concept of comprehensibility  </p></li>
<li><p>Responsible AI principles (Fairness, Accountability, Privacy)  </p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Degree of audience understanding  </p></li>
<li><p>Trustworthiness levels  </p></li>
<li><p>Feature relevance and stability metrics  </p></li>
<li><p>Model simplification degree  </p></li>
<li><p>Fairness measures (statistical parity, equalized odds)  </p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of consensus on definitions; interpretability-performance trade-off; absence of standard metrics; privacy risks in explanation; complexity of DL models  </p></li>
<li><p><strong>Enablers:</strong> Transparent model design; tailored post-hoc methods; audience-aware explanations; integration with fairness and accountability frameworks  </p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Builds on existing XAI surveys but advances an audience-centric definition and unified taxonomies; connects XAI explicitly to Responsible AI principles and privacy/data fusion contexts.  </p>

<h2>Summary</h2>

<p>This paper reframes explainability as inherently audience-dependent, situating it as a core component of actionability in AI systems. It provides a dual taxonomy of methods (transparent vs post-hoc; deep learning-specific) and links them to stakeholder goals such as trust, compliance, and fairness. The operational pathway includes selecting interpretable models where possible, applying targeted post-hoc techniques, and embedding explanations within Responsible AI frameworks that also address privacy and accountability. Actionability emerges from clarity, contextual relevance, feasibility, goal alignment, and the ability to inform decisions in a trustworthy, ethical, and auditable manner.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Clear definition, comprehensive features list, integration with broader Responsible AI context.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Detailed pathways and taxonomies for achieving explainability across model types.  </p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Given an audience, an explainable Artificial Intelligence is one that produces details or reasons to make its functioning clear or easy to understand.” (p.7)  </p></li>
<li><p>“Ease of understanding and clarity…reverts on different application purposes, such as better trustworthiness of the model’s output…” (p.8)  </p></li>
<li><p>“Target audience…must be taken into account jointly with the intelligibility and comprehensibility of the model in use.” (p.6)  </p></li>
<li><p>“XAI proposes…techniques that…enable humans to understand, appropriately trust, and effectively manage…” (p.3)  </p></li>
</ul>

<h2>Actionability References to Other</h2>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: DACE: Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization  </p>

<p>Authors: Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Hiroki Arimura  </p>

<p>DOI: 10.24963/ijcai.2020/391  </p>

<p>Year: 2020  </p>

<p>Publication Type: Conference  </p>

<p>Discipline/Domain: Artificial Intelligence, Machine Learning  </p>

<p>Subdomain/Topic: Explainable AI, Counterfactual Explanations, Optimization  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 82  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (implicit, framed as “realistic actions” in CE)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (DACE framework)  </p>

<p>Operationalization Present: Yes (MILO formulation, algorithmic steps)  </p>

<p>Primary Methodology: Conceptual + Quantitative (algorithm development and evaluation)  </p>

<p>Study Context: Post-hoc explanations for ML model decisions, focusing on financial datasets  </p>

<p>Geographic/Institutional Context: Japan (Hokkaido University, Fujitsu Laboratories, Tokyo Institute of Technology)  </p>

<p>Target Users/Stakeholders: End-users of ML systems, decision-makers in domains like finance and credit scoring  </p>

<p>Primary Contribution Type: New framework &amp; algorithm for actionable counterfactual explanations  </p>

<p>CL: Yes — “an action suggested by CE should be executable for users” (p. 2)  </p>

<p>CR: Yes — “evaluate its reality on the empirical data distribution” (p. 2)  </p>

<p>FE: Yes — “suggest an executable action for users” (p. 2)  </p>

<p>TI: Partial — timeliness not a central theme but actions are meant for decision contexts  </p>

<p>EX: Yes — cost function grounded in explainable metrics (MD, LOF)  </p>

<p>GA: Partial — goal alignment implied through user-desired outcomes  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>DACE: Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization  </p>

<p><strong>Authors:</strong>  </p>

<p>Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Hiroki Arimura  </p>

<p><strong>DOI:</strong>  </p>

<p>10.24963/ijcai.2020/391  </p>

<p><strong>Year:</strong>  </p>

<p>2020  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Artificial Intelligence, Machine Learning  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable AI, Counterfactual Explanations, Optimization  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the challenge of generating counterfactual explanations (CE) that are realistic and executable for end-users, particularly in domains like finance. Traditional CE methods often ignore feature correlations and outlier risks, producing implausible recommendations.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Japan — Hokkaido University, Fujitsu Laboratories Ltd., Tokyo Institute of Technology  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>End-users of ML systems, decision-makers (e.g., loan officers, credit applicants)  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual + Quantitative (algorithm development and comparative experiments)  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Novel framework &amp; optimization method for actionable counterfactual generation  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The paper proposes <em>Distribution-Aware Counterfactual Explanation</em> (DACE), a method for generating counterfactual actions that are both plausible and executable for users by incorporating empirical data distribution into the cost function. It introduces a new cost function combining Mahalanobis Distance (MD) to account for feature correlations and Local Outlier Factor (LOF) to avoid outlier recommendations. This cost function is optimized via a mixed-integer linear optimization (MILO) formulation that accommodates both linear and tree ensemble classifiers. Experiments on FICO and German credit datasets show that DACE produces more realistic counterfactuals than existing methods (MAD, TLPS, PCC) in terms of both MD and LOF scores, albeit at higher computational cost. Sensitivity analysis of the trade-off parameter λ demonstrates the method's flexibility in balancing plausibility and reliability.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is framed implicitly as producing <strong>realistic, executable actions</strong> that users can directly follow to change a model’s decision outcome. It is linked to plausibility in the empirical data space and avoiding unrealistic or statistically improbable changes.  </p>

<blockquote>
  <p>“The action suggested by CE should be executable for users” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“To extract realistic actions, we need to define a cost function C that considers the empirical distribution” (p. 2)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Alignment with <strong>empirical feature correlations</strong> (avoid impossible or uncorrelated changes)  </p></li>
<li><p>Avoidance of <strong>outlier regions</strong> in feature space  </p></li>
<li><p>Feasibility for the user to execute  </p></li>
<li><p>Model outcome change to desired target class  </p></li>
<li><p>Preservation of plausibility given real-world constraints</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name:</strong> Distribution-Aware Counterfactual Explanation (DACE)  </p></li>
<li><p><strong>Methods/Levers:</strong> Cost function combining squared Mahalanobis Distance and k-Local Outlier Factor; λ as trade-off parameter  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Define feasible action set A per feature constraints  </p>

<p> 2. Calculate MD and LOF for candidate actions  </p>

<p> 3. Formulate optimization as MILO problem  </p>

<p> 4. Solve using MILO solvers (e.g., CPLEX)  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Feature correlations from covariance matrix; density-based outlier scores from training set  </p></li>
<li><p><strong>Implementation Context:</strong> Works with linear and tree ensemble classifiers  </p></li>
</ul>

<blockquote>
  <p>“We propose a new cost function based on the Mahalanobis’ distance… and Local Outlier Factor… to evaluate the reality of actions” (p. 3)  </p>
</blockquote>

<blockquote>
  <p>“We formulate the problem… as a mixed-integer linear optimization problem” (p. 3)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — clear, interpretable perturbation vector linked to decision change  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — grounded in empirical distribution  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — avoids unrealistic changes  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — not central, but decisions are framed in near-term contexts  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — uses explainable statistical measures  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — aligns with user’s desired prediction outcome  </p></li>
<li><p><strong>Other Dimensions:</strong> Avoidance of outliers, maintenance of feature dependencies</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Mahalanobis Distance for correlated feature space measurement  </p></li>
<li><p>Local Outlier Factor for density-based anomaly detection  </p></li>
<li><p>Mixed-Integer Linear Optimization for discrete-continuous decision problems  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Low Mahalanobis Distance (plausibility with respect to feature correlations)  </p></li>
<li><p>Low LOF (avoidance of statistically rare configurations)  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> High computation time; requirement for feature covariance and neighborhood statistics; complexity for large datasets  </p></li>
<li><p><strong>Enablers:</strong> Use of MILO solvers; tunable λ for balancing plausibility vs. feasibility  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Extends integer linear optimization CE methods (e.g., Ustun et al., 2019; Russell, 2019) to nonlinear cost functions incorporating MD and LOF. Addresses robustness and plausibility concerns highlighted in prior critiques (e.g., Laugel et al., 2019; Rudin, 2019).</p>

<hr />

<h2>Summary</h2>

<p>The DACE framework reconceptualizes actionability in counterfactual explanations as the ability to produce <strong>realistic, empirically plausible, and user-executable changes</strong>. It introduces a distribution-aware cost function combining Mahalanobis Distance for respecting feature correlations and Local Outlier Factor for avoiding outlier regions. This function is optimized through a mixed-integer linear approach compatible with linear and tree ensemble models. Experiments on credit datasets show DACE yields counterfactuals with lower MD and LOF than prior methods, suggesting more realistic recommendations. The trade-off parameter λ offers flexibility for balancing plausibility and reliability, enabling multiple viable action options. The approach’s main strength lies in tightly coupling action generation with statistical properties of the training data, ensuring generated recommendations are both actionable and grounded in observed distributions.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 82 — Strong conceptualization of actionability as realistic, distribution-grounded change; explicit link to user execution  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Fully specified computational method with steps, constraints, and implementation details  </p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“The action suggested by CE should be executable for users” (p. 2)  </p></li>
<li><p>“Evaluate its reality on the empirical data distribution” (p. 2)  </p></li>
<li><p>“We propose a new cost function based on the Mahalanobis’ distance… and Local Outlier Factor… to evaluate the reality of actions” (p. 3)  </p></li>
<li><p>“Our aim is to find an action… that minimizes the cost… subject to H(x̄ + a) = t” (p. 5)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Ustun et al., 2019 (Actionable recourse in linear classification)  </p></li>
<li><p>Russell, 2019 (Diverse coherent explanations)  </p></li>
<li><p>Ballet et al., 2019 (Imperceptible adversarial attacks)  </p></li>
<li><p>Laugel et al., 2019 (Connectedness and proximity in CE)  </p></li>
<li><p>Rudin, 2019 (Critique of post-hoc explanations)</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Explainable AI: A Review of Machine Learning Interpretability Methods  </p>

<p>Authors: Pantelis Linardatos, Vasilis Papastefanopoulos, Sotiris Kotsiantis  </p>

<p>DOI: https://doi.org/10.3390/e23010018  </p>

<p>Year: 2021  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Artificial Intelligence, Machine Learning  </p>

<p>Subdomain/Topic: Explainable AI (XAI), Interpretability Methods  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 78  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: No (focuses on interpretability/explainability, not "actionability" per se)  </p>

<p>Contains Systematic Features/Dimensions: Yes (criteria, taxonomies, method categories)  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (taxonomy of interpretability methods)  </p>

<p>Operationalization Present: Yes (taxonomy + method-by-method review with applicability guidance)  </p>

<p>Primary Methodology: Literature Review  </p>

<p>Study Context: Survey of ML interpretability methods across data types, algorithms, and use cases  </p>

<p>Geographic/Institutional Context: University of Patras, Greece  </p>

<p>Target Users/Stakeholders: ML practitioners, researchers, applied data scientists, policymakers in regulated domains  </p>

<p>Primary Contribution Type: Taxonomy and comparative survey of methods  </p>

<p>CL: Partial (clarity linked to interpretability but not as "actionability" dimension)  </p>

<p>CR: Yes (methods often linked to model/data context)  </p>

<p>FE: Partial (some mention of feasibility of application but not as formal dimension)  </p>

<p>TI: No (timeliness not explicitly tied to interpretability)  </p>

<p>EX: Yes (explainability as separate but related concept to interpretability)  </p>

<p>GA: Partial (alignment with goals implied in fairness and trustworthiness contexts)  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Explainable AI: A Review of Machine Learning Interpretability Methods  </p>

<p><strong>Authors:</strong>  </p>

<p>Pantelis Linardatos, Vasilis Papastefanopoulos, Sotiris Kotsiantis  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.3390/e23010018  </p>

<p><strong>Year:</strong>  </p>

<p>2021  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Artificial Intelligence, Machine Learning  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable AI (XAI), Interpretability Methods  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper surveys the growing field of Explainable AI (XAI) in response to the challenges posed by black-box machine learning models in high-stakes domains such as healthcare, finance, and autonomous systems. It focuses on categorizing interpretability methods based on model specificity, scope (local vs. global), data type, and purpose, offering detailed descriptions, limitations, and links to implementations.</p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>University of Patras, Greece  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Machine learning practitioners, researchers, data scientists, domain experts, and policymakers  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Literature Review  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Taxonomy and comparative synthesis of methods</p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This paper presents a comprehensive literature review of machine learning interpretability methods, offering a detailed taxonomy that classifies techniques by their application to specific models (model-specific vs. model-agnostic), scope (local vs. global), and purpose (explaining black-box models, creating white-box models, promoting fairness, and analyzing sensitivity). The authors differentiate between interpretability and explainability, review evaluation methods, and compare tools for various data types (tabular, image, text, graph). The review includes strengths, weaknesses, and implementation resources. While not focused on “actionability” in the decision-making sense, it operationalizes interpretability by guiding method selection for practical use cases.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>The paper does not directly define “actionability,” but implicitly links interpretability/explainability to trustworthiness, fairness, and the ability to derive meaningful, human-understandable insights that can inform decisions.  </p>

<blockquote>
  <p>“Interpretability… is the degree to which a human can understand the cause of a decision” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“Explainability… is associated with the internal logic and mechanics inside a ML system” (p. 3)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<p>Implicitly, for interpretability methods to be “usable” in decisions, they must:  </p>

<ul>
<li><p>Relate model outputs to human-understandable inputs/features  </p></li>
<li><p>Provide transparency on decision mechanisms  </p></li>
<li><p>Support evaluation of fairness and bias  </p></li>
<li><p>Offer reproducible, context-relevant explanations  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Taxonomy of Interpretability Methods  </p></li>
<li><p><strong>Methods/Levers:</strong> Categorization by model specificity, scope, and purpose; detailed method descriptions with pros/cons and applicability  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify problem constraints → Choose category (e.g., explain black-box, create white-box) → Filter by scope, model compatibility, and data type → Apply method using provided tools/resources  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Not quantitative for “actionability,” but qualitative criteria for method selection (e.g., data type, local/global scope, fairness needs)  </p></li>
<li><p><strong>Implementation Context:</strong> Works across domains where interpretability is required for trust, compliance, or stakeholder understanding  </p></li>
</ul>

<blockquote>
  <p>“This taxonomy… identifies four major categories for interpretability methods… methods for explaining complex black-box models… creating white-box models… promoting fairness… analyzing sensitivity” (p. 5)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Partial — clarity is implied through interpretability  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — method applicability linked to model/data context  </p></li>
<li><p><strong>FE (Feasibility):</strong> Partial — feasibility discussed for method choice, not as formal criterion  </p></li>
<li><p><strong>TI (Timeliness):</strong> No  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — explicitly discussed as distinct from interpretability  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — fairness and trustworthiness goals referenced  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Fairness, Sensitivity, Model scope, Data type  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Distinction between interpretability and explainability (Doshi-Velez &amp; Kim, Miller)  </p></li>
<li><p>Prior taxonomies (Gilpin et al., Adadi &amp; Berrada, Guidotti et al.)  </p></li>
<li><p>Fairness frameworks (Hardt et al.)  </p></li>
<li><p>Sensitivity analysis foundations (Sobol, Saltelli)  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<p>No formal “actionability” metrics; evaluation focuses on interpretability quality via:  </p>

<ul>
<li><p>Application-grounded, human-grounded, functionally-grounded evaluation (Doshi-Velez &amp; Kim)  </p></li>
<li><p>Fairness measures (e.g., disparate impact, equalized odds)  </p></li>
<li><p>Sensitivity indices  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of formal definitions, context-specific constraints, model complexity, limited generalizability across domains  </p></li>
<li><p><strong>Enablers:</strong> Availability of open-source tools, clear taxonomies, method-model-data mapping  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Positions itself as more comprehensive than prior surveys by integrating model-agnostic/specific, local/global, and data-type dimensions into a unified taxonomy. Expands beyond deep learning explainability to include fairness and sensitivity analysis as part of interpretability.</p>

<hr />

<h2>Summary</h2>

<p>While not directly about “actionability,” the paper presents a mature operationalization of interpretability that enables informed method selection for practical contexts. The taxonomy and detailed review offer guidance on aligning explanation techniques with model type, scope, and data modality, indirectly supporting actionable decision-making in high-stakes domains. The inclusion of fairness and sensitivity methods adds a governance dimension, further connecting interpretability to responsible AI practices.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 78 — Strong on conceptual clarity for interpretability/explainability; lacks explicit actionability definitions but covers enabling dimensions.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Well-developed taxonomy with implementation guidance; practical pathway for selecting and applying methods.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Interpretability… is the ability to explain or present in understandable terms to a human” (p. 2)  </p></li>
<li><p>“Explainability… is associated with the internal logic and mechanics inside a ML system” (p. 3)  </p></li>
<li><p>“This taxonomy… identifies four major categories for interpretability methods…” (p. 5)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Doshi-Velez &amp; Kim (interpretability definition, evaluation methods)  </p></li>
<li><p>Miller (interpretability as cause understanding)  </p></li>
<li><p>Gilpin et al., Adadi &amp; Berrada, Guidotti et al. (prior taxonomies)  </p></li>
<li><p>Hardt et al. (fairness framework)  </p></li>
<li><p>Sobol, Saltelli (sensitivity analysis)</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review  </p>

<p>Authors: Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan Hines, John Dickerson, Chirag Shah  </p>

<p>DOI: https://doi.org/10.1145/3677119  </p>

<p>Year: 2024  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Computer Science / Machine Learning  </p>

<p>Subdomain/Topic: Explainable AI, Counterfactual Explanations, Algorithmic Recourse  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (explicit and implicit)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (rubric of desiderata, operational frameworks)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Review  </p>

<p>Study Context: Counterfactual explanations in ML for classification, primarily tabular data  </p>

<p>Geographic/Institutional Context: Global research literature, University of Washington &amp; Arthur AI  </p>

<p>Target Users/Stakeholders: ML practitioners, policymakers, system designers, regulated industries (finance, healthcare)  </p>

<p>Primary Contribution Type: Comprehensive literature review and taxonomy with evaluation rubric  </p>

<p>CL: Yes — “An effective counterfactual only proposes small changes in the features relative to the starting point” (p. 7)  </p>

<p>CR: Yes — “Recommendation should never change immutable features... preference order among mutable features” (p. 7)  </p>

<p>FE: Yes — Feasibility implied in actionability constraints and plausibility requirements (p. 7-8)  </p>

<p>TI: Partial — Timeliness is implicit in “amortized inference” and generation time metrics but not core desideratum  </p>

<p>EX: Yes — Explainability core to the survey’s scope (p. 2-3)  </p>

<p>GA: Yes — Goal alignment implicit in actionable, realistic, user-preference-aligned changes (p. 7)  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review</p>

<p><strong>Authors:</strong>  </p>

<p>Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan Hines, John Dickerson, Chirag Shah</p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1145/3677119</p>

<p><strong>Year:</strong>  </p>

<p>2024</p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal</p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Computer Science / Machine Learning</p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable AI, Counterfactual Explanations, Algorithmic Recourse</p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper synthesizes a fast-growing body of research on counterfactual explanations (CFEs) and algorithmic recourse, particularly in supervised ML classification for tabular data. It links CFEs to legal mandates (e.g., GDPR, ECOA), fairness, and trust in high-stakes decision-making domains like finance and healthcare.</p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>University of Washington (Seattle, USA), Arthur AI (Washington DC, USA)</p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>ML researchers, practitioners, policymakers, legal analysts, and regulated industry developers</p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Comprehensive literature review (&gt;350 papers) with comparative evaluation rubric</p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Taxonomy of CFE approaches, evaluation against desiderata, identification of gaps and open challenges</p>

<h2>General Summary of the Paper</h2>

<p>This survey reviews more than 350 papers proposing algorithms for generating CFEs and recourses in machine learning. CFEs provide actionable feedback — suggesting minimal, feasible feature changes to flip a model’s decision — without requiring access to model internals. The authors define a set of key desiderata (validity, actionability, sparsity, plausibility, causality) and assess each method’s handling of them. They also catalog datasets, evaluation metrics, and methodological variants, mapping the field's evolution since the seminal 2017 work by Wachter et al. The review highlights operational considerations such as model access requirements, amortization, handling of categorical variables, and integration of user preferences, while identifying unresolved challenges like robustness, bias mitigation, privacy, and real-world deployment.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as the feasibility of user-implementable changes to achieve a desired outcome without altering immutable or legally protected attributes, and while respecting real-world causal constraints.  </p>

<blockquote>
  <p>“A recommended counterfactual should never change the immutable features… applicant might have a preference order amongst the mutable features” (p. 7)  </p>
</blockquote>

<blockquote>
  <p>“Realistic and actionable… of little use if the recommendation were to decrease age by 10 years” (p. 6)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Changes must be <strong>valid</strong> (yield the desired class outcome)</p></li>
<li><p>Must target <strong>mutable, non-sensitive</strong> features</p></li>
<li><p>Should be <strong>sparse</strong> (few changes)</p></li>
<li><p>Must be <strong>plausible</strong> and close to the data manifold</p></li>
<li><p>Should respect <strong>causal dependencies</strong> among features</p></li>
<li><p>Align with <strong>user preferences</strong> and feasibility constraints</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Wachter et al. optimization framework; FACE; CounterNet; FastAR; GAN-based generators; VAE-based recourse  </p></li>
<li><p><strong>Methods/Levers:</strong> Distance minimization, sparsity-inducing norms, causal graph constraints, manifold regularization, preference modeling  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify mutable features, solve constrained optimization problem, optionally generate multiple diverse CFEs, evaluate on metrics like proximity and validity  </p></li>
<li><p><strong>Data &amp; Measures:</strong> L1/L2 distance, manifold closeness (VAE reconstruction error, k-NN distance), causal constraint satisfaction  </p></li>
<li><p><strong>Implementation Context:</strong> Mostly tabular ML classification, but extendable to images, text, graphs  </p></li>
</ul>

<blockquote>
  <p>“arg min … subject to f(x′)=y′ … updated to take into account actionable features A” (p. 7)  </p>
</blockquote>

<blockquote>
  <p>“Adding the data manifold loss term encourages… even if path is longer” (p. 8)</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Minimal, interpretable changes as core design goal (p. 7)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Must respect feature mutability and legal context (p. 7)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Operational constraints and plausibility (p. 6-8)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — Addressed through efficiency metrics (p. 14)  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — Core motivation of CFEs (p. 2-3)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — Tailored to help users achieve desired outcomes (p. 6-7)  </p></li>
<li><p><strong>Other Dimensions:</strong> Causality, Sparsity, Diversity</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Optimization-based definition from Wachter et al. (2017)</p></li>
<li><p>Thagard’s theory of explanatory coherence</p></li>
<li><p>Structural causal models (SCM)</p></li>
<li><p>Legal frameworks (GDPR, ECOA)</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li>Validity, Proximity, Number of Features Changed, Generation Time, Diversity, Plausibility, Causal Constraint Satisfaction</li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Bias in underlying model, lack of user preference data, privacy risks from query access, model dynamics (drift), handling categorical features  </p></li>
<li><p><strong>Enablers:</strong> Amortized inference, causal modeling, manifold regularization, interactive user interfaces</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Builds on the 2017 Wachter et al. framework and extends to fairness, interpretability, and causal reasoning literature. Integrates findings from psychology and philosophy on counterfactual reasoning.</p>

<h2>Summary</h2>

<p>The paper positions counterfactual explanations as a bridge between explainability and actionable change in ML, providing end-users with specific, feasible steps to alter outcomes. Actionability is defined through mutability constraints, minimality, plausibility, and causal consistency. The authors operationalize these criteria in an evaluation rubric applied to 350+ CFE methods, categorizing them by model access, optimization strategy, amortization, and feature handling. By combining a rigorous taxonomy with practical performance metrics, the work advances both conceptual clarity and implementation guidance for CFEs, while surfacing unresolved challenges in robustness, fairness, deployment, and user interaction.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Strong explicit and implicit definition of actionability, systematic feature list, comprehensive conceptual grounding.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Multiple concrete frameworks and workflows for achieving actionability, though timeliness and deployment aspects are less fully developed.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“A recommended counterfactual should never change the immutable features… preference order amongst the mutable features” (p. 7)  </p></li>
<li><p>“It is easier… to focus on changing a few things instead of many… advice which is realistic and actionable” (p. 6)  </p></li>
<li><p>“Adding the data manifold loss term encourages… path that follows data manifold” (p. 8)  </p></li>
<li><p>“CFE applicable to black-box models… place no restrictions on model complexity” (p. 3)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Wachter et al. (2017) — Foundational optimization formulation  </p></li>
<li><p>Thagard (1989) — Explanatory coherence theory  </p></li>
<li><p>Ustun et al. (2019) — Actionable recourse  </p></li>
<li><p>Karimi et al. (2020, 2021) — Causality in recourse  </p></li>
<li><p>Binns et al. (2018), Dodge et al. (2019) — User preference studies</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Navigating explanatory multiverse through counterfactual path geometry  </p>

<p>Authors: Kacper Sokol, Edward Small, Yueqing Xuan  </p>

<p>DOI: 10.1007/s10994-025-06769-2  </p>

<p>Year: 2025  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Machine Learning, Explainable AI  </p>

<p>Subdomain/Topic: Counterfactual Explanations, Explainability, Geometry of Explanations  </p>

<p>Eligibility: Yes  </p>

<p>Overall Relevance Score: 90  </p>

<p>Operationalization Score: 80  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual and Experimental  </p>

<p>Study Context: Counterfactual Explanations in Machine Learning Models  </p>

<p>Geographic/Institutional Context: International  </p>

<p>Target Users/Stakeholders: ML Practitioners, Researchers, AI Developers, Data Scientists  </p>

<p>Primary Contribution Type: Conceptual Framework, Experimental Evaluation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> Navigating explanatory multiverse through counterfactual path geometry  </p>

<p><strong>Authors:</strong> Kacper Sokol, Edward Small, Yueqing Xuan  </p>

<p><strong>DOI:</strong> 10.1007/s10994-025-06769-2  </p>

<p><strong>Year:</strong> 2025  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Machine Learning, Explainable AI  </p>

<p><strong>Subdomain/Topic:</strong> Counterfactual Explanations, Explainability, Geometry of Explanations  </p>

<p><strong>Contextual Background:</strong> The paper introduces the "explanatory multiverse" concept to explain counterfactual reasoning in machine learning (ML). It emphasizes the geometric relation between counterfactual paths, offering a more structured and human-centered way of navigating, comparing, and reasoning about counterfactual explanations. The concept includes novel spatially-aware desiderata—such as agency, loss of opportunity, and choice complexity—that enhance the decision-making process for explainees.  </p>

<p><strong>Geographic/Institutional Context:</strong> International research collaboration  </p>

<p><strong>Target Users/Stakeholders:</strong> Machine learning practitioners, AI developers, researchers in explainability  </p>

<p><strong>Primary Methodology:</strong> Conceptual framework development, experimental evaluation on tabular and image datasets  </p>

<p><strong>Primary Contribution Type:</strong> Conceptual framework for counterfactual explainability, experimental results  </p>

<h2>General Summary of the Paper</h2>

<p>The paper presents a novel framework called "explanatory multiverse" to address the multiplicity of counterfactual explanations in machine learning. Traditional counterfactual methods often overlook the relationships between paths leading to counterfactuals. The authors propose a geometry-based approach that accounts for properties like branching, divergence, and convergence in counterfactual paths. They introduce an all-in-one metric, "opportunity potential," to quantify these spatial properties and enable more informed decision-making for explainees. The method is tested on various tabular and image datasets, demonstrating the flexibility and effectiveness of explanatory multiverse in providing diverse and actionable insights.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<p>Reason if Not Eligible: n/a  </p>

<h2>How Actionability is Understood</h2>

<p>The paper defines actionability in the context of counterfactual explanations as the ability for explainees to navigate and select explanations based not only on their absolute differences but also on the properties of their connecting paths. This gives explainees more agency by allowing them to make informed decisions based on the structure and feasibility of different counterfactual paths.  </p>

<blockquote>
  <p>“Explanatory multiverse enhances the actionability of counterfactuals by considering the geometric relationships between paths and enabling more informed decision-making” (p. 3).  </p>
</blockquote>

<blockquote>
  <p>“The method grants explainees more agency, allowing them to select counterfactuals based on the path characteristics” (p. 4).</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<p>The key factors that make counterfactual explanations actionable in this context are:</p>

<ul>
<li><p><strong>Spatial Awareness:</strong> The geometry of counterfactual paths, including branching, divergence, and convergence, enables more informed decision-making.  </p></li>
<li><p><strong>User Agency:</strong> Explainees can select paths not only based on the counterfactual's outcome but also on the characteristics of the journey, such as how many changes are involved or how quickly they diverge.  </p></li>
<li><p><strong>Choice Complexity:</strong> The framework reduces cognitive load by offering a manageable set of diverse paths, avoiding overwhelming the explainee with too many options.  </p></li>
</ul>

<blockquote>
  <p>“Actionability is achieved by offering explainees diverse counterfactual options, reducing cognitive load and empowering them to make better decisions” (p. 8).  </p>
</blockquote>

<blockquote>
  <p>“By considering the spatial relationship between counterfactual paths, we allow explainees to choose paths that suit their needs and preferences” (p. 7).</p>
</blockquote>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<p>Actionability is operationalized through the development of the explanatory multiverse framework, which includes:</p>

<ul>
<li><p><strong>Geometric Representation of Counterfactual Paths:</strong> Paths are represented as vectors in a space, with properties such as length, branching, and divergence captured and quantified.  </p></li>
<li><p><strong>Opportunity Potential Metric:</strong> A novel metric that quantifies how much a counterfactual path can contribute to reaching alternative counterfactual outcomes, balancing between length and diversity.  </p></li>
<li><p><strong>Graph-based Implementation:</strong> A practical implementation using directed graphs to model counterfactual journeys, supporting diverse data types like tabular and image data.  </p></li>
</ul>

<blockquote>
  <p>“The explanatory multiverse framework is operationalized by applying vector spaces and graph-based models to represent counterfactual paths” (p. 9).  </p>
</blockquote>

<blockquote>
  <p>“The opportunity potential metric helps prioritize counterfactual paths that provide the best balance of actionability and distance” (p. 13).</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – The framework allows explainees to clearly understand the paths available and their characteristics.  </p>

<p> &gt; “The geometry of counterfactual paths, when made clear, allows explainees to navigate their choices with greater clarity” (p. 8).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – The paths are designed to be relevant to the explainee’s specific needs and constraints.  </p>

<p> &gt; “The framework tailors counterfactual paths to the individual needs and domain-specific constraints of the explainee” (p. 7).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – The paths are feasible, considering real-world constraints and limitations in the decision-making process.  </p>

<p> &gt; “Feasibility is embedded in the method, as paths are designed to account for the real-world constraints of feature changes” (p. 7).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes – The approach supports timely decision-making by offering fast, actionable insights.  </p>

<p> &gt; “The ability to make quick decisions is facilitated by the ease of navigating through the counterfactual multiverse” (p. 8).  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – The framework makes counterfactual explanations more interpretable by considering the geometry of the paths.  </p>

<p> &gt; “Explainability is enhanced by the structured approach that allows users to visualize and compare the paths leading to different counterfactuals” (p. 7).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – The framework ensures that counterfactuals align with the explainee’s goals, offering relevant and actionable outcomes.  </p>

<p> &gt; “Paths are designed to align with the user’s goal, whether it is achieving a certain classification outcome or finding alternative strategies” (p. 8).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<p>The authors draw on the notion of "possible worlds" from philosophy (Lewis, 1973) and cognitive science, where different counterfactual scenarios are seen as alternate worlds that can be navigated. This perspective informs their approach to counterfactual reasoning and the concept of the "explanatory multiverse."  </p>

<blockquote>
  <p>“The concept of explanatory multiverse is grounded in the idea of multiple possible worlds, which provides a richer framework for counterfactual explanations” (p. 7).</p>
</blockquote>

<h2>Indicators or Metrics for Actionability</h2>

<p>The key metric for actionability is <strong>opportunity potential</strong>, which quantifies the fraction of the reference path that contributes to reaching alternative counterfactual outcomes.  </p>

<blockquote>
  <p>“Opportunity potential is the all-in-one metric that helps quantify how well a counterfactual path can contribute to achieving alternative outcomes” (p. 13).</p>
</blockquote>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Cognitive overload from too many counterfactual options, lack of clarity in path properties, and the complexity of navigating multiple paths.  </p></li>
<li><p><strong>Enablers:</strong> Spatially-aware counterfactual paths, the ability to prioritize based on agency and feasibility, and the use of interactive explainability tools.  </p></li>
</ul>

<blockquote>
  <p>“By considering the geometry of counterfactual paths, we reduce cognitive overload and empower explainees with better agency” (p. 7).  </p>
</blockquote>

<blockquote>
  <p>“Enabling users to explore multiple paths at their own pace increases their ability to make meaningful decisions” (p. 8).</p>
</blockquote>

<h2>Relation to Existing Literature</h2>

<p>The paper positions its approach within the existing body of work on counterfactual explanations, noting how it extends traditional methods by incorporating geometric relationships and offering a more interactive and user-centered framework for explainability.  </p>

<blockquote>
  <p>“Explanatory multiverse is a step forward from current counterfactual methods, which typically ignore the relationships between different counterfactual paths” (p. 8).</p>
</blockquote>

<h2>Summary</h2>

<p>This paper introduces explanatory multiverse, a novel framework for navigating counterfactual explanations by considering the geometric relationships between counterfactual paths. It enhances actionability by giving explainees more agency, reducing cognitive overload, and providing more flexible and actionable insights. The approach is validated experimentally on various datasets, demonstrating its effectiveness and flexibility. The authors propose an all-in-one metric, opportunity potential, to quantify the benefits of their method, emphasizing the trade-off between user agency and explanation length.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 90 – The framework addresses key gaps in the current counterfactual explanation literature, offering a human-centered approach that enhances interpretability and actionability.  </p></li>
<li><p><strong>Operationalization Score:</strong> 80 – The approach is well operationalized through a novel metric and a graph-based implementation, though the methodology could be expanded further for broader applications.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“The method grants explainees more agency, allowing them to select counterfactuals based on the path characteristics” (p. 4).  </p></li>
<li><p>“Feasibility is embedded in the method, as paths are designed to account for the real-world constraints of feature changes” (p. 7).  </p></li>
<li><p>“Opportunity potential is the all-in-one metric that helps quantify how well a counterfactual path can contribute to achieving alternative outcomes” (p. 13).  </p></li>
<li><p>“The concept of explanatory multiverse is grounded in the idea of multiple possible worlds, which provides a richer framework for counterfactual explanations” (p. 7).</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Lewis, D. (1973). Counterfactuals. Harvard University Press.  </p></li>
<li><p>Sokol, K., &amp; Flach, P. (2020a). Glass-Box: Explaining AI decisions with counterfactual statements through conversation with a voice-enabled virtual assistant.  </p></li>
<li><p>van Looveren, A., &amp; Klaise, J. (2021). Interpretable counterfactual explanations guided by prototypes.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Medical-informed machine learning: integrating prior knowledge into medical decision systems  </p>

<p>Authors: Christel Sirocchi, Alessandro Bogliolo, Sara Montagna  </p>

<p>DOI: 10.1186/s12911-024-02582-4  </p>

<p>Year: 2024  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Medical Informatics, Machine Learning  </p>

<p>Subdomain/Topic: Medical Decision Support Systems, Actionable Machine Learning Models  </p>

<p>Eligibility: Yes  </p>

<p>Overall Relevance Score: 80  </p>

<p>Operationalization Score: 70  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Mixed Methods (Review and Case Study)  </p>

<p>Study Context: Medical, Healthcare Sector  </p>

<p>Geographic/Institutional Context: Italy, University of Urbino  </p>

<p>Target Users/Stakeholders: Clinicians, Healthcare Providers, Medical Researchers  </p>

<p>Primary Contribution Type: Conceptual Framework, Case Study  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> Medical-informed machine learning: integrating prior knowledge into medical decision systems  </p>

<p><strong>Authors:</strong> Christel Sirocchi, Alessandro Bogliolo, Sara Montagna  </p>

<p><strong>DOI:</strong> 10.1186/s12911-024-02582-4  </p>

<p><strong>Year:</strong> 2024  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Medical Informatics, Machine Learning  </p>

<p><strong>Subdomain/Topic:</strong> Medical Decision Support Systems, Actionable Machine Learning Models  </p>

<p><strong>Contextual Background:</strong> The paper addresses how machine learning (ML) models can be more effectively integrated with medical domain knowledge for better decision-making in clinical settings. The case study focuses on diabetes prediction, illustrating how integration at different stages of the ML pipeline enhances both performance and practical adoption in healthcare.  </p>

<p><strong>Geographic/Institutional Context:</strong> University of Urbino, Italy  </p>

<p><strong>Target Users/Stakeholders:</strong> Healthcare professionals, ML researchers in healthcare, medical decision support developers  </p>

<p><strong>Primary Methodology:</strong> Mixed Methods (Review and Case Study)  </p>

<p><strong>Primary Contribution Type:</strong> Conceptual Framework, Case Study  </p>

<h2>General Summary of the Paper</h2>

<p>The paper explores the integration of domain knowledge into machine learning models to improve the actionability of clinical decision systems. The authors argue that while ML has shown success in medical contexts, integrating clinical expertise at various stages of the ML pipeline can improve accuracy, interpretability, and compliance with medical guidelines. The case study on diabetes prediction highlights how rules, causal networks, and thresholds informed by clinical knowledge enhance model performance, particularly in data-limited scenarios.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<p>Reason if Not Eligible: n/a  </p>

<h2>How Actionability is Understood</h2>

<p>The paper defines actionability as the capacity of ML models to generate predictions that are not only accurate but also clinically relevant, interpretable, and aligned with medical protocols. It emphasizes the importance of making these predictions usable by clinicians in real-world scenarios.  </p>

<blockquote>
  <p>“The integration of medical domain knowledge throughout the ML pipeline is crucial for ensuring that predictions are not only accurate but also interpretable and aligned with clinical guidelines” (p. 5).</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<p>The authors identify several conditions necessary for an actionable ML model:</p>

<ul>
<li><p>Alignment with clinical guidelines and protocols  </p></li>
<li><p>Interpretability for healthcare practitioners  </p></li>
<li><p>Feasibility in real-world medical contexts  </p></li>
<li><p>Ensuring model decisions are both accurate and explainable  </p></li>
</ul>

<blockquote>
  <p>“Models must adhere to existing clinical protocols to ensure their acceptance in practice” (p. 3).  </p>
</blockquote>

<blockquote>
  <p>“The interpretability of the model plays a key role in gaining trust from healthcare professionals” (p. 7).</p>
</blockquote>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<p>The paper proposes operationalizing actionability by integrating medical knowledge at different stages of the ML pipeline:</p>

<ul>
<li><p><strong>Data Preprocessing:</strong> Use of expert-defined thresholds for discretizing continuous data, handling missing data with Bayesian inference based on domain knowledge.  </p></li>
<li><p><strong>Feature Engineering:</strong> Deriving composite indices or selecting features informed by clinical relevance, such as insulin sensitivity in diabetes models.  </p></li>
<li><p><strong>Model Learning:</strong> Custom loss functions penalize deviations from clinical rules, ensuring the model adheres to established medical knowledge.  </p></li>
<li><p><strong>Output Evaluation:</strong> Combining ML predictions with rule-based systems to filter out predictions inconsistent with clinical guidelines.  </p></li>
</ul>

<blockquote>
  <p>“Using a custom loss function helps improve recall, ensuring the model’s outputs are clinically relevant and accurate” (p. 12).  </p>
</blockquote>

<blockquote>
  <p>“The integration of rule-based modules alongside ML outputs increases adherence to clinical guidelines” (p. 13).</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – Clarity is explicitly linked to actionability.  </p>

<p> &gt; “Decision trees trained on discretized data provide more interpretable results, which are essential for clinical application” (p. 9).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – Actionability is directly tied to the relevance of model outcomes in the clinical context.  </p>

<p> &gt; “The model’s outcomes must align with established clinical knowledge to be actionable” (p. 4).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – Feasibility is tied to the integration of prior knowledge and the practical applicability of ML in clinical settings.  </p>

<p> &gt; “Integrating domain knowledge helps mitigate the feasibility challenges posed by limited data” (p. 7).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes – The paper mentions that actionability also depends on the timeliness of predictions in healthcare.  </p>

<p> &gt; “Timely predictions are crucial, especially in clinical settings where decisions must be made quickly” (p. 9).  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – Explainability is a key feature for ensuring model adoption in clinical practice.  </p>

<p> &gt; “Explainability is necessary to gain trust and make the model usable in real-world clinical settings” (p. 7).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – The paper explicitly states that alignment with clinical goals is critical.  </p>

<p> &gt; “Models must be aligned with healthcare goals to ensure they are actionable and integrate effectively into clinical practice” (p. 5).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<p>The authors base their conceptualization of actionability on the principles of explainable AI (XAI) and informed machine learning, integrating expert knowledge into machine learning models to improve both performance and interpretability in healthcare contexts.</p>

<h2>Indicators or Metrics for Actionability</h2>

<p>The paper does not propose explicit metrics for actionability, but it implies that models must demonstrate:</p>

<ul>
<li><p>High accuracy and recall (critical for clinical decision support)  </p></li>
<li><p>Interpretability and adherence to clinical guidelines  </p></li>
</ul>

<blockquote>
  <p>“Recall was significantly improved by integrating domain knowledge, making the model more clinically reliable” (p. 12).</p>
</blockquote>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of unified medical knowledge representation, conflicting clinical guidelines.  </p></li>
<li><p><strong>Enablers:</strong> Access to medical data, integration of expert knowledge, domain-specific custom loss functions.  </p></li>
</ul>

<blockquote>
  <p>“Barriers include inconsistencies in medical terminology, which can undermine the model’s performance” (p. 14).  </p>
</blockquote>

<blockquote>
  <p>“Enabling factors include the availability of structured domain knowledge, which facilitates knowledge integration into the ML pipeline” (p. 6).</p>
</blockquote>

<h2>Relation to Existing Literature</h2>

<p>The paper builds on existing work in informed machine learning and explainable AI, highlighting that integrating medical knowledge at every stage of the ML pipeline can overcome challenges inherent in purely data-driven models. It positions its approach within a growing body of literature that emphasizes the importance of domain knowledge integration in ML applications in healthcare.</p>

<h2>Summary</h2>

<p>The paper emphasizes the importance of integrating medical knowledge into machine learning models to ensure that they are actionable. This is achieved through methods such as using expert-defined thresholds, incorporating composite features, applying domain-specific loss functions, and combining ML with rule-based systems for output evaluation. The authors argue that these strategies improve the relevance, clarity, and interpretability of ML models, thus making them more suitable for clinical adoption.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 80 – The paper offers a well-rounded conceptualization of actionability, addressing key attributes like interpretability, relevance, and alignment with clinical goals.  </p></li>
<li><p><strong>Operationalization Score:</strong> 70 – The paper proposes clear methods for integrating domain knowledge into the ML pipeline but lacks detailed operational steps for broad implementation.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Models must adhere to existing clinical protocols to ensure their acceptance in practice” (p. 3).  </p></li>
<li><p>“The interpretability of the model plays a key role in gaining trust from healthcare professionals” (p. 7).  </p></li>
<li><p>“Using a custom loss function helps improve recall, ensuring the model’s outputs are clinically relevant and accurate” (p. 12).  </p></li>
<li><p>“Enabling factors include the availability of structured domain knowledge, which facilitates knowledge integration into the ML pipeline” (p. 6).</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Von Rueden L, Mayer S, Beckh K, Georgiev B, Giesselbach S, Heese R, et al. (2021). Informed Machine Learning: A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems. IEEE Trans Knowl Data Eng.  </p></li>
<li><p>Leiser F, Rank S, Schmidt-Kraepelin M, et al. (2023). Medical-informed machine learning: A scoping review and future research directions. Artif Intell Med.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: An Explanatory Model Steering System for Collaboration between Domain Experts and AI  </p>

<p>Authors: Aditya Bhattacharya, Simone Stumpf, Katrien Verbert  </p>

<p>DOI: https://doi.org/10.1145/3631700.3664886  </p>

<p>Year: 2024  </p>

<p>Publication Type: Conference (Adjunct Proceedings, ACM UMAP ’24)  </p>

<p>Discipline/Domain: Human-Computer Interaction (HCI) / Machine Learning (ML)  </p>

<p>Subdomain/Topic: Explainable AI (XAI), Interactive Machine Learning (IML), Human-AI Collaboration  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 88  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Implicit  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (Explanatory Model Steering System – EXMOS)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual + Experimental (user studies with healthcare experts)  </p>

<p>Study Context: AI model steering in healthcare prediction (diabetes)  </p>

<p>Geographic/Institutional Context: KU Leuven (Belgium), University of Glasgow (Scotland)  </p>

<p>Target Users/Stakeholders: Domain experts (healthcare professionals)  </p>

<p>Primary Contribution Type: System design and evaluation for human-in-the-loop AI steering  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>An Explanatory Model Steering System for Collaboration between Domain Experts and AI  </p>

<p><strong>Authors:</strong>  </p>

<p>Aditya Bhattacharya, Simone Stumpf, Katrien Verbert  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1145/3631700.3664886  </p>

<p><strong>Year:</strong>  </p>

<p>2024  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference (ACM UMAP ’24 Adjunct Proceedings)  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Human-Computer Interaction / Machine Learning  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable AI, Interactive ML, Human-AI Collaboration  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The work targets high-stakes domains, especially healthcare, where domain experts need to understand and improve AI predictions. The system, EXMOS, integrates multifaceted explanations with manual and automated data configuration, enabling experts to use their domain knowledge to steer models and address biases.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>KU Leuven (Belgium), University of Glasgow (Scotland)  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Healthcare professionals and other domain experts without deep ML expertise.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual system design + experimental evaluation (three user studies, 174 healthcare experts).  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Interactive system enabling domain expert-driven model refinement.  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The paper presents EXMOS, an <em>Explanatory Model Steering</em> system designed to enhance collaboration between domain experts and AI systems. It combines a multifaceted explanation dashboard (integrating data-centric and model-centric explanations) with manual and automated data configuration tools to fine-tune training data. The system aims to help experts identify biases, correct anomalies, and steer prediction models effectively. Evaluated in a healthcare-focused diabetes prediction use case with 174 healthcare professionals, the results underscore the value of expert involvement and the combination of explanation types for improved trust, understanding, and performance.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Implicitly, the authors frame actionability as the <strong>capacity for domain experts to meaningfully influence and improve AI model behavior through explainability and direct data intervention</strong>. Actionable factors are those that can be manipulated to change model outputs reliably and beneficially.  </p>

<blockquote>
  <p>“...obtaining important actionable and non-actionable factors” (p. 3)  </p>
</blockquote>

<blockquote>
  <p>“...steer prediction models by configuring the training data” (p. 2)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Identifiable through multifaceted explanations (data-centric + model-centric).</p></li>
<li><p>Directly modifiable in the data to affect predictions.</p></li>
<li><p>Relevant to domain goals (e.g., clinically significant features in healthcare).</p></li>
<li><p>Understandable to non-ML experts.</p></li>
<li><p>Feasible for correction or adjustment (manual or automated).  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Explanatory Model Steering System (EXMOS)  </p></li>
<li><p><strong>Methods/Levers:</strong>  </p>

<p> - Multifaceted explanations (data-centric: data quality, distributions, statistics; model-centric: SHAP importances, decision rules).</p>

<p> - Manual configuration: feature selection, filtering, guardrails.</p>

<p> - Automated configuration: issue detection, quantified impact, auto-corrections.</p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Present model explanations via dashboard.</p>

<p> 2. Domain expert inspects and identifies issues.</p>

<p> 3. Apply manual or automated configuration.</p>

<p> 4. Retrain and update explanations.</p></li>
<li><p><strong>Data &amp; Measures:</strong> Model accuracy before/after steering, data quality metrics, predictor variable distributions.</p></li>
<li><p><strong>Implementation Context:</strong> Healthcare (diabetes prediction).  </p></li>
</ul>

<blockquote>
  <p>“...manual configuration provides more control… remove corrupt, biased, or unimportant predictor variables” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“...automated configuration… identify data issues and offer potential corrections” (p. 2)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – explanations designed for understandability.  </p>

<p> &gt; “...enhancing user understandability” (p. 3)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – domain-specific, relevant features and predictors.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – manual and automated tools for implementing changes.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial – system allows immediate retraining, but timeliness not deeply explored.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – core to the system design.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial – aligns with expert goals implicitly via domain-specific features.  </p></li>
<li><p><strong>Other Dimensions:</strong> Control level (manual vs. automated), bias mitigation.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Data-centric AI principles.</p></li>
<li><p>Explainable AI theory (global explanations, SHAP, surrogate models).</p></li>
<li><p>Human-in-the-loop model steering.</p></li>
<li><p>Prior work on multifaceted explanations.</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Change in model accuracy post-configuration.</p></li>
<li><p>Data quality scores.</p></li>
<li><p>Distribution shifts in predictor variables.</p></li>
<li><p>Quantified impact of identified issues.</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lower control in automated configuration; expertise needed for interpreting explanations.  </p></li>
<li><p><strong>Enablers:</strong> Multifaceted explanations; interactive tools; retraining with feedback.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Extends work on XAI and IML by integrating multifaceted explanations with direct data configuration, drawing on Bhattacharya et al. (2024) CHI findings and data-centric AI literature.</p>

<hr />

<h2>Summary</h2>

<p>This paper introduces EXMOS, a system enabling domain experts to act on AI models through clear, relevant, and manipulable explanations. Actionability here is tied to the capacity to identify, understand, and adjust model-relevant data and features—either manually or automatically—to improve performance and alignment with domain needs. The system’s evaluation in healthcare shows the importance of combining explanation modalities and giving users control over model steering. The approach is both conceptually grounded in XAI theory and operationalized via an interactive, technically robust tool, making it a strong candidate for cross-domain application.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 88 — Strong implicit conceptualization of actionability, tied to concrete features and human-in-the-loop design.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Clear, domain-tested methods for achieving actionability via multifaceted explanations and data configuration.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“...obtaining important actionable and non-actionable factors” (p. 3)  </p></li>
<li><p>“...manual configuration provides more control… remove corrupt, biased, or unimportant predictor variables” (p. 2)  </p></li>
<li><p>“...automated configuration… identify data issues and offer potential corrections” (p. 2)  </p></li>
<li><p>“...enhancing user understandability” (p. 3)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Bhattacharya et al. (2024) – EXMOS: Multifaceted explanations and data configurations (CHI ’24)  </p></li>
<li><p>Daochen Zha et al. (2023) – Data-centric AI survey  </p></li>
<li><p>Teso &amp; Kersting (2019) – Explanatory Interactive Machine Learning  </p></li>
<li><p>Lundberg &amp; Lee (2017) – SHAP values framework</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Explanation User Interfaces: A Systematic Literature Review  </p>

<p>Authors: Eleonora Cappuccio, Andrea Esposito, Francesco Greco, Giuseppe Desolda, Rosa Lanzilotti, Salvatore Rinzivillo  </p>

<p>DOI: https://doi.org/XXXXXXX.XXXXXXX  </p>

<p>Year: 2025  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Human-Computer Interaction, Artificial Intelligence  </p>

<p>Subdomain/Topic: Explainable AI (XAI), Explanation User Interfaces (XUIs), Human-Centered AI (HCAI)  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 92  </p>

<p>Operationalization Score: 95  </p>

<p>Contains Definition of Actionability: Yes (implicit, as actionable explanations in XUIs)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (HERMES)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Systematic Literature Review  </p>

<p>Study Context: Global, multi-domain XUI design and evaluation research  </p>

<p>Geographic/Institutional Context: Various academic and industry contexts worldwide  </p>

<p>Target Users/Stakeholders: Domain experts, non-experts, AI experts, system designers  </p>

<p>Primary Contribution Type: Comprehensive SLR + Design Framework (HERMES)  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: —  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Explanation User Interfaces: A Systematic Literature Review  </p>

<p><strong>Authors:</strong>  </p>

<p>Eleonora Cappuccio, Andrea Esposito, Francesco Greco, Giuseppe Desolda, Rosa Lanzilotti, Salvatore Rinzivillo  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/XXXXXXX.XXXXXXX  </p>

<p><strong>Year:</strong>  </p>

<p>2025  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Human-Computer Interaction, Artificial Intelligence  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable AI (XAI), Explanation User Interfaces (XUIs), Human-Centered AI (HCAI)  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper synthesizes research on Explanation User Interfaces—UIs that present AI explanations to users—through a systematic review of 146 publications. It integrates algorithmic, design, and evaluation perspectives to close the gap between XAI theory and practical, human-centered deployment.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Global, with case studies and literature spanning multiple sectors.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Domain experts (e.g., clinicians, financial analysts), non-experts, AI experts, XUI designers.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Systematic Literature Review (Kitchenham protocol + PRISMA).  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Comprehensive SLR and practical design framework (HERMES).  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This SLR examines 146 studies on Explanation User Interfaces, covering design influences, XAI techniques, evaluation practices, and guiding principles. It categorizes findings by application domain, user type, data type, AI model, and explanation modality. The review identifies dominant methods (e.g., SHAP, feature importance, counterfactuals), visualization preferences (heatmaps, bar charts), and user evaluation metrics (trust, usability, workload). From this synthesis, the authors derive HERMES—a framework enabling practitioners to design, evaluate, and tailor XUIs to domain, task, and user context, with guidelines embedded in an interactive web tool.  </p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>The paper treats <em>actionability</em> as the capacity of explanations to enable users to make informed, context-relevant decisions. Explanations in XUIs should be <strong>meaningful</strong>, <strong>contextualized</strong>, and <strong>integrated into user workflows</strong>, enabling trust, understanding, and effective use.  </p>

<blockquote>
  <p>“Placing explanations together with additional contextual information enhances their relevance and interpretability…” (p. 24)  </p>
</blockquote>

<blockquote>
  <p>“Their primary concern is whether an explanation supports their decision-making process rather than simply revealing model internals.” (p. 22)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Alignment with user goals and expertise level.  </p></li>
<li><p>Clear, jargon-free communication.  </p></li>
<li><p>Contextual information supporting interpretation.  </p></li>
<li><p>Interactivity allowing exploration and “what-if” reasoning.  </p></li>
<li><p>Multi-level visualizations offering both overview and detail.  </p></li>
<li><p>Adaptability/personalization to user background and cognitive style.  </p></li>
<li><p>Trust-building through transparency, reliability indicators, and meaningful feature selection.  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> HERMES (Human-cEnteRed developMent of Explainable user interfaceS)  </p></li>
<li><p><strong>Methods/Levers:</strong> Literature-derived guidelines; filters for AI model, task, domain, user type, XAI model, and explanation modality.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify project constraints → query HERMES → receive guideline cards with references → integrate into design → evaluate using suggested methods and metrics.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> User type, domain, AI/XAI techniques, explanation modality, evaluation metrics (trust, usability, workload, helpfulness).  </p></li>
<li><p><strong>Implementation Context:</strong> Multi-domain; adaptable to expert/non-expert users in high- and low-stakes settings.  </p></li>
</ul>

<blockquote>
  <p>“HERMES… enables designers to either align their XUIs with an existing use context or explore potential design directions…” (p. 23)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — “clear, jargon-free language that adapts to the user’s context” (p. 19).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — “placing explanations together with additional contextual information enhances relevance” (p. 24).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — “adaptable to user’s expertise… without overwhelming the user” (p. 24).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — timeliness implied via integration into workflows and interactive, on-demand exploration.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — multiple explanation modalities and transparency-building techniques.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — guidelines stress aligning with user mental models and decision-making needs.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Interactivity, personalization, trust calibration, workload management.  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>DARPA XAI framework.  </p></li>
<li><p>Human-Centered Design (ISO 9241-210).  </p></li>
<li><p>Human-Centered AI (Shneiderman).  </p></li>
<li><p>Value Sensitive Design.  </p></li>
<li><p>SAFE-AI (Situation Awareness Framework).  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Trust, usability, workload, satisfaction, perceived effectiveness, helpfulness.  </p></li>
<li><p>Task performance metrics tied to explanation use.  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of co-design practices; limited transparency evaluation; generic rather than context-specific guidelines; underuse of participatory methods.  </p></li>
<li><p><strong>Enablers:</strong> Human-centered, iterative design; multimodal explanation formats; integration with domain workflows; adaptable detail level; trust calibration methods.  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Integrates and extends prior work on interactivity, transparency, and tailoring explanations to user needs. Moves beyond isolated algorithmic or HCI perspectives by linking XAI techniques, UI presentation, and user evaluation in one framework.  </p>

<hr />

<h2>Summary</h2>

<p>This SLR reframes XUI research around actionability, emphasizing that explanations must be not only technically accurate but also embedded in human workflows, personalized, and context-rich. The HERMES framework operationalizes this by providing design guidelines filtered by domain, user type, AI/XAI model, and explanation modality, plus evaluation method recommendations. Actionability here is multi-dimensional—clarity, relevance, feasibility, trust, adaptability—achieved through human-centered design, interactive exploration, and context-aware presentation. The paper’s novelty lies in unifying algorithmic, design, and evaluation insights into a practical tool that supports the deployment of effective, trustworthy XUIs.  </p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 92 — Strong implicit definition of actionability with extensive feature mapping and multi-domain validation.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 — HERMES provides concrete, adaptable design-to-evaluation workflow grounded in literature.  </p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“[XUI is] the sum of outputs of an XAI system that the user can directly interact with…” (p. 5)  </p></li>
<li><p>“Placing explanations together with additional contextual information enhances their relevance and interpretability…” (p. 24)  </p></li>
<li><p>“Their primary concern is whether an explanation supports their decision-making process rather than simply revealing model internals.” (p. 22)  </p></li>
<li><p>“HERMES… enables designers to either align their XUIs with an existing use context or explore potential design directions…” (p. 23)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>[13] Barda et al., 2020 — User-centered displays in healthcare.  </p></li>
<li><p>[72] Jansen et al., 2024 — Contextualizing explanations for low AI-literacy.  </p></li>
<li><p>[83] Kim et al., 2023 — Aligning explanations with human reasoning.  </p></li>
<li><p>[126] Okolo et al., 2024 — Accessible language for community health workers.  </p></li>
<li><p>[174] Wysocki et al., 2023 — Trust and utility in clinical decision-making.  </p></li>
<li><p>[183] Zytek et al., 2022 — Usability challenges in high-stakes AI.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models  </p>

<p>Authors: Furui Cheng, Yao Ming, Huamin Qu  </p>

<p>DOI: 10.1109/TVCG.2020.3030342  </p>

<p>Year: 2021  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Computer Science / Human-Computer Interaction / Explainable AI  </p>

<p>Subdomain/Topic: Counterfactual Explanations, Visual Analytics, Decision Support  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (explicitly framed through counterfactual explanations)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (DECE system architecture and workflow)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: System Design + Use Cases + Expert Interview  </p>

<p>Study Context: Explainable ML for decision-making tasks across domains (healthcare, finance, education)  </p>

<p>Geographic/Institutional Context: Hong Kong University of Science and Technology; Bloomberg L.P.  </p>

<p>Target Users/Stakeholders: Model developers, decision-makers, decision subjects  </p>

<p>Primary Contribution Type: Interactive Visualization System with integrated counterfactual generation &amp; subgroup analysis  </p>

<p>CL: Yes — “counterfactual explanations… tell the user how to gain the desired prediction with minimal changes to the input” (Abstract)  </p>

<p>CR: Yes — contextual constraints in counterfactual generation ensure relevance to user needs (p. 1440–1441)  </p>

<p>FE: Yes — feasibility addressed through constraints on feature changes and post-hoc validity (p. 1441)  </p>

<p>TI: Partial — timeliness not a focus, though DECE supports interactive, on-demand exploration  </p>

<p>EX: Yes — explainability central to both instance- and subgroup-level counterfactual visualizations (p. 1439)  </p>

<p>GA: Yes — users tailor explanations to specific goals via constraints/preferences (p. 1440)  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong> DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models  </p>

<p><strong>Authors:</strong> Furui Cheng, Yao Ming, Huamin Qu  </p>

<p><strong>DOI:</strong> 10.1109/TVCG.2020.3030342  </p>

<p><strong>Year:</strong> 2021  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Computer Science / Human-Computer Interaction / Explainable AI  </p>

<p><strong>Subdomain/Topic:</strong> Counterfactual Explanations, Visual Analytics, Decision Support  </p>

<p><strong>Contextual Background:</strong> Focuses on making ML model decisions interpretable and actionable for a variety of decision-making contexts (loan approval, medical diagnosis, admissions). Uses counterfactuals to bridge human interpretability and actionability.  </p>

<p><strong>Geographic/Institutional Context:</strong> Hong Kong University of Science and Technology; Bloomberg L.P.  </p>

<p><strong>Target Users/Stakeholders:</strong> Model developers, decision-makers, and decision subjects.  </p>

<p><strong>Primary Methodology:</strong> System design and implementation with three use cases and an expert interview.  </p>

<p><strong>Primary Contribution Type:</strong> Interactive visualization platform integrating counterfactual generation with subgroup-level exploratory analysis.</p>

<h2>General Summary of the Paper</h2>

<p>The paper introduces <strong>DECE</strong>, a model-agnostic visualization system that combines counterfactual explanation generation with interactive subgroup analysis to explore and understand ML decisions. Counterfactuals are framed as inherently actionable because they indicate minimal changes needed to achieve a desired outcome. DECE supports instance-level customization (constraints, preference-based adjustments) and subgroup-level comparative analysis to reveal decision boundaries, biases, and general patterns. The system is evaluated through three domain-specific use cases (healthcare, finance, education) and expert interviews with medical trainees, demonstrating its flexibility and capacity to yield actionable insights.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is defined through <strong>counterfactual explanations</strong> — minimal, feasible changes to input features that would result in a desired prediction.  </p>

<blockquote>
  <p>“A counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input” (Abstract)  </p>
</blockquote>

<blockquote>
  <p>“Counterfactual explanations aim to find a minimal change in data that ‘flips’ the model’s prediction… They provide actionable guidance to end-users in a user-friendly way” (p. 1439)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Minimal, targeted changes to features (proximity)</p></li>
<li><p>Feasibility of changes in real-world context (constraints, post-hoc validity)</p></li>
<li><p>Diversity of possible actionable paths (multiple CF examples)</p></li>
<li><p>Sparsity (few features changed for interpretability)</p></li>
<li><p>Customizability to user’s preferences and constraints</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> DECE system, R-counterfactuals method  </p></li>
<li><p><strong>Methods/Levers:</strong> Integration of DiCE framework; multi-objective optimization (validity, proximity, diversity); interactive constraint setting; rule-support counterfactuals for subgroup exploration  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Generate raw CFs → apply constraints/preferences → sparsity refinement → post-hoc validation → visualize in instance or subgroup views → refine hypotheses through R-counterfactuals  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Uses tabular classification datasets; measures validity, proximity (weighted Manhattan), diversity, sparsity  </p></li>
<li><p><strong>Implementation Context:</strong> Instance view for personal actionable guidance; table view for subgroup hypothesis testing and comparative bias analysis  </p></li>
</ul>

<blockquote>
  <p>“We want to offer diverse options (R1)… and allow them to add constraints (R2) to reflect their preferences” (p. 1440)  </p>
</blockquote>

<blockquote>
  <p>“Post-hoc validity… ensures that generated CF examples are feasible solutions in reality” (p. 1441)</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — minimal, clear changes make CFs easy to interpret (Abstract)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — constraints ensure applicability to user’s context (p. 1440–1441)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — feasibility addressed via constraints and real-world ranges (p. 1441)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — DECE supports real-time interaction, but timeliness not deeply theorized  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — visualization and explanation central (p. 1439)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — constraints allow tailoring to user’s personal or institutional goals (p. 1440)  </p></li>
<li><p><strong>Other Dimensions:</strong> Diversity, sparsity as explanation-enhancing factors</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Wachter et al.’s unconditional counterfactual explanations  </p></li>
<li><p>DiCE framework for diverse counterfactual generation  </p></li>
<li><p>Exploratory Data Analysis principles (Tukey) for subgroup hypothesis refinement</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Validity (flips prediction)</p></li>
<li><p>Proximity (minimal change)</p></li>
<li><p>Diversity (variety of actionable paths)</p></li>
<li><p>Sparsity (few features change)</p></li>
<li><p>Post-hoc validity (feasible in real-world domain constraints)</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Overwhelming complexity of unconstrained CFs; infeasible feature changes; user knowledge gaps  </p></li>
<li><p><strong>Enablers:</strong> Interactive constraints, subgroup hypothesis refinement, clear visualization of CF-feature relationships</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Builds on counterfactual explanation literature (Wachter et al., Mothilal et al.) but extends to subgroup-level exploratory analysis with integrated visualization. Contrasts with model-specific explainability tools by offering a model-agnostic, tabular-data-focused solution.</p>

<h2>Summary</h2>

<p>Cheng et al. (2021) present <strong>DECE</strong>, an interactive, model-agnostic visual analytics system integrating counterfactual explanations for actionable and explainable ML decision support. Actionability is explicitly tied to counterfactuals that specify minimal, feasible, and interpretable changes leading to desired outcomes. The system operationalizes actionability through a multi-step, user-driven workflow — generating, constraining, validating, and visualizing CF examples — and extends beyond individual instances to subgroup-level hypothesis testing using R-counterfactuals. Three diverse use cases and expert interviews show DECE’s capacity to aid decision-makers, reveal model biases, and provide tailored actionable recommendations.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Direct, explicit definition of actionability; strong conceptual framing; detailed feature set (clarity, relevance, feasibility, etc.)  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Full workflow and algorithmic approach to achieving actionable insights, integrated into a usable system</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“A counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input” (Abstract)  </p></li>
<li><p>“Counterfactual explanations aim to find a minimal change in data that ‘flips’ the model’s prediction… They provide actionable guidance to end-users” (p. 1439)  </p></li>
<li><p>“We want to offer diverse options… and allow them to add constraints… to reflect their preferences” (p. 1440)  </p></li>
<li><p>“Post-hoc validity… ensures that generated CF examples are feasible solutions in reality” (p. 1441)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Wachter et al. (2017) — Unconditional counterfactuals framework  </p></li>
<li><p>Mothilal et al. (2020) — DiCE framework for diverse CFs  </p></li>
<li><p>Ustun et al. (2019) — Actionable recourse in linear classification</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Planning for Action: The Impact of an Asthma Action Plan Decision Support Tool Integrated into an Electronic Health Record (EHR) at a Large Health Care System  </p>

<p>Authors: Lindsay Kuhn, Kelly Reeves, Yhenneko Taylor, Hazel Tapp, Andrew McWilliams, Andrew Gunter, Jeffrey Cleveland, Michael Dulin  </p>

<p>DOI: 10.3122/jabfm.2015.03.140248  </p>

<p>Year: 2015  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Healthcare, Decision Support Systems  </p>

<p>Subdomain/Topic: Asthma Management, Clinical Decision Support  </p>

<p>Eligibility: Yes  </p>

<p>Overall Relevance Score: 90  </p>

<p>Operationalization Score: 85  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Experimental and Empirical Analysis  </p>

<p>Study Context: Asthma management in large healthcare systems  </p>

<p>Geographic/Institutional Context: Carolinas HealthCare System, USA  </p>

<p>Target Users/Stakeholders: Healthcare Providers, Asthma Patients, Decision Support System Developers  </p>

<p>Primary Contribution Type: Platform implementation, outcome analysis  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> Planning for Action: The Impact of an Asthma Action Plan Decision Support Tool Integrated into an Electronic Health Record (EHR) at a Large Health Care System  </p>

<p><strong>Authors:</strong> Lindsay Kuhn, Kelly Reeves, Yhenneko Taylor, Hazel Tapp, Andrew McWilliams, Andrew Gunter, Jeffrey Cleveland, Michael Dulin  </p>

<p><strong>DOI:</strong> 10.3122/jabfm.2015.03.140248  </p>

<p><strong>Year:</strong> 2015  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Healthcare, Decision Support Systems  </p>

<p><strong>Subdomain/Topic:</strong> Asthma Management, Clinical Decision Support  </p>

<p><strong>Contextual Background:</strong> The paper evaluates the integration of an electronic asthma action plan (eAAP) decision support tool into the Electronic Health Record (EHR) at the Carolinas HealthCare System. The aim of the eAAP is to streamline asthma management by providing evidence-based recommendations and creating individualized patient handouts to improve self-management. The study explores the impact of this decision support tool on asthma outcomes, including reductions in asthma exacerbations and medication use.  </p>

<p><strong>Geographic/Institutional Context:</strong> Carolinas HealthCare System, USA  </p>

<p><strong>Target Users/Stakeholders:</strong> Healthcare providers, asthma patients, decision support system developers  </p>

<p><strong>Primary Methodology:</strong> Empirical analysis of asthma outcomes before and after eAAP receipt, using propensity score matching for control comparisons  </p>

<p><strong>Primary Contribution Type:</strong> Platform implementation, outcome analysis  </p>

<h2>General Summary of the Paper</h2>

<p>This study investigates the impact of an electronic asthma action plan (eAAP) decision support tool integrated into the Electronic Health Record (EHR) system. The tool was designed to provide real-time, guideline-based decision support to healthcare providers and create personalized patient handouts. The study measured the outcomes of patients receiving the eAAP, particularly focusing on reductions in asthma exacerbations, hospital visits, and oral steroid use. Results indicated that children who received the eAAP had significantly fewer exacerbations, fewer oral steroid prescriptions, and a lower likelihood of asthma-related emergency visits. The tool was successfully integrated across multiple practices within the healthcare system, showing promise in improving patient outcomes and asthma management.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<p>Reason if Not Eligible: n/a  </p>

<h2>How Actionability is Understood</h2>

<p>In this context, actionability refers to the ability of patients and providers to use the asthma action plan to take appropriate actions in managing asthma. The eAAP provides actionable information by offering tailored recommendations for asthma management, outlining daily medications, recognizing symptom changes, and guiding patients on when to seek medical care.  </p>

<blockquote>
  <p>“Actionable information in the eAAP is delivered through clear, individualized instructions, empowering patients to manage their asthma effectively” (p. 5).  </p>
</blockquote>

<blockquote>
  <p>“Actionability is achieved by providing patients with a structured plan that includes clear instructions on what to do in the event of worsening asthma symptoms” (p. 6).</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<p>The authors identify several key factors that contribute to making asthma management actionable:</p>

<ul>
<li><p><strong>Clear Instructions:</strong> Detailed guidance on what actions to take in different scenarios (e.g., daily medication, when to use rescue medication, when to seek urgent care).  </p></li>
<li><p><strong>Personalization:</strong> The tool tailors the plan to each patient, ensuring that the instructions are relevant to their specific asthma severity and medication requirements.  </p></li>
<li><p><strong>Ease of Use:</strong> The tool is embedded within the EHR, allowing for seamless integration into the workflow, thus reducing barriers to implementation.  </p></li>
</ul>

<blockquote>
  <p>“Actionability is facilitated when patients receive tailored, clear, and contextually relevant recommendations, which are easy to implement within their daily routines” (p. 7).  </p>
</blockquote>

<blockquote>
  <p>“The eAAP's integration into the EHR allows for a streamlined and efficient process that ensures providers can easily offer actionable recommendations” (p. 8).</p>
</blockquote>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<p>The eAAP operationalizes actionability by providing a decision support tool embedded within the EHR. This tool:</p>

<ol>
<li><p><strong>Generates Tailored Action Plans:</strong> For each patient, the eAAP creates an individualized action plan based on the latest clinical guidelines.  </p></li>
<li><p><strong>Offers Real-Time Decision Support:</strong> The system provides guideline-based recommendations at the point of care, helping providers make informed decisions.  </p></li>
<li><p><strong>Supports Patient Self-Management:</strong> It generates patient handouts that provide clear instructions on how to manage asthma symptoms and when to take specific actions.  </p></li>
</ol>

<blockquote>
  <p>“The eAAP ensures actionability by offering real-time, actionable recommendations embedded within the provider's workflow, making it easier to manage asthma care” (p. 9).  </p>
</blockquote>

<blockquote>
  <p>“Patient handouts generated by the tool are tailored to individual needs, making them practical and actionable for self-management” (p. 10).</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – The action plan provides clear instructions that are easy for patients to understand and follow.  </p>

<p> &gt; “Clarity is vital to ensure that the patient understands when and how to take specific actions in response to worsening asthma” (p. 8).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – The eAAP tailors the recommendations based on the patient’s individual asthma severity and medication needs.  </p>

<p> &gt; “Contextual relevance ensures that the action plan aligns with the patient’s specific asthma control status and history” (p. 7).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – The tool integrates seamlessly into the provider’s workflow, making it feasible for providers to deliver actionable plans.  </p>

<p> &gt; “Feasibility is ensured by embedding the tool into the EHR, making it accessible at the point of care without disrupting workflow” (p. 9).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes – The eAAP provides timely guidance that can be acted upon immediately in the event of an exacerbation.  </p>

<p> &gt; “Timeliness is critical in asthma management, and the eAAP ensures that recommendations are available when needed” (p. 9).  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – The plan is easy to understand, and the rationale behind each recommendation is clear.  </p>

<p> &gt; “Explainability is embedded in the tool's design, which provides easily understandable instructions for both patients and providers” (p. 8).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – The recommendations align with the patient’s goal of managing asthma effectively and avoiding exacerbations.  </p>

<p> &gt; “Goal alignment ensures that the action plan helps patients achieve better asthma control, reducing hospitalizations and emergency visits” (p. 7).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<p>The eAAP is grounded in evidence-based asthma management guidelines from the National Heart, Lung, and Blood Institute (NHLBI). The tool aims to streamline and simplify these guidelines for providers, ensuring that patients receive appropriate care and guidance.  </p>

<blockquote>
  <p>“The eAAP is based on evidence from the NHLBI asthma guidelines, which are integrated into the tool to provide real-time decision support” (p. 7).</p>
</blockquote>

<h2>Indicators or Metrics for Actionability</h2>

<p>The primary metrics for actionability in this study are <strong>asthma exacerbations</strong>, <strong>ED visits</strong>, and <strong>use of oral steroids</strong>. The study also tracks the <strong>frequency of eAAP usage</strong> as an indicator of how often the tool is being applied in practice.  </p>

<blockquote>
  <p>“Actionability is evaluated by tracking reductions in asthma exacerbations and hospital visits, as well as the uptake and use of the eAAP” (p. 6).</p>
</blockquote>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Provider resistance to new tools, complexity of asthma management guidelines, lack of incentives for using the eAAP.  </p></li>
<li><p><strong>Enablers:</strong> Integration of the tool into the EHR, clear and actionable recommendations, ease of use, and provider feedback during the pilot phase.  </p></li>
</ul>

<blockquote>
  <p>“Provider adoption was a key enabler of the tool’s success, but without clear incentives, its use may remain limited” (p. 8).  </p>
</blockquote>

<blockquote>
  <p>“Embedding the eAAP within the EHR ensured that it was easy to use and available at the point of care, making it more likely to be acted upon” (p. 9).</p>
</blockquote>

<h2>Relation to Existing Literature</h2>

<p>The paper builds on existing research around asthma self-management and clinical decision support tools. It addresses gaps in the literature by evaluating the real-world impact of an EHR-integrated asthma action plan on patient outcomes.  </p>

<blockquote>
  <p>“This study contributes to the literature on asthma self-management by demonstrating the impact of technology in improving asthma care and reducing exacerbations” (p. 10).</p>
</blockquote>

<h2>Summary</h2>

<p>The paper examines the impact of an electronic asthma action plan (eAAP) decision support tool integrated into the EHR. The tool provides evidence-based, individualized recommendations at the point of care and generates patient handouts to support self-management. The results suggest that children receiving the eAAP had significantly fewer asthma exacerbations, fewer oral steroid prescriptions, and reduced emergency visits. The study highlights the importance of technology in streamlining asthma care and improving patient outcomes.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 90 – The paper offers valuable insights into the impact of decision support tools on asthma management and patient outcomes.  </p></li>
<li><p><strong>Operationalization Score:</strong> 85 – The eAAP is well-implemented and evaluated in a real-world healthcare setting, though broader dissemination and impact could be further studied.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Actionable information is delivered through clear, individualized instructions, empowering patients to manage their asthma effectively” (p. 5).  </p></li>
<li><p>“The eAAP’s integration into the EHR allows for a streamlined and efficient process that ensures providers can easily offer actionable recommendations” (p. 9).  </p></li>
<li><p>“Goal alignment ensures that the action plan helps patients achieve better asthma control, reducing hospitalizations and emergency visits” (p. 7).  </p></li>
<li><p>“This study contributes to the literature on asthma self-management by demonstrating the impact of technology in improving asthma care and reducing exacerbations” (p. 10).</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>National Heart, Lung, and Blood Institute (2007). Expert Panel Report 3: Guidelines for the Diagnosis and Management of Asthma.  </p></li>
<li><p>Roberts, N., et al. (2010). Development of an Electronic Pictorial Asthma Action Plan. Patient Educ Couns.  </p></li>
<li><p>Hanson, T.K., et al. (2013). Increasing Availability to and Ascertaining Value of Asthma Action Plans. J School Health.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: On Sense Making and the Generation of Knowledge in Visual Analytics  </p>

<p>Authors: Milena Vuckovic, Johanna Schmidt  </p>

<p>DOI: 10.3390/analytics1020008  </p>

<p>Year: 2022  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Visual Analytics, Cognitive Science  </p>

<p>Subdomain/Topic: Data Visualization, Mental Models, Knowledge Generation  </p>

<p>Eligibility: Yes  </p>

<p>Overall Relevance Score: 85  </p>

<p>Operationalization Score: 80  </p>

<p>Contains Definition of Actionability: No  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual, Qualitative  </p>

<p>Study Context: Cognitive processes in interactive visual systems  </p>

<p>Geographic/Institutional Context: Austria  </p>

<p>Target Users/Stakeholders: Data Analysts, Visualization Practitioners, Cognitive Scientists  </p>

<p>Primary Contribution Type: Conceptual Framework, Cognitive Models  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> On Sense Making and the Generation of Knowledge in Visual Analytics  </p>

<p><strong>Authors:</strong> Milena Vuckovic, Johanna Schmidt  </p>

<p><strong>DOI:</strong> 10.3390/analytics1020008  </p>

<p><strong>Year:</strong> 2022  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Visual Analytics, Cognitive Science  </p>

<p><strong>Subdomain/Topic:</strong> Data Visualization, Mental Models, Knowledge Generation  </p>

<p><strong>Contextual Background:</strong> The paper explores the cognitive mechanisms behind sense-making and knowledge generation in the context of visual analytics (VA). It addresses how analysts use visualizations and interactive data systems to form mental models, which evolve through interaction with data. The paper focuses on understanding the interplay between external representations (data visualizations) and internal cognitive processes. It aims to contribute to the understanding of how interactive systems impact the formation of mental models and influence decision-making during data analysis tasks.  </p>

<p><strong>Geographic/Institutional Context:</strong> VRVis GmbH, Vienna, Austria  </p>

<p><strong>Target Users/Stakeholders:</strong> Data analysts, researchers in visual analytics and cognitive science  </p>

<p><strong>Primary Methodology:</strong> Conceptual analysis of mental models, qualitative assessment of data exploration tasks  </p>

<p><strong>Primary Contribution Type:</strong> Cognitive framework, exploration of mental model formation in data analysis  </p>

<h2>General Summary of the Paper</h2>

<p>This paper discusses the role of sense-making and cognitive processes in visual analytics, particularly focusing on how data visualization tools influence the generation of knowledge. The authors explore mental models—the cognitive imprints of data systems—formed by analysts during interactive data exploration. The paper identifies key cognitive phases involved in the data science workflow, such as discovery, integration, profiling, modeling, and reporting, and describes how these phases shape and refine mental models. Through the analysis of different visualization tools, the authors provide insights into how distinct systems (scientific, commercial, and notebook-based) affect cognitive activities and sense-making processes. The findings highlight the dynamic nature of mental models and their evolution through interaction with external visual systems.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<p>Reason if Not Eligible: n/a  </p>

<h2>How Actionability is Understood</h2>

<p>The paper does not directly address actionability in terms of user decision-making or interventions. However, it implies that actionability in visual analytics is achieved when analysts can effectively navigate and manipulate visualizations to generate meaningful insights. The cognitive models described help clarify how users interact with and make sense of data, which indirectly informs how actionable knowledge is extracted from visual systems.  </p>

<blockquote>
  <p>“The cognitive process of generating mental models from visual data systems enables analysts to make informed decisions based on visual representations and interactions” (p. 5).</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<p>The factors that contribute to actionability in visual analytics are:</p>

<ul>
<li><p><strong>Clear Visual Representations:</strong> Effective visualizations help users form accurate mental models.  </p></li>
<li><p><strong>Interactivity:</strong> The ability to interact with the data facilitates deeper engagement and clearer insights.  </p></li>
<li><p><strong>Cognitive Models:</strong> Mental models that evolve through interaction with visual systems enable users to act upon the insights derived from the data.  </p></li>
</ul>

<blockquote>
  <p>“Interactive visual systems are essential in helping users build the necessary cognitive models that drive actionable insights” (p. 6).  </p>
</blockquote>

<blockquote>
  <p>“Actionability is achieved when analysts can make sense of complex data through evolving mental models, guided by visual cues” (p. 7).</p>
</blockquote>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<p>While the paper does not explicitly define a formal process for operationalizing actionability, it suggests that actionability in visual analytics can be facilitated through:</p>

<ul>
<li><p><strong>Tool Design:</strong> The design of visualization tools should support cognitive processes by enabling the creation and refinement of mental models.  </p></li>
<li><p><strong>Iterative Interaction:</strong> Analysts’ continuous interaction with data visualizations allows for refinement of mental models, leading to actionable insights.  </p></li>
<li><p><strong>Task-Oriented Exploration:</strong> Engaging with specific tasks like data discovery, integration, and modeling helps create actionable knowledge through the evolution of mental models.  </p></li>
</ul>

<blockquote>
  <p>“Actionability is achieved through interactive tools that help analysts engage with data in an iterative, task-driven manner” (p. 6).  </p>
</blockquote>

<blockquote>
  <p>“The interaction with diverse visualization systems fosters a cycle of refining mental models, which ultimately leads to actionable insights” (p. 7).</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – The clarity of the visual representation and the mental model it generates is crucial for actionability.  </p>

<p> &gt; “Clear visual cues allow analysts to form coherent mental models, which are essential for actionable insights” (p. 5).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – The mental models formed are highly contextual, shaped by both the data and the system’s organizational structure.  </p>

<p> &gt; “The context in which the data is explored plays a significant role in shaping the mental models and actionable insights” (p. 7).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – The ease of use and interaction with the visual system influences the feasibility of generating actionable knowledge.  </p>

<p> &gt; “The usability of visualization tools directly impacts the feasibility of generating actionable insights” (p. 7).  </p></li>
<li><p><strong>TI (Timeliness):</strong> No – The paper does not focus on timeliness in decision-making or how quickly actionability can be achieved.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – The explainability of the visual system and the underlying data contributes to actionable knowledge.  </p>

<p> &gt; “Explainable visualizations help analysts understand the data, thereby making the resulting insights actionable” (p. 6).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – The mental models and visualizations must align with the user’s goals to ensure that the insights generated are actionable.  </p>

<p> &gt; “The alignment of visual tools with the analyst’s goals is essential for ensuring that the insights are actionable” (p. 6).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<p>The paper draws on several theories regarding cognitive processes, particularly in the context of sense-making and knowledge generation, such as the “cognitive collage” theory by Liu and Stasko (2010) and the “sense-making loop” by Pirolli and Card (2005). These theories inform the authors’ approach to understanding how mental models evolve during data exploration and how these models guide decision-making.  </p>

<blockquote>
  <p>“Mental models evolve iteratively through interaction with visual systems, forming a cycle of understanding that drives decision-making” (p. 6).</p>
</blockquote>

<h2>Indicators or Metrics for Actionability</h2>

<p>The paper does not present explicit metrics for actionability but suggests that actionability can be assessed through the quality of mental models and their ability to facilitate data-driven decision-making.  </p>

<blockquote>
  <p>“Actionability can be evaluated through the effectiveness of the mental models in driving informed decisions” (p. 7).</p>
</blockquote>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Complexity of data, inadequate tool design, lack of interactivity, insufficient domain knowledge.  </p></li>
<li><p><strong>Enablers:</strong> Clear visualization design, interactivity, iterative data exploration, task-oriented workflows.  </p></li>
</ul>

<blockquote>
  <p>“Barriers to actionability include poor tool design and lack of engagement with the data, while enablers include clear visualizations and the ability to interact with the data” (p. 6).  </p>
</blockquote>

<blockquote>
  <p>“Iterative engagement with the data through interactive visualizations helps refine mental models, which in turn supports actionable knowledge generation” (p. 7).</p>
</blockquote>

<h2>Relation to Existing Literature</h2>

<p>The paper builds on existing work in cognitive science and visualization, particularly in relation to how humans process and interact with data. It extends these theories by applying them to visual analytics systems and exploring how these systems support sense-making and knowledge generation.  </p>

<blockquote>
  <p>“This study extends existing cognitive theories by applying them to the context of visual analytics, showing how mental models evolve through interaction with interactive data visualizations” (p. 7).</p>
</blockquote>

<h2>Summary</h2>

<p>The paper explores the cognitive mechanisms behind sense-making and knowledge generation in visual analytics. It examines how analysts form and refine mental models through interaction with different types of visualization tools. The paper emphasizes the importance of clear, interactive visualizations in facilitating actionable insights and proposes a framework for understanding the dynamic and iterative nature of mental model formation in data exploration. The findings contribute to the broader understanding of how cognitive processes support data-driven decision-making in complex, interactive environments.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 85 – The paper offers significant insights into the cognitive aspects of visual analytics and how these impact actionable knowledge generation.  </p></li>
<li><p><strong>Operationalization Score:</strong> 80 – The paper outlines a conceptual framework but does not provide detailed operational steps for implementing actionability in practice.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Clear visual cues allow analysts to form coherent mental models, which are essential for actionable insights” (p. 5).  </p></li>
<li><p>“The context in which the data is explored plays a significant role in shaping the mental models and actionable insights” (p. 7).  </p></li>
<li><p>“Actionability can be evaluated through the effectiveness of the mental models in driving informed decisions” (p. 7).  </p></li>
<li><p>“Mental models evolve iteratively through interaction with visual systems, forming a cycle of understanding that drives decision-making” (p. 6).</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Liu, Z., &amp; Stasko, J.T. (2010). Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective. IEEE Trans. Vis. Comput. Graph.  </p></li>
<li><p>Pirolli, P., &amp; Card, S. (2005). The Sensemaking Process and Leverage Points for Analyst Technology as Identified through Cognitive Task Analysis. International Conference on Intelligence Analysis.  </p></li>
<li><p>Mayr, E., Schreder, G., Smuc, M., &amp; Windhager, F. (2016). Measuring Mental Models of Information Visualizations. Proceedings of the BELIV Workshop.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: From Data Mining to Knowledge Discovery in Databases  </p>

<p>Authors: Usama Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth  </p>

<p>DOI: n/a  </p>

<p>Year: 1996  </p>

<p>Publication Type: Journal (AI Magazine)  </p>

<p>Discipline/Domain: Computer Science / Artificial Intelligence  </p>

<p>Subdomain/Topic: Knowledge Discovery in Databases (KDD), Data Mining  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 88  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (implicit via “useful knowledge,” “valid, novel, potentially useful, and understandable patterns”)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (KDD process model)  </p>

<p>Operationalization Present: Yes (nine-step KDD process, integration of methods, evaluation criteria)  </p>

<p>Primary Methodology: Conceptual / Review  </p>

<p>Study Context: Conceptual overview with examples from science, business, and industrial applications  </p>

<p>Geographic/Institutional Context: Global, with examples from US, Europe, and international scientific collaborations  </p>

<p>Target Users/Stakeholders: Data analysts, AI researchers, domain experts, decision-makers in industry/science  </p>

<p>Primary Contribution Type: Conceptual framework and methodological guidance  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Partial  </p>

<p>TI: No  </p>

<p>EX: Yes  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>From Data Mining to Knowledge Discovery in Databases  </p>

<p><strong>Authors:</strong>  </p>

<p>Usama Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth  </p>

<p><strong>DOI:</strong>  </p>

<p>n/a  </p>

<p><strong>Year:</strong>  </p>

<p>1996  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal (AI Magazine)  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Computer Science / Artificial Intelligence  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Knowledge Discovery in Databases (KDD), Data Mining  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>This seminal article positions KDD as a multidisciplinary process to extract valid, novel, potentially useful, and understandable patterns from large data sets, connecting it to fields like statistics, machine learning, and databases. It presents definitions, methodological steps, application examples, and research challenges.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Global scope; examples include astronomy (SKICAT), marketing, finance, fraud detection, manufacturing, and telecommunications.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Data scientists, AI researchers, statisticians, business analysts, domain specialists.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual / Review  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Conceptual framework and methodological synthesis  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The authors provide a comprehensive overview of the Knowledge Discovery in Databases (KDD) field, distinguishing it from the narrower concept of data mining. They define KDD as the nontrivial process of identifying valid, novel, potentially useful, and understandable patterns in data, and outline a nine-step iterative process from understanding the domain to acting on discovered knowledge. The paper reviews major data-mining goals (prediction and description), methods (classification, clustering, regression, summarization, dependency modeling, change detection), and algorithmic components (representation, evaluation, search). It situates KDD in an interdisciplinary context, discusses practical applications, and identifies key challenges such as high dimensionality, overfitting, and integrating prior knowledge.  </p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is implicitly defined through patterns that are “valid, novel, potentially useful, and understandable” and that can lead to some benefit or inform decision-making. The notion of “interestingness” is introduced as a combination of validity, novelty, usefulness, and simplicity.  </p>

<blockquote>
  <p>“The discovered patterns should be… potentially useful, that is, lead to some benefit to the user or task.” (p. 41)  </p>
</blockquote>

<blockquote>
  <p>“An important notion, called interestingness… is usually taken as an overall measure of pattern value, combining validity, novelty, usefulness, and simplicity.” (p. 41)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Validity on new data  </p></li>
<li><p>Novelty (new to the system/user)  </p></li>
<li><p>Potential usefulness (benefit to user/task)  </p></li>
<li><p>Understandability (directly or after processing)  </p></li>
<li><p>Interestingness threshold as user/domain-specific filter  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> KDD Process Model  </p></li>
<li><p><strong>Methods/Levers:</strong> Data selection, preprocessing, transformation, data mining, interpretation/evaluation  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Nine-step process from understanding domain → selecting data → cleaning/preprocessing → reduction/projection → matching goals to methods → exploratory analysis/model selection → data mining → interpreting results → acting on knowledge  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Quantitative measures of certainty, utility, and interestingness; model evaluation criteria (accuracy, novelty, understandability)  </p></li>
<li><p><strong>Implementation Context:</strong> Applied in domains like astronomy, marketing, fraud detection, manufacturing, telecom, and sports analytics  </p></li>
</ul>

<blockquote>
  <p>“The KDD process is interactive and iterative… First is developing an understanding of the application domain… Ninth is acting on the discovered knowledge.” (p. 42)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – Understandability is necessary for knowledge to be actionable.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – Domain and user goals guide selection of knowledge.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Partial – Discussed indirectly via computational constraints and applicability to real-world systems.  </p></li>
<li><p><strong>TI (Timeliness):</strong> No – Not explicitly linked to actionability.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – Emphasis on interpretable models (e.g., decision trees vs. neural networks).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial – Goals defined from customer’s viewpoint but not elaborated as a formal dimension.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Novelty, validity, interestingness, simplicity  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Statistical inference and uncertainty quantification  </p></li>
<li><p>Machine learning and pattern recognition methods  </p></li>
<li><p>Interestingness measures from prior KDD literature (Silberschatz &amp; Tuzhilin 1995)  </p></li>
<li><p>Interdisciplinary integration with databases, AI, visualization  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Prediction accuracy on new data  </p></li>
<li><p>Utility (e.g., cost savings, efficiency gains)  </p></li>
<li><p>Novelty (relative to system/user knowledge)  </p></li>
<li><p>Simplicity/complexity measures (e.g., model size)  </p></li>
<li><p>Subjective interestingness  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<p><strong>Barriers:</strong>  </p>

<ul>
<li><p>Overfitting and spurious patterns  </p></li>
<li><p>High dimensionality and data volume  </p></li>
<li><p>Missing/noisy data  </p></li>
<li><p>Complex relationships between variables  </p></li>
<li><p>Lack of interpretability  </p></li>
</ul>

<p><strong>Enablers:</strong>  </p>

<ul>
<li><p>Prior domain knowledge integration  </p></li>
<li><p>Data cleaning and preprocessing  </p></li>
<li><p>Scalable algorithms and computational efficiency  </p></li>
<li><p>Visualization and human–computer interaction  </p></li>
<li><p>Integration with other systems (DBMS, visualization, agents)  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Positions KDD as encompassing and extending statistical and machine-learning approaches by emphasizing the entire process from raw data to actionable knowledge. Builds on prior work in interestingness measures, pattern discovery, and automated learning, while warning against “data dredging.”  </p>

<hr />

<h2>Summary</h2>

<p>This paper defines KDD as a comprehensive, iterative process for extracting valid, novel, potentially useful, and understandable patterns from data, distinguishing it from the narrower data-mining step. Actionability is framed through these four qualities, combined in an “interestingness” function that is domain- and user-specific. The authors operationalize actionability via a nine-step KDD process, emphasizing integration of domain understanding, method selection, model evaluation, and knowledge deployment. They highlight multiple dimensions important for actionability—clarity, contextual relevance, novelty, validity, simplicity, and explainability—and address barriers like overfitting, noise, and complexity. This work remains foundational in articulating a methodological framework for transforming data into actionable knowledge.  </p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 88 — Strong conceptual clarity on what makes patterns actionable (valid, novel, useful, understandable), with integration into a coherent process.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Detailed nine-step process, evaluation measures, and integration with practical application contexts.  </p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“[KDD is] the nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data.” (p. 40)  </p></li>
<li><p>“We also want patterns to be… potentially useful, that is, lead to some benefit to the user or task.” (p. 41)  </p></li>
<li><p>“An important notion, called interestingness… combines validity, novelty, usefulness, and simplicity.” (p. 41)  </p></li>
<li><p>“The KDD process is interactive and iterative… First is developing an understanding of the application domain… Ninth is acting on the discovered knowledge.” (p. 42)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Silberschatz &amp; Tuzhilin (1995) — Subjective measures of interestingness  </p></li>
<li><p>Piatetsky-Shapiro &amp; Matheus (1994) — Interestingness of deviations  </p></li>
<li><p>Hand (1994) — Statistical perspectives on data analysis  </p></li>
<li><p>Brachman &amp; Anand (1996) — Human-centered KDD process</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Explanation in Artificial Intelligence: Insights from the Social Sciences  </p>

<p>Authors: Tim Miller  </p>

<p>DOI: arXiv:1706.07269v3  </p>

<p>Year: 2018  </p>

<p>Publication Type: Journal (Preprint)  </p>

<p>Discipline/Domain: Artificial Intelligence, Social Sciences  </p>

<p>Subdomain/Topic: Explainable AI (XAI), Human-Agent Interaction  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 85  </p>

<p>Contains Definition of Actionability: Yes (implicit — reframed as explainability in XAI, with actionable aspects derived from social science principles)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual Review  </p>

<p>Study Context: Application of social science theories of explanation to design and implementation of XAI systems  </p>

<p>Geographic/Institutional Context: University of Melbourne, Australia  </p>

<p>Target Users/Stakeholders: AI researchers, designers of explainable systems, HCI practitioners, cognitive scientists  </p>

<p>Primary Contribution Type: Theoretical synthesis and design implications for XAI  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Explanation in Artificial Intelligence: Insights from the Social Sciences  </p>

<p><strong>Authors:</strong>  </p>

<p>Tim Miller  </p>

<p><strong>DOI:</strong>  </p>

<p>arXiv:1706.07269v3  </p>

<p><strong>Year:</strong>  </p>

<p>2018  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal (Preprint)  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Artificial Intelligence, Social Sciences  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable AI (XAI), Human-Agent Interaction  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper synthesizes findings from philosophy, cognitive psychology/science, and social psychology on how humans explain decisions and behaviors. It applies these to the design of explainable AI systems, focusing on everyday explanations, human-agent interaction, and the contextual, social, and cognitive biases that shape explanation effectiveness.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>School of Computing and Information Systems, University of Melbourne, Australia.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>AI researchers, system designers, HCI specialists, cognitive scientists, practitioners building explainable AI systems.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual Review  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Theoretical synthesis and design recommendations.  </p>

<h2>General Summary of the Paper</h2>

<p>This paper reviews over 250 works from social sciences to inform explainable AI (XAI) design, arguing that human explanation practices offer critical guidance for making AI outputs understandable and trustworthy. It emphasizes four major insights: explanations are contrastive, selectively biased, not primarily probabilistic, and inherently social. The work categorizes explanation as cognitive, product, and social processes, and reviews models of causality, attribution, and explanation selection/evaluation. It bridges these with XAI design needs, advocating explicit “models of self” for AI, contrastive and context-aware explanation generation, and interactive, conversational formats that align with user expectations and cognitive biases.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>In the XAI context, “actionability” is implicitly tied to providing explanations that enable users to trust, interpret, and effectively respond to AI decisions. Actionable explanations are those that are contextually relevant, cognitively accessible, socially attuned, and operationally aligned with user goals.  </p>

<blockquote>
  <p>“Explanations are not just the presentation of associations and causes… they are contextual” (p. 6)  </p>
</blockquote>

<blockquote>
  <p>“Explanations are social — they are a transfer of knowledge, presented as part of a conversation… relative to the explainer’s beliefs about the explainee’s beliefs” (p. 6)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Contrastive framing: answers “Why P rather than Q?”  </p></li>
<li><p>Selection of relevant causes over exhaustive causality  </p></li>
<li><p>Avoidance of purely statistical justification; preference for causal narratives  </p></li>
<li><p>Social alignment: tailoring to explainee’s beliefs, knowledge, and context  </p></li>
<li><p>Structuring at the right “level” of explanation (material, formal, efficient, final)  </p></li>
<li><p>Incorporation of abnormality, intentionality, and controllability as salience cues  </p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Contrastive Explanation, Model of Self, Overton’s Structure of Explanation, Malle’s Social Attribution Model, Hilton’s Conversational Model.  </p></li>
<li><p><strong>Methods/Levers:</strong> Identify fact–foil pairs; use abnormality detection; infer explainee’s knowledge state; apply cognitive biases in selection; design multi-level explanation models.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Determine explainee’s question and implicit foil  </p>

<p> 2. Identify minimal relevant causes based on contrastive differences  </p>

<p> 3. Filter through abnormality, intentionality, and goal alignment criteria  </p>

<p> 4. Present in conversational, iterative format, tailored to the explainee’s model  </p></li>
<li><p><strong>Data &amp; Measures:</strong> User knowledge models, causal chains, model abstractions, interaction logs.  </p></li>
<li><p><strong>Implementation Context:</strong> Human–agent interaction systems, decision-support tools, autonomous systems.  </p></li>
</ul>

<blockquote>
  <p>“An intelligent agent must be able to reason about its own causal model… alongside the decision-making mechanisms” (p. 22)  </p>
</blockquote>

<blockquote>
  <p>“Providing two complete explanations does not take advantage of contrastive questions” (p. 21)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — “Explanations are not just causal chains… must be interpretable by lay-users” (p. 20)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — “They are contextual… explainee cares only about a small subset” (p. 6)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Ensuring explanations are within user’s cognitive capacity, via selection and abstraction.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — Discussed in terms of interaction timing and explanation when needed.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — Entire paper centers on making AI outputs explainable through social science insights.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — Emphasis on aligning explanation with explainee’s goals and social purpose.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Abnormality, Intentionality, Functionality, Coherence, Simplicity.  </p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Aristotle’s Four Causes  </p></li>
<li><p>Halpern &amp; Pearl’s Structural Causal Models  </p></li>
<li><p>Malle’s Social Attribution Framework  </p></li>
<li><p>Hilton’s Conversational Model of Explanation  </p></li>
<li><p>Grice’s Maxims of Conversation  </p></li>
<li><p>Overton’s Structure of Scientific Explanation  </p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<p>No direct quantitative KPIs; proposes qualitative alignment metrics such as relevance, simplicity, coherence, and fit to explainee’s knowledge.</p>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Overemphasis on causal attribution over explanation; failure to infer foils; cognitive overload from exhaustive explanations; lack of social tailoring.  </p></li>
<li><p><strong>Enablers:</strong> Inferring foils; using cognitive biases constructively; interactive dialogue; models of self and other; multi-level causal modeling.  </p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions XAI as overly reliant on researcher intuition, contrasting with robust, experimentally validated social science models of explanation. Integrates philosophical, psychological, and conversational frameworks into a design-oriented synthesis.</p>

<h2>Summary</h2>

<p>Miller (2018) reframes explainability in AI as a human-agent interaction problem grounded in social science findings. Actionability, in this framing, means delivering explanations that users can comprehend, trust, and use to inform decisions, which requires contextual, contrastive, selective, and socially aware communication. The paper operationalizes these ideas via established theories (e.g., Malle’s, Hilton’s, Overton’s), proposing design steps like foil identification, abnormality detection, and conversational delivery. It advances XAI by bridging computational models with human cognitive and social processes, offering a structured path from philosophical foundations to practical implementation.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Rich, explicit linkage of social science principles to explanation as a basis for actionable AI outputs; strong conceptual grounding and comprehensive feature mapping.  </p></li>
<li><p><strong>Operationalization Score:</strong> 85 — Provides concrete design principles and procedural guidance, though lacks direct empirical evaluation of proposed XAI implementations.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Explanations are not just the presentation of associations and causes… they are contextual” (p. 6)  </p></li>
<li><p>“Explanations are social — they are a transfer of knowledge, presented as part of a conversation” (p. 6)  </p></li>
<li><p>“An intelligent agent must be able to reason about its own causal model… a model of self” (p. 22)  </p></li>
<li><p>“Providing two complete explanations does not take advantage of contrastive questions” (p. 21)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Halpern &amp; Pearl (2005) on Structural Causal Models  </p></li>
<li><p>Malle (2004) on Social Attribution  </p></li>
<li><p>Hilton (1990) on Conversational Models  </p></li>
<li><p>Grice (1975) on Maxims of Conversation  </p></li>
<li><p>Overton (2012) on Structure of Explanation  </p></li>
<li><p>Lipton (1990) on Contrastive Explanation</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Decision Support Systems: The Next Decade  </p>

<p>Authors: Peter G.W. Keen  </p>

<p>DOI: n/a  </p>

<p>Year: 1987  </p>

<p>Publication Type: Journal Article  </p>

<p>Discipline/Domain: Information Systems / Management Science  </p>

<p>Subdomain/Topic: Decision Support Systems (DSS), Actionability in Decision Support  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 88  </p>

<p>Operationalization Score: 85  </p>

<p>Contains Definition of Actionability: Yes (implicit and explicit through decision support conceptualization)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Partial  </p>

<p>Contains Framework/Model: Yes (Extended Decision Support model)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual / Position Paper  </p>

<p>Study Context: DSS research and practice globally, with examples from business, technology, and management  </p>

<p>Geographic/Institutional Context: International; references to US, Europe, Asia; author from International Center for Information Technologies, USA  </p>

<p>Target Users/Stakeholders: Senior managers, DSS developers, information systems professionals, organizational decision makers  </p>

<p>Primary Contribution Type: Conceptual framework and agenda for DSS research and practice  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Decision Support Systems: The Next Decade  </p>

<p><strong>Authors:</strong>  </p>

<p>Peter G.W. Keen  </p>

<p><strong>DOI:</strong>  </p>

<p>n/a  </p>

<p><strong>Year:</strong>  </p>

<p>1987  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Article  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Information Systems / Management Science  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Decision Support Systems (DSS), Actionability in Decision Support  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the evolution and future direction of Decision Support Systems, framing DSS as both a technological and intellectual tool to enhance organizational decision-making. It reflects on ten years of DSS development and proposes an agenda for the next decade that emphasizes decisions that “really matter,” active support, and integration of emerging technologies such as expert systems, document-based systems, and telecommunications.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>International; examples drawn from US, Europe, Asia.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Senior managers, DSS builders, information systems professionals, organizational decision-makers.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual / Position Paper  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Conceptual framework and research/practice agenda  </p>

<h2>General Summary of the Paper</h2>

<p>The article reviews the first decade of DSS, noting the shift from technology bottlenecks to an environment where technology is abundant, and the challenge is maximizing its strategic value. Keen critiques the lack of an agreed definition of DSS, highlighting the need for both “definitions for understanding” (broad, conceptual, innovative) and “definitions for action” (practical, organizationally relevant). He introduces the concept of <em>Extended Decision Support</em>—a more active, consultative approach aimed at decisions of organizational significance—contrasting it with Passive, Traditional, and Normative support models. The paper advocates rebalancing the “D” in DSS toward decision-focused research, incorporating multicriteria decision-making, and leveraging new tools like expert systems, document-based systems, and telecommunications. A detailed agenda for research, scholarship, and practice is provided.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as the capacity of DSS to provide decision support that improves the quality, creativity, and learning of decisions that “really matter” in organizations. This includes shifting from passive tool provision to active, consultative roles where DSS influences decision processes.  </p>

<blockquote>
  <p>“DSS is concerned with intellectual as well as computer-related technologies… We need to have a more ambitious view of decision making…” (p. 255)  </p>
</blockquote>

<blockquote>
  <p>“The agenda… is to apply intellectual and computer-related technologies to amplify creativity and learning in decisions that really matter.” (p. 256)  </p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Decision relevance: supports critical, high-impact organizational decisions.  </p></li>
<li><p>Integration of judgment with analytic tools.  </p></li>
<li><p>Contextual fit to user needs and organizational priorities.  </p></li>
<li><p>Ability to improve decision process quality, not just provide data.  </p></li>
<li><p>Leveraging appropriate technology for the decision context.  </p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Extended Decision Support (EDS)  </p></li>
<li><p><strong>Methods/Levers:</strong> Explicit targeting of significant decisions; blending analytic models with AI and document management; focus on organizational decision-making.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify decision areas of high value; build systems integrating analytic and technological tools; engage as consultants, not just system builders; apply iterative prototyping.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Use organizational data stores; integrate document-based info; apply multicriteria decision-making methods.  </p></li>
<li><p><strong>Implementation Context:</strong> Senior management planning, competitive/environmental scanning, organizational problem-solving.  </p></li>
</ul>

<blockquote>
  <p>“Extended support involves an explicit effort to influence and guide decision making… while respecting the primacy of judgement…” (p. 258)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Systems must be understandable and usable to decision makers.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Support must align with “decisions that really matter.”  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Tools and approaches must be practical in organizational settings.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — Focus on reducing “information float” and delivering alerts before issues escalate.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — EDS aims to make reasoning visible (e.g., semi-expert systems showing rule triggers).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — Systems must align with primary business goals and user priorities.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong>  </p>

<p> - Level of support (Passive, Traditional, Extended, Normative)  </p>

<p> - Organizational integration (link with IS and data resources)  </p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Herbert Simon’s concepts of satisficing vs. optimization.  </p></li>
<li><p>Cognitive psychology and Carnegie School decision-making research.  </p></li>
<li><p>Management Science and multicriteria decision-making theories.  </p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Targeting high-value decisions.  </p></li>
<li><p>Reducing decision-making delays (“information float”).  </p></li>
<li><p>Integration of analytic and judgmental elements.  </p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of clear DSS definitions; overemphasis on technology over decision focus; “cherry-picking” easy applications; drift to passive support.  </p></li>
<li><p><strong>Enablers:</strong> Emerging AI tools; document-based DSS; telecommunications; strong linkages with IS and organizational data; hybrid professionals with both technical and domain expertise.  </p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Positions DSS as an evolution from Management Science and early decision-making theories, but critiques the field for losing decision focus. Advocates borrowing from organizational theory, political science, and MCDM to strengthen conceptual foundations.</p>

<h2>Summary</h2>

<p>Keen’s paper reframes DSS for its second decade, arguing for a more ambitious and decision-centered approach. Actionability here is about supporting critical organizational decisions, enhancing creativity and learning, and integrating judgment with technology. The proposed <em>Extended Decision Support</em> model moves beyond providing tools to actively guiding decisions, leveraging new technologies like AI, document-based systems, and telecommunications. Keen distinguishes between conceptual (“definition for understanding”) and practical (“definition for action”) views, urging researchers and practitioners to clarify their mission and target high-value decisions. This reconceptualization aims to ensure DSS remains a distinctive and impactful field rather than a commodity subset of end-user computing.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 88 — Strong conceptual treatment of actionability with explicit features and a detailed framework, though not framed using “actionability” terminology.  </p></li>
<li><p><strong>Operationalization Score:</strong> 85 — Provides concrete methods (EDS model, target market identification, technology integration), though implementation detail is more strategic than procedural.  </p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“DSS is concerned with intellectual as well as computer-related technologies…” (p. 255)  </p></li>
<li><p>“Apply intellectual and computer-related technologies to amplify creativity and learning in decisions that really matter.” (p. 256)  </p></li>
<li><p>“Extended support involves an explicit effort to influence and guide decision making…” (p. 258)  </p></li>
<li><p>“Reduce information ‘float’…” (p. 264)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Keen &amp; Scott Morton (1978) <em>Decision Support Systems: An Organizational Perspective</em>  </p></li>
<li><p>Elam et al. (1986) <em>A Vision for DSS Research</em>  </p></li>
<li><p>Herbert Simon (1969) <em>Sciences of the Artificial</em>  </p></li>
<li><p>Sprague &amp; Carlson (1982) <em>Building Effective Decision Support Systems</em></p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Co-Designing a Real-Time Classroom Orchestration Tool to Support Teacher–AI Complementarity  </p>

<p>Authors: Kenneth Holstein, Bruce M. McLaren, Vincent Aleven  </p>

<p>DOI: http://dx.doi.org/10.18608/jla.2019.62.3  </p>

<p>Year: 2019  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Learning Analytics / Human–Computer Interaction  </p>

<p>Subdomain/Topic: Co-design of AI-enhanced classroom orchestration tools  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 88  </p>

<p>Operationalization Score: 95  </p>

<p>Contains Definition of Actionability: Yes (implicit and partial explicit)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (Replay Enactments prototyping method)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Mixed Methods (qualitative need-finding, iterative prototyping, in-lab simulation, classroom pilots, experimental evaluation)  </p>

<p>Study Context: K–12 AI-enhanced classrooms using Intelligent Tutoring Systems (ITS)  </p>

<p>Geographic/Institutional Context: US middle schools, Carnegie Mellon University-led research  </p>

<p>Target Users/Stakeholders: K–12 teachers, students, educational technologists  </p>

<p>Primary Contribution Type: Empirical case study and methodological framework  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Co-Designing a Real-Time Classroom Orchestration Tool to Support Teacher–AI Complementarity  </p>

<p><strong>Authors:</strong>  </p>

<p>Kenneth Holstein, Bruce M. McLaren, Vincent Aleven  </p>

<p><strong>DOI:</strong>  </p>

<p>http://dx.doi.org/10.18608/jla.2019.62.3  </p>

<p><strong>Year:</strong>  </p>

<p>2019  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Learning Analytics / Human–Computer Interaction  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Participatory design, AI in education, teacher orchestration tools  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The study addresses the challenge of designing AI-driven learning analytics (LA) tools that meaningfully involve non-technical stakeholders, specifically K–12 teachers, throughout the design process. It focuses on the development of <em>Lumilo</em>, a mixed-reality smart glasses system for real-time classroom orchestration in AI-enhanced classrooms.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Conducted in US middle schools in collaboration with Carnegie Mellon University.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Middle-school teachers, students, educational technologists.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Mixed methods: generative design (interviews, card sorting, storytelling), iterative prototyping (low–high fidelity), Replay Enactments simulations, live classroom pilots, experimental evaluation.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Empirical design case study and introduction of a novel prototyping method for data-driven algorithmic systems.  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The paper presents the first end-to-end co-design case study of a complex learning analytics tool—<em>Lumilo</em>, a smart glasses system for K–12 teachers using AI tutoring systems. The authors integrate participatory design, iterative prototyping, and evaluation to ensure teacher needs drive system design. They introduce Replay Enactments (REs), a novel prototyping method combining authentic data, algorithms, and embodied role-play to simulate classroom contexts. Findings show that effective actionable analytics must enhance teacher awareness, respect autonomy, be context-sensitive, and link directly to instructional decisions. Classroom trials demonstrated <em>Lumilo</em>'s potential to equalize learning outcomes by reallocating teacher attention towards struggling students.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as analytics that:  </p>

<ul>
<li><p>Link directly to specific teacher decisions and interventions in real time.  </p></li>
<li><p>Provide timely, context-relevant, and interpretable insights that support in-the-moment decision-making.  </p></li>
<li><p>Enhance rather than replace teacher autonomy.  </p></li>
</ul>

<blockquote>
  <p>“Prompting teachers to reflect on what real-time decisions a particular information display might inform often led them to notice ways in which the display could be made more useful…” (p. 47)  </p>
</blockquote>

<blockquote>
  <p>Teachers distinguished between “seeing thought processes” and abstract mastery probabilities, noting the former was more <em>actionable</em> for immediate instructional rerouting (p. 31).  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Direct linkage between analytics and possible teacher interventions.  </p></li>
<li><p>Timeliness to act during a learning episode.  </p></li>
<li><p>Interpretability to justify and trust recommendations.  </p></li>
<li><p>Contextual relevance to the specific class, student, and task.  </p></li>
<li><p>Respect for teacher autonomy and flexibility in use.  </p></li>
<li><p>Grounding automated inferences in raw, concrete student artifacts.  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Replay Enactments (REs) for co-design and prototyping.  </p></li>
<li><p><strong>Methods/Levers:</strong> Generative need-finding (superpowers exercise, storytelling), iterative prototyping (lo–hi fidelity), simulation-based REs, classroom pilots.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify teacher needs → prototype low-fidelity displays → mid-fidelity HoloLens prototypes → RE simulations with authentic ITS data → classroom deployment and iteration.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> ITS logs, real-time detectors for misuse, struggle, performance, engagement; teacher movement and gaze tracking; qualitative feedback.  </p></li>
<li><p><strong>Implementation Context:</strong> US middle-school ITS classrooms.  </p></li>
</ul>

<blockquote>
  <p>“REs… enable earlier, nuanced observations of the interplay between human and machine judgments…” (p. 41)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – indicators visually simple with on-demand elaborations (p. 38).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – tailored to class-level, student-level needs (p. 33).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – designs respect teacher constraints, cognitive load (p. 33, p. 47).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes – real-time analytics to intervene “in the moment” (p. 31, p. 47).  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – grounded in raw student artifacts to justify inferences (p. 39).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – respect teacher goals, autonomy, instructional style (p. 35, p. 47).  </p></li>
<li><p><strong>Other Dimensions Named:</strong> Selective sharing, adaptability of thresholds, anonymity for help-seeking.  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Participatory/co-design principles from HCI.  </p></li>
<li><p>Human–machine function allocation literature.  </p></li>
<li><p>Open learner models and explainable AI in education.  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Accuracy and interpretability of student state detectors.  </p></li>
<li><p>Teacher time allocation toward students with greater need.  </p></li>
<li><p>Reduction in learning outcome gaps.  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Teacher overload; autonomy concerns; risk of distraction; privacy; lack of transparency in ITS logic.  </p></li>
<li><p><strong>Enablers:</strong> Wearable displays; context-sensitive analytics; selective visibility; raw data grounding; flexible customization.  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Extends prior LA co-design frameworks by demonstrating a full-cycle, stakeholder-driven design with a novel embodied simulation method. Bridges AI in education and HCI with a focus on actionable, real-time orchestration support.</p>

<hr />

<h2>Summary</h2>

<p>The authors detail a multi-year co-design process culminating in <em>Lumilo</em>, a wearable real-time analytics tool for K–12 teachers in AI-enhanced classrooms. They conceptualize actionability as timely, interpretable, and context-relevant information that directly informs instructional choices without undermining autonomy. The design process surfaced teacher priorities such as “seeing thought processes,” identifying unvoiced needs, and supporting discreet help requests. Operationalization centered on <em>Replay Enactments</em>—immersive, data-driven simulations enabling iterative refinement of analytics and interfaces before live deployment. Classroom results showed improved equity in teacher attention and student learning outcomes. The study offers methodological guidance for co-designing actionable LA systems.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 88 – Strong implicit and partial explicit conceptualization of actionability with detailed features; slightly less formal definitional clarity prevents a perfect score.  </p></li>
<li><p><strong>Operationalization Score:</strong> 95 – Comprehensive, multi-phase, and innovative operationalization with REs; rich linkage between needs, design, and classroom impact.  </p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Such skill mastery estimates were less actionable… if teachers could follow students’ thought processes in real-time… this could provide opportunities… to ‘re-route’ students…” (p. 31)  </p></li>
<li><p>“Receiving more direct… feedback about the effects of their own teaching… could help them adjust their instruction on-the-spot…” (p. 43)  </p></li>
<li><p>“Ground automated inferences in ‘raw’ examples… Showing these example errors is crucial… in supporting teacher trust…” (p. 39)  </p></li>
<li><p>“Prompting teachers to reflect on what… might inform… often led them to notice ways… display could be made more useful…” (p. 47)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Bull &amp; Kay (2016) on grounding analytics in raw data.  </p></li>
<li><p>Martinez-Maldonado et al. (2016) LATUX workflow.  </p></li>
<li><p>Doshi-Velez &amp; Kim (2017) on interpretable ML.  </p></li>
<li><p>Aguilar (2018) on social comparison in analytics.  </p></li>
<li><p>Beck &amp; Gong (2013) on detecting “wheel-spinning.”</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Clinical Practice Guidelines: A Manual for Developing Evidence-Based Guidelines to Facilitate Performance Measurement and Quality Improvement  </p>

<p>Authors: Richard M. Rosenfeld, MD, MPH; Richard N. Shiffman, MD, MCIS  </p>

<p>DOI: 10.1016/j.otohns.2006.06.1277  </p>

<p>Year: 2006  </p>

<p>Publication Type: Journal Article (Special Contribution)  </p>

<p>Discipline/Domain: Medicine / Health Policy  </p>

<p>Subdomain/Topic: Clinical Practice Guideline Development  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 98  </p>

<p>Contains Definition of Actionability: Yes (explicit, as part of defining actionable guideline recommendations)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (COGS, AGREE, GLIA-based framework)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual / Methodological Guide  </p>

<p>Study Context: Guideline development in clinical medicine  </p>

<p>Geographic/Institutional Context: USA; American Academy of Otolaryngology–Head and Neck Surgery, Yale School of Medicine  </p>

<p>Target Users/Stakeholders: Clinicians, healthcare organizations, specialty societies, performance measurement developers  </p>

<p>Primary Contribution Type: Comprehensive, step-by-step manual for actionable guideline creation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Clinical Practice Guidelines: A Manual for Developing Evidence-Based Guidelines to Facilitate Performance Measurement and Quality Improvement  </p>

<p><strong>Authors:</strong>  </p>

<p>Richard M. Rosenfeld, MD, MPH; Richard N. Shiffman, MD, MCIS  </p>

<p><strong>DOI:</strong>  </p>

<p>10.1016/j.otohns.2006.06.1277  </p>

<p><strong>Year:</strong>  </p>

<p>2006  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Article (Special Contribution)  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Medicine / Health Policy  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Clinical Practice Guideline Development  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The manual addresses how to systematically produce clinical practice guidelines that are implementable, measurable, and of high methodological quality. It emphasizes efficiency (12-month target), multidisciplinary collaboration, explicit action statements, and the link between recommendations and performance measurement.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>USA; American Academy of Otolaryngology–Head and Neck Surgery Foundation; Yale School of Medicine  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Clinicians, specialty societies, healthcare organizations, policymakers, and quality improvement bodies  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual / Methodological Guide  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Step-by-step framework for developing actionable, evidence-based clinical practice guidelines  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This manual provides a tested, pragmatic methodology for developing evidence-based clinical practice guidelines (CPGs) intended to facilitate both performance measurement and quality improvement. The authors integrate existing quality standards such as the AGREE instrument, the Conference on Guideline Standardization (COGS) checklist, and the GuideLine Implementability Appraisal (GLIA) tool into a comprehensive process. Key features include defining boldfaced, actionable recommendation statements, linking them explicitly to evidence profiles, and ensuring transparency about values and patient preferences. The manual details all phases—from topic selection and multidisciplinary team formation to literature review, statement drafting, evidence grading, external appraisal, and implementation planning—aimed at producing guidelines that are specific, measurable, and adaptable to performance metrics.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as the creation of <strong>specific, boldfaced key action statements</strong> that direct measurable clinical behaviors, linked to explicit conditions, target populations, and intended outcomes, with evidence-based strength ratings.  </p>

<blockquote>
  <p>“Guidelines should contain a series of key, boldfaced action statements that can be used to describe desired behavior, measure performance, and assess quality.” (p. S1)  </p>
</blockquote>

<blockquote>
  <p>“An ideal key, boldfaced statement describes… When, Who should do what, To whom, why, and how.” (p. S12)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Explicitly states conditions under which to act (decidability)  </p></li>
<li><p>Specifies precise, measurable clinician actions (executability)  </p></li>
<li><p>Links actions to evidence strength and harm–benefit balance  </p></li>
<li><p>Provides rationale, supporting evidence, and value judgments  </p></li>
<li><p>Identifies intended audience and settings  </p></li>
<li><p>Incorporates patient preferences where relevant  </p></li>
<li><p>Is feasible to implement in real-world workflows  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> AGREE instrument, COGS checklist, GLIA tool, AAP evidence grading system  </p></li>
<li><p><strong>Methods/Levers:</strong> Systematic literature search, multidisciplinary consensus, explicit evidence-to-recommendation linkage, use of performance-measure-ready key statements  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> 12-month plan including topic definition, team assembly, literature review, drafting, evidence grading, external appraisal, peer review, and implementation planning  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Evidence profiles (aggregate evidence quality, benefits, harms, costs, values, role of patient preferences, policy level)  </p></li>
<li><p><strong>Implementation Context:</strong> CPGs applicable across diverse clinical settings, designed to support performance measures and maintenance of certification  </p></li>
</ul>

<blockquote>
  <p>“Guideline implementers agree that statements are easiest to implement if parsed into statements of the form: if (conditions) then (actions).” (p. S12)  </p>
</blockquote>

<blockquote>
  <p>“Evidence profile… lists all decisions made by the group” (p. S21)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — explicit, unambiguous statements required (p. S2)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — tailored to defined populations, settings, and users (p. S8–S9)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — GLIA dimension includes “effect on process of care” (p. S23)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Yes — goal to produce within 12 months; timeliness affects impact (p. S1, S26–S27)  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — each action has supporting rationale, evidence, and values (p. S16–S21)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — recommendations linked to quality improvement and patient outcome goals (p. S1)  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Decidability, executability, measurability, flexibility, novelty/innovation (GLIA, p. S23)  </p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Institute of Medicine’s definition of CPGs  </p></li>
<li><p>AGREE instrument for quality appraisal  </p></li>
<li><p>COGS checklist for standardized reporting  </p></li>
<li><p>AAP’s 3-step recommendation strength framework  </p></li>
<li><p>GLIA tool for implementability appraisal  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Presence of explicit “if–then” statements  </p></li>
<li><p>Evidence profile completeness (benefit–harm balance, evidence grade)  </p></li>
<li><p>Linkage to measurable outcomes for performance assessment  </p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<p><strong>Barriers:</strong>  </p>

<ul>
<li><p>Clinician resistance to changing ingrained habits  </p></li>
<li><p>Procedural skills or equipment gaps (p. S23)  </p></li>
<li><p>Cost of recommended interventions (p. S23)  </p></li>
</ul>

<p><strong>Enablers:</strong>  </p>

<ul>
<li><p>Educational outreach and workshops  </p></li>
<li><p>Multidisciplinary buy-in from development stage  </p></li>
<li><p>Free public access to guidelines  </p></li>
<li><p>Algorithmic presentation for clarity (p. S21–S22)  </p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Positions itself as a synthesis and operationalization of prior work (IOM, AGREE, COGS, GLIA), moving from <strong>conceptual quality standards</strong> to a <strong>practical, reproducible process</strong> designed for producing measurable, implementable guidelines.</p>

<hr />

<h2>Summary</h2>

<p>Rosenfeld and Shiffman’s manual is a blueprint for creating <strong>actionable, performance-measure-ready clinical practice guidelines</strong>. Actionability is embedded in the use of boldfaced, condition-specific, measurable action statements linked to explicit evidence profiles. The manual specifies <strong>how</strong> to develop these statements through a structured, multidisciplinary process grounded in AGREE, COGS, GLIA, and AAP grading frameworks. Attributes such as clarity, contextual relevance, feasibility, timeliness, explainability, and goal alignment are explicitly tied to actionability. Operationalization is detailed step-by-step, from topic selection to implementation and updating, with strong emphasis on transparency, value declaration, and stakeholder engagement. This positions the manual as a high-value resource for both the <strong>conceptualization</strong> and <strong>practical execution</strong> of actionable guideline development.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Provides explicit, comprehensive conceptualization of actionability in CPGs, including definitions, attributes, and structured methods.  </p></li>
<li><p><strong>Operationalization Score:</strong> 98 — Offers full, replicable process for achieving actionability, including tools, templates, and measurable outputs.  </p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Guidelines should contain a series of key, boldfaced action statements that can be used to describe desired behavior, measure performance, and assess quality.” (p. S1)  </p></li>
<li><p>“An ideal key, boldfaced statement describes… When, Who should do what, To whom, why, and how.” (p. S12)  </p></li>
<li><p>“Evidence profile… lists all decisions made by the group.” (p. S21)  </p></li>
<li><p>“Guideline implementers agree that statements are easiest to implement if parsed into statements of the form: if (conditions) then (actions).” (p. S12)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Field MJ, Lohr KN (1990) — IOM definition of CPGs  </p></li>
<li><p>AGREE Collaboration (2003) — AGREE Instrument  </p></li>
<li><p>Shiffman et al. (2003) — COGS checklist  </p></li>
<li><p>AAP Steering Committee (2004) — Recommendation classification framework  </p></li>
<li><p>Shiffman et al. (2005) — GLIA instrument</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Seeking Truth and Actionable Knowledge: How the Scientific Method Inhibits Both</p>

<p>Authors: Chris Argyris</p>

<p>DOI: n/a</p>

<p>Year: n/a</p>

<p>Publication Type: Journal Article</p>

<p>Discipline/Domain: Organizational Studies / Social Science Methodology</p>

<p>Subdomain/Topic: Actionable Knowledge; Organizational Defensive Routines; Scientific Method Critique</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 92</p>

<p>Operationalization Score: 85</p>

<p>Contains Definition of Actionability: Yes (implicit, conceptualized as knowledge enabling effective intervention and change in organizational contexts)</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Yes</p>

<p>Contains Interpretability: Partial</p>

<p>Contains Framework/Model: Yes (Model I, Model II, 0–1 Learning System)</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Conceptual with empirical illustrations</p>

<p>Study Context: Organizational settings, primarily corporate and institutional</p>

<p>Geographic/Institutional Context: U.S.-based, Harvard University</p>

<p>Target Users/Stakeholders: Social scientists, organizational leaders, change agents</p>

<p>Primary Contribution Type: Theoretical framework and methodological critique</p>

<p>CL: Yes</p>

<p>CR: Yes</p>

<p>FE: Yes</p>

<p>TI: Partial</p>

<p>EX: Yes</p>

<p>GA: Yes</p>

<p>Reason if Not Eligible: n/a</p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p><em>Seeking Truth and Actionable Knowledge: How the Scientific Method Inhibits Both</em></p>

<p><strong>Authors:</strong>  </p>

<p>Chris Argyris</p>

<p><strong>DOI:</strong>  </p>

<p>n/a</p>

<p><strong>Year:</strong>  </p>

<p>n/a</p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Article</p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Organizational Studies / Social Science Methodology</p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Actionable Knowledge; Organizational Defensive Routines; Scientific Method Critique</p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses how conventional scientific research methods can unintentionally inhibit the production of actionable knowledge in social science, particularly in contexts involving organizational defensive routines. It critiques “Model I” theories-in-use and limited learning systems, arguing for a shift toward methods and theories that facilitate double-loop learning and effective intervention.</p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>U.S., Harvard University</p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Social scientists, organizational leaders, consultants, change agents</p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual analysis with empirical illustrations from organizational research</p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Theoretical framework and methodological critique</p>

<h2>General Summary of the Paper</h2>

<p>Argyris critiques the standard application of the scientific method in social sciences, arguing that it often reinforces defensive routines within organizations, limiting both truth-seeking and the generation of actionable knowledge. Through the concepts of “Model I” and “0–1 learning systems,” he shows how prevailing research practices mirror the very defensive behaviors they aim to study, especially when dealing with threatening or embarrassing issues. He calls for the creation of normative models of “rare universes” where defensive routines are minimized, coupled with robust intervention and instructional theories. This approach would enable researchers to produce knowledge that is not only valid but also usable by practitioners in real-world settings.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionable knowledge is framed as information that enables effective change in systems characterized by defensive routines. It must be valid, disconfirmable, and usable under everyday conditions.  </p>

<blockquote>
  <p>“In order to provide a comprehensive description… we must produce propositions about what happens when we try to change them” (p. 12)  </p>
</blockquote>

<blockquote>
  <p>“Researchers should focus on making their normative theories as comprehensive and as empirically valid as possible… and study the processes by which individuals can use the theories in everyday life” (p. 18)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Explicit recognition and surfacing of undiscussable issues</p></li>
<li><p>Valid, disconfirmable knowledge</p></li>
<li><p>Normative models enabling rare but desirable organizational states</p></li>
<li><p>Practical usability under real-time conditions</p></li>
<li><p>Alignment between espoused theories and theories-in-use</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Model I, Model II, 0–1 Learning Systems</p></li>
<li><p><strong>Methods/Levers:</strong> Double-loop learning; theory-of-intervention design; theory-of-instruction development</p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Diagnose defensive routines → Create normative models → Develop intervention &amp; instructional strategies → Implement in real organizational contexts → Provide disconfirmable evidence</p></li>
<li><p><strong>Data &amp; Measures:</strong> Observable behavioral data (conversation transcripts), theory-in-use analysis</p></li>
<li><p><strong>Implementation Context:</strong> Organizational change efforts where defensive routines are prevalent  </p></li>
</ul>

<blockquote>
  <p>“More time and effort should be spent on learning how to produce normative models of rare universes… and study the processes by which individuals can use the theories in everyday life” (p. 18)</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – Must make tacit theories explicit and testable (p. 18)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – Models must work in real organizational contexts (p. 18)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – Must be usable under everyday conditions (p. 19–20)  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial – Emphasis on real-time usability but not extensively discussed as “timeliness”  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – Provide rationale and make embedded values explicit (p. 18)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – Designed to improve organizational learning and reduce defensive routines  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Disconfirmability; empirical validity under natural conditions</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Organizational Learning Theory (Argyris &amp; Schön, 1974, 1978)  </p></li>
<li><p>Model I / Model II theories-in-use  </p></li>
<li><p>Double-loop learning  </p></li>
<li><p>Defensive routines theory</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Reduction in organizational defensive routines</p></li>
<li><p>Ability to discuss previously undiscussable issues</p></li>
<li><p>Observable changes in theory-in-use</p></li>
<li><p>Successful use of interventions in real-time situations</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Defensive reasoning; organizational culture; lack of intervention skills; research methods reinforcing defensive routines  </p></li>
<li><p><strong>Enablers:</strong> Normative models; explicit theories-in-use; real-time practice; creation of safe contexts for learning</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Argyris positions his critique against traditional scientific method prescriptions (Campbell &amp; Stanley) and psychological theories (learning theory, mass communication research), showing how these often presuppose and perpetuate Model I conditions, limiting the scope of actionable insights.</p>

<h2>Summary</h2>

<p>Argyris’ paper argues that prevailing social science research practices inadvertently reinforce the very defensive patterns they aim to study, limiting both truth-seeking and actionable knowledge. Actionability, in his view, requires producing normative models of alternative organizational realities, grounded in disconfirmable evidence and usable under everyday conditions. This entails shifting focus from abstract generalizations to theory-in-use analysis, developing intervention and instructional frameworks, and openly addressing embedded values and assumptions. The paper’s distinctive contribution is its integration of organizational learning theory, methodological critique, and operational guidance for producing knowledge that organizations can genuinely use to change entrenched behaviors.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 92 — Strong implicit conceptualization of actionability with systematic features and explicit links to organizational learning theory.</p></li>
<li><p><strong>Operationalization Score:</strong> 85 — Provides concrete frameworks (Model I/II), methods, and steps, though practical examples of large-scale implementation are limited.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“[Researchers should] study the processes by which individuals can use the theories in everyday life” (p. 18)  </p></li>
<li><p>“More time and effort should be spent on learning how to produce normative models of rare universes…” (p. 18)  </p></li>
<li><p>“It is not possible for human beings to change their theory-in-use because they wish to do so… requires new skills and new values” (p. 19)  </p></li>
<li><p>“In order for human beings to use propositions, they must be producible under everyday life conditions” (p. 19–20)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Argyris &amp; Schön, <em>Theory in Practice</em> (1974)  </p></li>
<li><p>Argyris &amp; Schön, <em>Organizational Learning</em> (1978)  </p></li>
<li><p>Argyris, <em>Reasoning, Learning and Action</em> (1982)  </p></li>
<li><p>Campbell &amp; Stanley, <em>Experimental and Quasi-experimental Design for Research</em> (1963)</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations  </p>

<p>Authors: Amir-Hossein Karimi, Gilles Barthe, Bernhard Schölkopf, Isabel Valera  </p>

<p>DOI: 10.1145/3442188.3445899  </p>

<p>Year: 2021  </p>

<p>Publication Type: Journal Article  </p>

<p>Discipline/Domain: Computer Science / Machine Learning  </p>

<p>Subdomain/Topic: Algorithmic Recourse, Explainable AI, Causal Inference  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual + Review  </p>

<p>Study Context: Automated decision-making in consequential domains (finance, justice, healthcare, hiring)  </p>

<p>Geographic/Institutional Context: Not location-specific; examples from EU GDPR, US legal contexts  </p>

<p>Target Users/Stakeholders: Affected individuals, ML practitioners, legal scholars, researchers  </p>

<p>Primary Contribution Type: Conceptual framework + literature survey  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations  </p>

<p><strong>Authors:</strong>  </p>

<p>Amir-Hossein Karimi, Gilles Barthe, Bernhard Schölkopf, Isabel Valera  </p>

<p><strong>DOI:</strong>  </p>

<p>10.1145/3442188.3445899  </p>

<p><strong>Year:</strong>  </p>

<p>2021  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Article  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Computer Science / Machine Learning  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Algorithmic Recourse, Explainable AI, Causal Inference  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper reviews and unifies definitions, formulations, and solutions for algorithmic recourse in settings where automated decisions significantly impact individuals’ lives. It clarifies distinctions between contrastive explanations and consequential recommendations, framing them within causal reasoning. It situates recourse alongside ethical ML concerns (fairness, robustness, privacy, security) and identifies open research directions.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Not geographically restricted; draws on EU GDPR and US legal notions.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Individuals affected by automated decisions, ML practitioners, policymakers, legal scholars.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual synthesis and literature review.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Conceptual framework + systematic survey of technical literature.</p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This paper consolidates the rapidly growing literature on algorithmic recourse — the provision of explanations and recommendations enabling individuals to change unfavorable decisions from automated systems. It distinguishes between <strong>contrastive explanations</strong> (“Why outcome P rather than Q?”) and <strong>consequential recommendations</strong> (“What actions should I take to achieve Q?”), grounding both in causal inference theory. The authors present unified definitions, formulate the problem as constrained optimization, and categorize constraints such as actionability, plausibility, diversity, and sparsity. They survey over 50 recourse algorithms, analyzing properties like optimality, coverage, and runtime. The paper also explores how recourse interacts with fairness, robustness, security, and privacy, and outlines future research directions beyond deterministic, supervised, and individualized contexts.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<hr />

<h2>How Actionability is Understood</h2>

<p>The authors define algorithmic recourse as enabling affected individuals to <strong>understand</strong> and <strong>act</strong> to alleviate an unfavorable outcome, exercising “temporally-extended agency”:contentReference[oaicite:0]{index=0}. Actionability in this context involves not only knowing why a decision occurred but also receiving feasible, effective recommendations to change it.  </p>

<blockquote>
  <p>“An actionable set of changes a person can undertake in order to improve their outcome” (p. n/a)  </p>
</blockquote>

<blockquote>
  <p>“Recourse is offered when the individual is given explanations…and offered recommendations on how to obtain [the desired outcome] in the future” (p. n/a)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Comprehensibility (clear link between features and outcome)  </p></li>
<li><p>Feasibility of interventions (actions possible for the individual)  </p></li>
<li><p>Plausibility (recommendations correspond to realistic states)  </p></li>
<li><p>Causal validity (recommendations derived from interventions in a structural causal model, not just feature manipulations)  </p></li>
<li><p>Alignment with individual goals and constraints  </p></li>
<li><p>Efficiency (minimal cost/effort to achieve the outcome)</p></li>
</ul>

<hr />

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Contrastive explanations vs. consequential recommendations  </p></li>
<li><p><strong>Methods/Levers:</strong> Constrained optimization (distance metrics for explanations; cost functions for recommendations)  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Identify current decision outcome and features  </p>

<p> 2. For explanations: find minimal changes in feature space leading to a different outcome (Eq. 1)  </p>

<p> 3. For recommendations: identify feasible actions within a causal model that lead to a favorable outcome with minimal cost (Eq. 2)  </p>

<p> 4. Apply plausibility, actionability, diversity, and sparsity constraints  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Dissimilarity metrics (e.g., MAD-weighted Manhattan, mixed ℓp norms), cost measures (percentile shifts, ℓp norms)  </p></li>
<li><p><strong>Implementation Context:</strong> Applied to tabular, image, and text data; models include tree-based, kernel-based, differentiable, and others.  </p></li>
</ul>

<blockquote>
  <p>“Minimal consequential recommendations…result in a contrastive explanation when acted upon” (p. n/a)  </p>
</blockquote>

<blockquote>
  <p>“Offering nearest contrastive explanations that are not attainable through minimal effort is of secondary importance” (p. n/a)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Explanations should reveal causal relationships between features and outcome.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Recommendations must account for individual-specific constraints and context.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Only actionable interventions (do-operations) feasible for the individual.  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — Time-sensitive nature acknowledged (stationarity assumption), but not deeply operationalized.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — Transparency in how recommendations are derived.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — Recommendations should align with individual’s goals but often implicit.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Plausibility, diversity, sparsity, robustness, fairness.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Structural Causal Models (Pearl)  </p></li>
<li><p>Counterfactual reasoning in philosophy of science (Lewis, Lipton)  </p></li>
<li><p>Explainable AI literature  </p></li>
<li><p>Ethical ML frameworks (fairness, accountability, GDPR compliance)</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Distance measures (MAD-weighted Manhattan, ℓp norms)  </p></li>
<li><p>Cost measures (effort, percentile shifts)  </p></li>
<li><p>Feasibility constraints satisfaction rate  </p></li>
<li><p>Plausibility constraint adherence  </p></li>
<li><p>Optimality, coverage, runtime</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Incomplete causal knowledge, infeasible recommendations, reliance on manipulable but implausible features, security/privacy risks, non-robustness to model shifts.  </p></li>
<li><p><strong>Enablers:</strong> Accurate causal models, open-source implementations, user interfaces for non-technical stakeholders, diversity in recourse options.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>The paper integrates insights from explainable AI, causal inference, and optimization, positioning algorithmic recourse as distinct from related fields like adversarial perturbations or actionable knowledge discovery by emphasizing stakeholder trust, plausibility, and feasibility.</p>

<hr />

<h2>Summary</h2>

<p>This survey formalizes and unifies the concept of algorithmic recourse, distinguishing between contrastive explanations and consequential recommendations and embedding both in a causal inference framework. Actionability is conceptualized as the combination of comprehensible reasoning, feasible interventions, and realistic outcomes, tailored to an individual’s context. The authors provide operational formulations, catalog a broad range of algorithms, and detail constraints that ensure real-world applicability. They argue for prioritizing minimal consequential recommendations over nearest contrastive explanations and connect recourse to ethical ML considerations like fairness, robustness, and privacy. Future research directions challenge current assumptions of determinism, supervision, and individuality, calling for richer, more realistic recourse systems.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Clear, explicit conceptualization of actionability, systematic identification of its features, and deep integration with related ethical ML considerations.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Provides detailed formulations, metrics, and algorithmic approaches to achieve actionability, though practical implementation still depends on strong causal assumptions.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“[Recourse is] an actionable set of changes a person can undertake in order to improve their outcome” (p. n/a)  </p></li>
<li><p>“Recourse is offered when the individual is given explanations…and offered recommendations” (p. n/a)  </p></li>
<li><p>“Minimal consequential recommendations…result in a contrastive explanation when acted upon” (p. n/a)  </p></li>
<li><p>“Offering nearest contrastive explanations that are not attainable through minimal effort is of secondary importance” (p. n/a)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Wachter et al. (2017) — Counterfactual explanations and GDPR compliance  </p></li>
<li><p>Karimi et al. (2020) — Algorithmic recourse from counterfactuals to interventions  </p></li>
<li><p>Ustun et al. (2019) — Actionable recourse in linear classification  </p></li>
<li><p>Miller (2019) — Contrastive explanation in AI  </p></li>
<li><p>Pearl (2000) — Causality: Models, Reasoning, and Inference</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: On the Trade-offs between Adversarial Robustness and Actionable Explanations  </p>

<p>Authors: Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju  </p>

<p>DOI: 10.3390/analytics1020008  </p>

<p>Year: 2024  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Machine Learning, Explainable AI  </p>

<p>Subdomain/Topic: Adversarial Robustness, Counterfactual Explanations  </p>

<p>Eligibility: Yes  </p>

<p>Overall Relevance Score: 90  </p>

<p>Operationalization Score: 85  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Theoretical and Empirical Analysis  </p>

<p>Study Context: Adversarial Robustness vs. Actionable Explanations in Machine Learning  </p>

<p>Geographic/Institutional Context: Harvard University  </p>

<p>Target Users/Stakeholders: AI Researchers, ML Practitioners, Data Scientists  </p>

<p>Primary Contribution Type: Theoretical Analysis, Empirical Evaluation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Yes  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: n/a  </p>

<!--META_END-->

<p><strong>Title:</strong> On the Trade-offs between Adversarial Robustness and Actionable Explanations  </p>

<p><strong>Authors:</strong> Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju  </p>

<p><strong>DOI:</strong> 10.3390/analytics1020008  </p>

<p><strong>Year:</strong> 2024  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Machine Learning, Explainable AI  </p>

<p><strong>Subdomain/Topic:</strong> Adversarial Robustness, Counterfactual Explanations  </p>

<p><strong>Contextual Background:</strong> The paper explores the trade-offs between two important characteristics of machine learning models: adversarial robustness and the ability to provide actionable explanations. The authors examine how adversarially robust models affect the cost (ease of implementation) and validity (probability of success) of counterfactual explanations, which are typically used for generating actionable recourses. The study theoretically and empirically investigates these trade-offs using real-world datasets and compares state-of-the-art algorithms for adversarially robust and non-robust models.  </p>

<p><strong>Geographic/Institutional Context:</strong> Harvard University  </p>

<p><strong>Target Users/Stakeholders:</strong> AI researchers, machine learning practitioners, stakeholders in high-stakes decision-making applications  </p>

<p><strong>Primary Methodology:</strong> Theoretical bounds, empirical analysis on real-world datasets  </p>

<p><strong>Primary Contribution Type:</strong> Theoretical framework, empirical study  </p>

<h2>General Summary of the Paper</h2>

<p>This paper examines the relationship between adversarial robustness and the generation of actionable recourses in machine learning models. Adversarially robust models are trained to resist small perturbations in input data, which improves model reliability but may affect the feasibility and validity of the recourses (counterfactual explanations) provided to affected individuals. The authors present both theoretical and empirical analyses to investigate how the degree of adversarial robustness impacts the cost and validity of recourses, demonstrating that stronger robustness typically leads to higher recourse costs and reduced validity. The paper highlights the inherent trade-offs between model robustness and the practical utility of actionable explanations.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<p>Reason if Not Eligible: n/a  </p>

<h2>How Actionability is Understood</h2>

<p>In this context, actionability is understood as the ability to provide actionable recourses (counterfactual explanations) to individuals who are impacted by model predictions. These recourses suggest the minimal changes that an individual should make to their input data to change the model’s outcome. The paper emphasizes that for recourses to be actionable, they need to be both feasible (i.e., easy to implement) and valid (i.e., likely to result in the desired outcome).  </p>

<blockquote>
  <p>“Actionable explanations are those that provide individuals with practical, implementable changes to their data to achieve a positive model prediction” (p. 4).  </p>
</blockquote>

<blockquote>
  <p>“The ability to generate valid and feasible recourses is a key aspect of actionability in machine learning models” (p. 6).</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<p>For counterfactual explanations to be actionable, they must meet two key criteria:</p>

<ol>
<li><p><strong>Feasibility (Cost):</strong> The cost of implementing the changes suggested by the explanation should be minimal, meaning that the required changes to the data are small and easy to apply.  </p></li>
<li><p><strong>Validity:</strong> The recourse should have a high probability of achieving the desired model outcome, ensuring that the changes will indeed result in a positive decision.  </p></li>
</ol>

<blockquote>
  <p>“Actionability is achieved when the cost of implementing the changes is low, and the probability of achieving the desired outcome is high” (p. 5).  </p>
</blockquote>

<blockquote>
  <p>“The balance between the cost of recourses and their validity is a crucial factor in determining actionability” (p. 5).</p>
</blockquote>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<p>Actionability is operationalized by evaluating the <strong>cost</strong> and <strong>validity</strong> of counterfactual explanations generated by adversarially robust and non-robust models. The paper examines these aspects both theoretically (using bounds on cost and validity) and empirically (using real-world datasets).  </p>

<ul>
<li><p><strong>Cost:</strong> Measured by the L2-norm distance between the original and counterfactual instances.  </p></li>
<li><p><strong>Validity:</strong> Measured by the probability that the counterfactual leads to the desired outcome (e.g., a positive loan approval prediction).  </p></li>
</ul>

<blockquote>
  <p>“We measure the cost of recourse as the L2 distance between the factual instance and the generated counterfactual” (p. 7).  </p>
</blockquote>

<blockquote>
  <p>“The validity of recourses is evaluated by computing the probability of achieving the desired model outcome” (p. 7).</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes – Clarity of the changes needed is inherent in the definition of actionable recourses, as they must specify the minimal changes required.  </p>

<p> &gt; “Clear explanations are necessary for actionability, as individuals need to know exactly what changes to make to their data” (p. 6).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes – The recourses must be relevant to the individual's context, meaning they should lead to a valid outcome for the individual’s situation.  </p>

<p> &gt; “Contextual relevance is key to actionability, as the changes suggested must result in a valid decision for the individual” (p. 6).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes – Feasibility is central to actionability, ensuring that the changes are easy for the affected individual to implement.  </p>

<p> &gt; “Feasible recourses are those that are realistic and easy to implement within the constraints of the individual’s context” (p. 5).  </p></li>
<li><p><strong>TI (Timeliness):</strong> No – Timeliness is not specifically addressed in the paper, but it may be an implicit factor in the validity and cost of the recourses.  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes – Actionable explanations must be understandable, so the individual knows how to act on the advice.  </p>

<p> &gt; “Explainability is essential for actionability, as users must understand the suggested changes to effectively implement them” (p. 6).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes – The recourses should align with the individual's goal, ensuring that the suggested changes move them toward achieving the desired outcome.  </p>

<p> &gt; “Goal alignment is a key aspect of actionable explanations, as the recourses must help individuals reach their desired outcome” (p. 5).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<p>The paper builds on existing theories in machine learning interpretability, particularly the work on counterfactual explanations and adversarial robustness. The authors extend these ideas by analyzing how adversarially robust models impact the quality of recourses generated by state-of-the-art algorithms.  </p>

<blockquote>
  <p>“This paper extends the existing literature on adversarial robustness and counterfactual explanations by analyzing the trade-offs between these two important properties” (p. 4).</p>
</blockquote>

<h2>Indicators or Metrics for Actionability</h2>

<p>The primary metrics used to measure actionability are <strong>cost</strong> (measured as the L2-norm distance between the factual and counterfactual instances) and <strong>validity</strong> (measured by the probability that the counterfactual will lead to the desired outcome).  </p>

<blockquote>
  <p>“We use the L2-norm to quantify the cost of recourses and measure validity by evaluating the probability of achieving the target model outcome” (p. 7).</p>
</blockquote>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Adversarial robustness introduces challenges, such as increased cost and reduced validity of recourses, which can hinder actionability.  </p></li>
<li><p><strong>Enablers:</strong> Non-robust models, which provide lower-cost and higher-validity recourses, enable more effective actionability.  </p></li>
</ul>

<blockquote>
  <p>“The increased cost and decreased validity of recourses in adversarially robust models create a significant barrier to actionability” (p. 8).  </p>
</blockquote>

<blockquote>
  <p>“Non-robust models provide lower-cost and higher-validity recourses, facilitating more actionable explanations” (p. 8).</p>
</blockquote>

<h2>Relation to Existing Literature</h2>

<p>The paper fills a gap in the existing literature by explicitly examining the trade-offs between adversarial robustness and actionability, an area that has received little attention in previous work.  </p>

<blockquote>
  <p>“Our work is one of the first to examine the trade-offs between adversarial robustness and actionable explanations, a gap that existing literature has not adequately addressed” (p. 4).</p>
</blockquote>

<h2>Summary</h2>

<p>This paper investigates the trade-offs between adversarial robustness and actionable explanations in machine learning models. It provides both theoretical and empirical analyses, demonstrating that adversarially robust models tend to increase the cost and decrease the validity of algorithmic recourses, making them less actionable. The findings highlight the inherent challenges of balancing model robustness with the need for reliable and feasible recourses, and suggest that adversarial robustness can complicate the provision of actionable insights.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 90 – The paper addresses an important and underexplored area in machine learning, offering valuable insights into the trade-offs between model robustness and explainability.  </p></li>
<li><p><strong>Operationalization Score:</strong> 85 – The paper presents a clear framework for evaluating the cost and validity of recourses but could benefit from more detailed guidance for practitioners on how to implement these findings.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Feasible recourses are those that are realistic and easy to implement within the constraints of the individual’s context” (p. 5).  </p></li>
<li><p>“Actionability is achieved when the cost of implementing the changes is low, and the probability of achieving the desired outcome is high” (p. 5).  </p></li>
<li><p>“The increased cost and decreased validity of recourses in adversarially robust models create a significant barrier to actionability” (p. 8).  </p></li>
<li><p>“Our work is one of the first to examine the trade-offs between adversarial robustness and actionable explanations” (p. 4).</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Wachter, S., Mittelstadt, B., &amp; Russell, C. (2018). Counterfactual explanations without opening the black box: Automated decisions and the GDPR.  </p></li>
<li><p>Ustun, B., Spangher, A., &amp; Liu, Y. (2019). Actionable recourse in linear classification.  </p></li>
<li><p>Pawelczyk, M., Broelemann, K., &amp; Kasneci, G. (2020). Learning model-agnostic counterfactual explanations for tabular data.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: FACE: Feasible and Actionable Counterfactual Explanations</p>

<p>Authors: Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, Peter Flach</p>

<p>DOI: https://doi.org/10.1145/3375627.3375850</p>

<p>Year: 2020</p>

<p>Publication Type: Conference</p>

<p>Discipline/Domain: Artificial Intelligence / Machine Learning</p>

<p>Subdomain/Topic: Explainable AI (XAI), Counterfactual Explanations</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 90</p>

<p>Operationalization Score: 95</p>

<p>Contains Definition of Actionability: Yes (implicit and explicit via feasibility + actionable path requirements)</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Yes</p>

<p>Contains Interpretability: Yes</p>

<p>Contains Framework/Model: Yes (FACE algorithm)</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Conceptual + Algorithmic with Empirical Demonstration</p>

<p>Study Context: Algorithmic explainability for decision-making systems</p>

<p>Geographic/Institutional Context: University of Bristol, University of Ghent</p>

<p>Target Users/Stakeholders: Individuals receiving automated decisions (e.g., loan applicants), AI practitioners, policy/regulation compliance</p>

<p>Primary Contribution Type: Conceptual framework + Algorithm</p>

<p>CL: Yes — clarity of feasible, coherent, and interpretable path is essential for actionability.</p>

<p>CR: Yes — contextual relevance to real-world feasibility emphasized.</p>

<p>FE: Yes — feasibility explicitly required for actionability.</p>

<p>TI: Partial — timeliness is not central, but feasibility implicitly assumes achievable change within realistic time.</p>

<p>EX: Yes — explainability as part of model-agnostic, understandable paths.</p>

<p>GA: Yes — goal alignment with desired class/outcome is fundamental.</p>

<p>Reason if Not Eligible: N/A</p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>FACE: Feasible and Actionable Counterfactual Explanations  </p>

<p><strong>Authors:</strong>  </p>

<p>Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, Peter Flach  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1145/3375627.3375850  </p>

<p><strong>Year:</strong>  </p>

<p>2020  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Conference  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Artificial Intelligence / Machine Learning  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Explainable AI, Counterfactual Explanations  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses limitations in existing counterfactual explanation methods in machine learning, specifically focusing on producing counterfactuals that are not only close in feature space but also <em>feasible</em> and <em>actionable</em>. It is designed for practical decision-making contexts, such as loan approvals, where advice must be realistic and aligned with the user’s circumstances.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>University of Bristol, University of Ghent  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Loan applicants, individuals affected by automated decision systems, explainability tool developers, regulators.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual + Algorithmic with empirical demonstration on synthetic and MNIST datasets.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Novel algorithm (FACE) + conceptual reframing of counterfactual actionability.  </p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This paper critiques the dominant “closest possible world” approach to counterfactual explanations, highlighting that such counterfactuals may reside in low-density, unrealistic regions of the data space and lack feasible paths for the individual to achieve the desired state. The authors introduce <em>FACE</em> — Feasible and Actionable Counterfactual Explanations — which generate counterfactuals coherent with the data distribution and connected via high-density, feasible paths to the original instance. FACE uses density-weighted shortest paths in a graph constructed from the data to identify achievable transitions. Demonstrations on synthetic datasets and MNIST illustrate the approach. FACE improves actionability by incorporating feasibility constraints, domain knowledge, and classifier confidence thresholds.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<hr />

<h2>How Actionability is Understood</h2>

<p>The paper defines actionability in terms of producing counterfactuals that are:</p>

<ul>
<li><p>Situated in high-density regions of the feature space.</p></li>
<li><p>Connected to the original data point by a feasible, realistic transformation path.</p></li>
</ul>

<blockquote>
  <p>“We identify two essential properties of counterfactual explanations: feasibility and actionability” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“…providing actionable and feasible paths to transform a selected instance into one that meets a certain goal” (p. 1)  </p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Feasibility of the counterfactual state (achievable in real life).</p></li>
<li><p>High-density region representation (coherence with data distribution).</p></li>
<li><p>Existence of a feasible path with short length and high density.</p></li>
<li><p>Avoidance of unrealistic or offensive prescriptions (e.g., changing immutable attributes).</p></li>
<li><p>Alignment with desired class outcome and real-world constraints.</p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> FACE (Feasible and Actionable Counterfactual Explanations)</p></li>
<li><p><strong>Methods/Levers:</strong> Density-weighted shortest path search over a graph of data points.</p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Construct a graph using KDE, k-NN, or ε-graph based on dataset.  </p>

<p> 2. Apply prediction confidence and density thresholds.  </p>

<p> 3. Remove infeasible transitions using domain constraints (immutable/conditionally mutable features).  </p>

<p> 4. Run Dijkstra’s algorithm to find shortest high-density path to a target class instance.</p></li>
<li><p><strong>Data &amp; Measures:</strong> Density estimates (KDE), classifier confidence scores, distance metrics.</p></li>
<li><p><strong>Implementation Context:</strong> Model-agnostic, applicable to tabular or image data.</p></li>
</ul>

<blockquote>
  <p>“Our approach… generates counterfactuals that are coherent with the underlying data distribution and supported by ‘feasible paths’ of change” (p. 1)  </p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL:</strong> Yes — counterfactuals must be interpretable and coherent with the data.</p></li>
<li><p><strong>CR:</strong> Yes — paths must be relevant to real-world conditions and domain constraints.</p></li>
<li><p><strong>FE:</strong> Yes — feasibility is explicitly central.</p></li>
<li><p><strong>TI:</strong> Partial — implicitly considered via feasible steps achievable over time.</p></li>
<li><p><strong>EX:</strong> Yes — explanations are model-agnostic and understandable.</p></li>
<li><p><strong>GA:</strong> Yes — targets aligned with desired outcome class.</p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> High-density path requirement.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Counterfactual and contrastive explanations literature (Wachter et al., 2017).</p></li>
<li><p>Graph-theoretic shortest paths (Dijkstra’s algorithm).</p></li>
<li><p>Kernel density estimation for distribution-aware distances.</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Density thresholds.</p></li>
<li><p>Prediction confidence thresholds.</p></li>
<li><p>Path length in density-weighted space.</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Low-density/unrealistic counterfactuals; immutable features; classifier uncertainty in sparse regions.</p></li>
<li><p><strong>Enablers:</strong> Density-weighted feasible paths; domain knowledge constraints; customizable cost functions.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>FACE is compared against Wachter et al. (2017), Ustun et al. (2019), Russell (2019), and Waa et al. (2018), surpassing them in combining model-agnostic applicability, discrete feature handling, and explicit feasibility/actionability requirements.</p>

<hr />

<h2>Summary</h2>

<p>The authors introduce FACE, a method for generating counterfactual explanations that are feasible and actionable, addressing gaps in the current “closest possible world” paradigm. FACE ensures counterfactuals are located in high-density regions and connected to the original instance through realistic transformation paths. This is operationalized using density-weighted shortest path algorithms with feasibility constraints. The method is model-agnostic, supports discrete features, incorporates domain restrictions, and produces explanations coherent with the data distribution. Demonstrations show FACE avoids impractical or misleading counterfactuals, offering more trustworthy and implementable recommendations.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 90 — Strong and explicit conceptualization of actionability in counterfactual explanations; well-integrated into methodological proposal.</p></li>
<li><p><strong>Operationalization Score:</strong> 95 — Detailed algorithmic approach, with parameters, constraints, and examples, fully linked to achieving actionability.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“We identify two essential properties of counterfactual explanations: feasibility and actionability” (p. 2)</p></li>
<li><p>“Providing actionable and feasible paths to transform a selected instance into one that meets a certain goal” (p. 1)</p></li>
<li><p>“Feasibility of the counterfactual data point, continuity and feasibility of the path linking it with the data point being explained, and high density along this path” (p. 3)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Wachter, Mittelstadt, &amp; Russell (2017) — Counterfactual Explanations framework.</p></li>
<li><p>Ustun, Spangher, &amp; Liu (2019) — Actionable recourse.</p></li>
<li><p>Russell (2019) — Diverse coherent explanations.</p></li>
<li><p>Waa et al. (2018) — Local foil trees for contrastive explanations.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Evaluating the understandability and actionability of online CKD educational materials</p>

<p>Authors: Emi Furukawa, Tsuyoshi Okuhara, Hiroko Okada, Yuriko Nishiie, Takahiro Kiuchi</p>

<p>DOI: https://doi.org/10.1007/s10157-023-02401-6</p>

<p>Year: 2024</p>

<p>Publication Type: Journal</p>

<p>Discipline/Domain: Health Communication / Nephrology</p>

<p>Subdomain/Topic: Chronic Kidney Disease (CKD) patient education, online health information evaluation</p>

<p>Eligibility: Eligible</p>

<p>Overall Relevance Score: 85</p>

<p>Operationalization Score: 80</p>

<p>Contains Definition of Actionability: Yes (implicit via PEMAT framework and study framing)</p>

<p>Contains Systematic Features/Dimensions: Yes</p>

<p>Contains Explainability: Partial</p>

<p>Contains Interpretability: Yes (via understandability dimension)</p>

<p>Contains Framework/Model: Yes (Japanese version of PEMAT-P)</p>

<p>Operationalization Present: Yes</p>

<p>Primary Methodology: Quantitative content analysis</p>

<p>Study Context: Evaluation of Japanese-language online CKD educational webpages</p>

<p>Geographic/Institutional Context: Japan; The University of Tokyo</p>

<p>Target Users/Stakeholders: CKD patients, their families, general public</p>

<p>Primary Contribution Type: Empirical evaluation and methodological application</p>

<p>CL: Yes</p>

<p>CR: Yes</p>

<p>FE: Yes</p>

<p>TI: No</p>

<p>EX: Partial</p>

<p>GA: Partial</p>

<p>Reason if Not Eligible: N/A</p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Evaluating the understandability and actionability of online CKD educational materials  </p>

<p><strong>Authors:</strong>  </p>

<p>Emi Furukawa, Tsuyoshi Okuhara, Hiroko Okada, Yuriko Nishiie, Takahiro Kiuchi  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1007/s10157-023-02401-6  </p>

<p><strong>Year:</strong>  </p>

<p>2024  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Health Communication / Nephrology  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Chronic Kidney Disease (CKD) patient education, online health information evaluation  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>CKD is prevalent yet under-recognized in Japan, with low public awareness and limited health literacy. Online resources are a key channel for patient education but may lack clarity and practical guidance to support behavior change.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Japan; conducted by The University of Tokyo  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>CKD patients, their families, general public  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Quantitative content analysis of Japanese-language CKD webpages using PEMAT-P, GQS, and jReadability  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Empirical evaluation of online educational material quality and actionability  </p>

<h2>General Summary of the Paper</h2>

<p>This study systematically evaluated 186 Japanese-language online educational materials on chronic kidney disease (CKD) for their understandability and actionability using the Japanese version of the Patient Education Materials Assessment Tool for Printed Materials (PEMAT-P). The analysis also considered content quality (GQS) and readability (jReadability). The results revealed low average scores for understandability (61.5%) and especially actionability (38.7%), with lifestyle modification pages outperforming those on symptoms or treatment. Materials from for-profit companies tended to be more understandable and visually supportive than those from medical or academic institutions. The authors recommend adopting plain language, defining medical terms, and using clear visual aids and actionable tools to improve patient engagement and health behaviors.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong>  </p>

<h2>How Actionability is Understood</h2>

<p>Actionability is framed through the PEMAT definition: materials are actionable if they clearly identify actions the user can take, break them into explicit steps, use direct address, provide tangible tools, and employ visual aids to facilitate action.  </p>

<blockquote>
  <p>“PEMAT systematically examines how the required action points are presented” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“The material clearly identifies at least one action the user can take… breaks down any action into explicit steps… provides tangible tools” (Table 2, p. 5)  </p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Clearly stated, specific actions for the user  </p></li>
<li><p>Directly addressing the user when describing actions  </p></li>
<li><p>Breaking actions into explicit, manageable steps  </p></li>
<li><p>Providing tangible tools (e.g., checklists, planners)  </p></li>
<li><p>Using visual aids to make it easier to act on instructions  </p></li>
<li><p>Explaining how to use visual elements to support actions  </p></li>
</ul>

<h2><strong>How Actionability is Achieved / Operationalized</strong></h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Japanese version of PEMAT-P  </p></li>
<li><p><strong>Methods/Levers:</strong> Binary-item assessment of 7 actionability criteria (agree/disagree)  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify CKD webpages → classify by topic/source/audience → score using PEMAT-P actionability items → analyze by ANOVA and post-hoc tests  </p></li>
<li><p><strong>Data &amp; Measures:</strong> Actionability percentage score (threshold 70% for acceptable)  </p></li>
<li><p><strong>Implementation Context:</strong> Japanese-language CKD patient educational webpages  </p></li>
</ul>

<blockquote>
  <p>“We calculated the PEMAT-P scores… multiplying the result by 100 to obtain a percentage… set the threshold of 70% to be considered… actionable” (p. 3)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — “Many had difficulty using only common, everyday language and did not explain medical terms” (p. 3)  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — lifestyle modification materials were more relevant and actionable than disease overview (p. 4)  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — tangible tools/checklists suggested for feasibility (p. 6)  </p></li>
<li><p><strong>TI (Timeliness):</strong> No explicit link found  </p></li>
<li><p><strong>EX (Explainability):</strong> Partial — some use of captions, but many visual aids unclear (p. 5)  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — lifestyle recommendations aligned with health goals (p. 4)  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Use of plain language, structured layout, visual reinforcement, and defined terms  </p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>PEMAT framework (AHRQ)  </p></li>
<li><p>National Action Plan on Health Literacy (U.S. HHS)  </p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>PEMAT-P actionability score (% of applicable items marked “agree”)  </p></li>
<li><p>Threshold ≥70% considered actionable  </p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Excessive medical jargon; lack of visual aids for actions; absence of tangible tools; unclear illustrations; missing summaries  </p></li>
<li><p><strong>Enablers:</strong> Use of plain language; clear, patient-centered visuals; structured actionable steps; commercial company design practices  </p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Findings align with prior studies showing lower actionability than understandability, and the need for better visual design and plain language. Contrasts with English-language materials where public institutions often perform better.  </p>

<h2>Summary</h2>

<p>The paper offers a robust, operationalized view of actionability grounded in the PEMAT framework, applied to CKD patient education materials in Japan. Actionability hinges on clear, user-directed, and stepwise guidance supplemented by tangible tools and visual aids. The study shows that most current materials fall short, particularly those from medical and academic institutions, while commercial entities fare better in design and clarity. Recommendations include adopting plain language, defining technical terms, improving visual supports, and integrating practical tools to enhance patient engagement and self-management. This makes the paper valuable both for conceptual understanding and for methodological replication in evaluating other health topics.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 85 — Strong conceptual framing via PEMAT and clear link between attributes and actionability, though no novel theoretical definition beyond operational tool.  </p></li>
<li><p><strong>Operationalization Score:</strong> 80 — Fully operationalized through PEMAT-P items and scoring; provides detailed criteria and thresholds, but lacks qualitative exploration of patient-perceived actionability.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“PEMAT systematically examines how the required action points are presented” (p. 2)  </p></li>
<li><p>“The material clearly identifies at least one action the user can take” (Table 2, p. 5)  </p></li>
<li><p>“Lacked clear and concise charts and illustrations to encourage action” (p. 1)  </p></li>
<li><p>“Webpages… lacked visual aids to encourage the audience to take action” (p. 3)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Shoemaker SJ et al. (2014) — Development of PEMAT (Patient Educ Couns)  </p></li>
<li><p>National Action Plan to Improve Health Literacy (U.S. HHS, 2010)  </p></li>
<li><p>Morony S et al. (2017) — CKD lifestyle info and actionability analysis</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Directive Explanations for Actionable Explainability in Machine Learning Applications  </p>

<p>Authors: Ronal Singh, Tim Miller, Henrietta Lyons, Liz Sonenberg, Eduardo Velloso, Frank Vetere, Piers Howe, Paul Dourish  </p>

<p>DOI: 10.1145/3579363  </p>

<p>Year: 2023  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Human-Computer Interaction / Artificial Intelligence  </p>

<p>Subdomain/Topic: Explainable AI (XAI), Counterfactual Explanations, Actionable Recourse  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (explicitly defines “directive explanations” as a form of actionable explanation)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (MDP-based model)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Mixed Methods (Quantitative + Qualitative user studies, conceptual modeling)  </p>

<p>Study Context: Credit scoring and employee satisfaction prediction systems  </p>

<p>Geographic/Institutional Context: United States participants, University of Melbourne research team  </p>

<p>Target Users/Stakeholders: Loan officers, HR officers, decision recipients (customers, employees)  </p>

<p>Primary Contribution Type: Conceptual model + empirical evaluation  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Yes  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong> Directive Explanations for Actionable Explainability in Machine Learning Applications  </p>

<p><strong>Authors:</strong> Ronal Singh, Tim Miller, Henrietta Lyons, Liz Sonenberg, Eduardo Velloso, Frank Vetere, Piers Howe, Paul Dourish  </p>

<p><strong>DOI:</strong> 10.1145/3579363  </p>

<p><strong>Year:</strong> 2023  </p>

<p><strong>Publication Type:</strong> Journal  </p>

<p><strong>Discipline/Domain:</strong> Human-Computer Interaction / Artificial Intelligence  </p>

<p><strong>Subdomain/Topic:</strong> Explainable AI, Counterfactuals, Actionable Recourse  </p>

<p><strong>Contextual Background:</strong> The paper addresses the gap between counterfactual explanations (which state how inputs must differ for a different outcome) and actionable explanations (which tell recipients what actions to take). It proposes “directive explanations” that specify concrete or generic actions to achieve a desired outcome, formalizing them via Markov Decision Processes.  </p>

<p><strong>Geographic/Institutional Context:</strong> Conducted by University of Melbourne with US-based MTurk participants.  </p>

<p><strong>Target Users/Stakeholders:</strong> Decision recipients, intermediary decision communicators, designers of ML-based decision systems.  </p>

<p><strong>Primary Methodology:</strong> Mixed Methods (Quantitative + Qualitative studies + conceptual modeling)  </p>

<p><strong>Primary Contribution Type:</strong> Conceptual model (MDP framework) + empirical evaluation.</p>

<h2>General Summary of the Paper</h2>

<p>The authors propose <strong>directive explanations</strong> as a way to make AI explanations more actionable by explicitly providing sequences of actions that lead from the current state to a desired counterfactual outcome. They define two types—<strong>directive-specific</strong> (concrete actions) and <strong>directive-generic</strong> (action categories)—and formalize generation using an MDP framework. Two user studies in credit scoring and employee satisfaction domains compare directive explanations with non-directive counterfactuals, showing a clear preference for directive forms. Thematic analysis reveals that preferences are shaped by feasibility, social sensitivity, and autonomy. The authors conclude that actionability is context- and user-dependent, advocating a human-centered, context-specific approach.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>Actionability is framed as enabling <strong>recourse</strong>—guiding individuals not just on what feature values would yield a different outcome, but on <strong>what specific or generic actions they can take</strong> to reach that state.  </p>

<blockquote>
  <p>“A directive explanation … offers specific actions an individual could take to achieve their desired outcome.” (p. 1)  </p>
</blockquote>

<blockquote>
  <p>“Counterfactual explanations should be directive in that they should include suggestions or recommendations of the action(s) the individual could perform…” (p. 2)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Ties counterfactuals to <strong>mutable and feasible actions</strong>.</p></li>
<li><p>Specifies <strong>sequences</strong> of dependent actions, not just one-step changes.</p></li>
<li><p>Accounts for <strong>action costs</strong> and individual feasibility.</p></li>
<li><p>Provides either <strong>specific actionable steps</strong> or <strong>generic guidance</strong> to preserve autonomy.</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> MDP-based directive explanation generation model.  </p></li>
<li><p><strong>Methods/Levers:</strong> Use of Monte Carlo Tree Search to find policies transitioning from factual to counterfactual states.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong></p>

<p> 1. Generate counterfactual states using existing algorithms (e.g., Russell 2019).</p>

<p> 2. Define mutable features and possible actions.</p>

<p> 3. Model state transitions and action costs in MDP.</p>

<p> 4. Search for optimal policy (action sequence) to reach counterfactual.</p>

<p> 5. Post-process for directive-generic explanations by grouping actions.</p></li>
<li><p><strong>Data &amp; Measures:</strong> Credit scoring and employee satisfaction datasets; user preference rankings; thematic coding of qualitative justifications.</p></li>
<li><p><strong>Implementation Context:</strong> Simulated loan officer and HR officer decision communication.</p></li>
</ul>

<blockquote>
  <p>“Actions from πi must lead from x to ci… model must capture different ways to achieve specific outcomes… account for action costs…” (p. 6)  </p>
</blockquote>

<blockquote>
  <p>“Policy πi is the source of the directives in the directive explanations.” (p. 6–7)</p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — explicit link between action and outcome. “Provides clear actions… so the customer will know what to do next.” (p. 16)</p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — tailored to recipient’s situation and domain. (p. 16–17)</p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — consideration of whether directives are realistic and achievable. (p. 17)</p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — relevance discussed when outcomes are imminent, but not formalized as a dimension.</p></li>
<li><p><strong>EX (Explainability):</strong> Yes — explanations remain interpretable, showing causal pathways.</p></li>
<li><p><strong>GA (Goal Alignment):</strong> Yes — directives are aligned with recipient’s desired outcome.</p></li>
<li><p><strong>Other Dimensions:</strong> Autonomy (directive-generic explanations preserve choice), Social Acceptability (avoid condescending or overly personal directives).</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Counterfactual explanations literature (Wachter et al. 2017)</p></li>
<li><p>Algorithmic recourse and causal modeling (Karimi et al. 2021)</p></li>
<li><p>Markov Decision Processes and planning theory (Puterman 2014, Geffner &amp; Bonet 2013)</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>User preference ranking between explanation types.</p></li>
<li><p>Qualitative themes on perceived usefulness, feasibility, autonomy.</p></li>
<li><p>Domain-specific acceptance patterns.</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Social sensitivity of directives, infeasibility of actions, lack of user autonomy, condescending tone.</p></li>
<li><p><strong>Enablers:</strong> Clear linkage between actions and outcomes, multiple feasible options, domain familiarity, personalization.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Builds on counterfactual explanations but addresses lack of explicit action guidance. Extends recourse work by modeling multi-step action sequences and accounting for costs, aligning with causal model-based proposals.</p>

<h2>Summary</h2>

<p>This paper advances the concept of <strong>actionable explainability</strong> by formalizing “directive explanations” that move beyond stating hypothetical changes to prescribing concrete or generic actions. It operationalizes this through an MDP model that sequences feasible, cost-sensitive actions from the factual to the counterfactual state. Two empirical studies demonstrate that users prefer directive explanations, especially in unfavorable decision contexts, though preferences depend on domain, feasibility, and social considerations. The work contributes both a theoretical framework for actionable recourse and empirical evidence supporting directive over non-directive counterfactuals, while emphasizing the need for human-centered and context-specific tailoring.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Explicitly defines actionability, ties it to recourse, offers detailed conceptual and empirical analysis with multiple dimensions.</p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Provides a full computational method (MDP model) and empirical validation; could improve by integrating cost/feasibility personalization into generation.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“[A] directive explanation … offers specific actions an individual could take to achieve their desired outcome.” (p. 1)</p></li>
<li><p>“Counterfactual explanations should be directive in that they should include suggestions or recommendations of the action(s) the individual could perform…” (p. 2)</p></li>
<li><p>“Actions from πi must lead from x to ci… model must capture different ways to achieve specific outcomes… account for action costs…” (p. 6)</p></li>
<li><p>“Provides clear actions… so the customer will know what to do next.” (p. 16)</p></li>
<li><p>“I picked [directive-generic] based on how feasible I thought each strategy would be.” (p. 17)</p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Wachter et al. 2017 (counterfactual explanations)</p></li>
<li><p>Karimi et al. 2021 (algorithmic recourse via causal models)</p></li>
<li><p>Tsirtsis et al. 2021 (sequential decision-making counterfactuals)</p></li>
<li><p>Russell 2019 (diverse counterfactual generation)</p></li>
<li><p>Puterman 2014; Geffner &amp; Bonet 2013 (MDP and planning frameworks)</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Evaluating Online and Offline Health Information With the Patient Education Materials Assessment Tool: Protocol for a Systematic Review  </p>

<p>Authors: Emi Furukawa, Tsuyoshi Okuhara, Mingxin Liu, Hiroko Okada, Takahiro Kiuchi  </p>

<p>DOI: 10.2196/63489  </p>

<p>Year: 2025  </p>

<p>Publication Type: Journal Article (Protocol)  </p>

<p>Discipline/Domain: Health Communication / Health Literacy  </p>

<p>Subdomain/Topic: Patient Education Materials Evaluation  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 85  </p>

<p>Operationalization Score: 70  </p>

<p>Contains Definition of Actionability: Yes  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: No  </p>

<p>Contains Interpretability: No  </p>

<p>Contains Framework/Model: Yes  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Systematic Review Protocol (Conceptual/Methodological)  </p>

<p>Study Context: Systematic review of studies evaluating patient education materials using the PEMAT  </p>

<p>Geographic/Institutional Context: International; led by The University of Tokyo, Japan  </p>

<p>Target Users/Stakeholders: Health communication researchers, patient educators, health institutions, policy-makers  </p>

<p>Primary Contribution Type: Methodological framework for systematic review  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: No  </p>

<p>EX: No  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Evaluating Online and Offline Health Information With the Patient Education Materials Assessment Tool: Protocol for a Systematic Review  </p>

<p><strong>Authors:</strong>  </p>

<p>Emi Furukawa, Tsuyoshi Okuhara, Mingxin Liu, Hiroko Okada, Takahiro Kiuchi  </p>

<p><strong>DOI:</strong>  </p>

<p>10.2196/63489  </p>

<p><strong>Year:</strong>  </p>

<p>2025  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Article (Protocol)  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Health Communication / Health Literacy  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Patient Education Materials Evaluation  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper presents a protocol for a systematic review of studies using the Patient Education Materials Assessment Tool (PEMAT) to evaluate the understandability and actionability of health information materials across different formats, contexts, and languages.  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>International scope; coordinated by The University of Tokyo Hospital and Graduate School of Medicine.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Health communication researchers, patient education specialists, health literacy advocates, public health institutions.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual and methodological protocol for systematic review.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Methodological framework and synthesis approach.  </p>

<h2>General Summary of the Paper</h2>

<p>This protocol outlines a systematic review plan to analyze how the PEMAT has been used to evaluate the understandability and actionability of patient education materials in various health contexts. It defines clear inclusion and exclusion criteria, search strategies across five major databases, and detailed plans for data extraction and synthesis. The review aims to map evaluated domains, assess average quality levels, and identify research and practice gaps for improving health materials. It also plans subgroup analyses by material type, clinical field, and source type. By integrating and comparing PEMAT-based evaluations globally, the authors expect to provide actionable recommendations for developing more understandable and actionable patient education resources.</p>

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<h2>How Actionability is Understood</h2>

<p>Actionability is defined by PEMAT as “the likelihood that the reader or viewer will know how to act on the information presented in the material” (p. 2). The review treats actionability as a measurable, comparable quality dimension that determines whether materials can guide patients and the public toward specific health behaviors.  </p>

<blockquote>
  <p>“Actionability refers to the likelihood that the reader or viewer will know how to act on the information presented in the material.” (p. 2)  </p>
</blockquote>

<blockquote>
  <p>“Understanding the material alone is insufficient; a separate evaluation is necessary to determine whether audience can translate the material’s content into actionable behavior.” (p. 2)</p>
</blockquote>

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Clear, specific instructions for action.  </p></li>
<li><p>Concrete steps enabling readers to perform recommended behaviors.  </p></li>
<li><p>Contextual relevance to target users.  </p></li>
<li><p>Structured presentation that facilitates translation from information to behavior.</p></li>
</ul>

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Patient Education Materials Assessment Tool (PEMAT)  </p></li>
<li><p><strong>Methods/Levers:</strong> Application of PEMAT-P (print) and PEMAT-A/V (audiovisual) formats, scoring actionability and understandability separately.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Literature search → screening → data extraction of PEMAT scores → subgroup analysis by material type, clinical field, source.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> PEMAT’s actionability items (20–26 for print, 20–22 &amp; 25 for audiovisual), scored as percentages.  </p></li>
<li><p><strong>Implementation Context:</strong> Applied in diverse cultural and linguistic contexts for cross-study comparison.  </p></li>
</ul>

<blockquote>
  <p>“On the practical side, the PEMAT visualizes the challenges of materials to find the most understandable and actionable materials among the many available.” (p. 2)  </p>
</blockquote>

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Clear presentation and understandable content are necessary precursors to action.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Materials must match the needs and settings of the intended audience.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — Materials must present actions the audience can realistically perform.  </p></li>
<li><p><strong>TI (Timeliness):</strong> No — Not explicitly linked to actionability.  </p></li>
<li><p><strong>EX (Explainability):</strong> No — Not explicitly tied to actionability.  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — Alignment with intended health behavior is implied but not systematically stated.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Understandability as a prerequisite; cultural and linguistic adaptability.</p></li>
</ul>

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Health literacy theory.  </p></li>
<li><p>Garner et al.’s three-step model of audience interaction with materials (reading, understanding, responding).  </p></li>
<li><p>Organizational health literacy frameworks (Healthy People 2030).</p></li>
</ul>

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>PEMAT actionability score (0–100%).  </p></li>
<li><p>Item-level scoring on explicit action guidance and steps.</p></li>
</ul>

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> Lack of patient perspective in PEMAT scoring; heterogeneity in methods across studies; exclusion of non-English literature.  </p></li>
<li><p><strong>Enablers:</strong> Standardized, validated PEMAT tool; availability in multiple languages; ability to compare across contexts.</p></li>
</ul>

<h2>Relation to Existing Literature</h2>

<p>Builds on prior scoping reviews of health material quality assessment but is the first systematic synthesis of PEMAT-based evaluations.</p>

<h2>Summary</h2>

<p>This protocol establishes a systematic approach for synthesizing global evidence on the understandability and actionability of patient education materials, as assessed by PEMAT. It conceptualizes actionability as the material’s capacity to guide audiences toward specific health actions and positions clarity, contextual relevance, and feasibility as essential attributes. By collecting and comparing PEMAT scores across contexts and formats, the review will generate insights into areas where materials succeed or fall short in supporting behavior change. It will also identify priority areas for improvement in health communication practices.</p>

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 85 — Strong, explicit conceptualization of actionability and its components within PEMAT; multiple dimensions tied to actionability.  </p></li>
<li><p><strong>Operationalization Score:</strong> 70 — Provides a clear methodological framework for assessing actionability via PEMAT, but as a protocol, lacks empirical application results.</p></li>
</ul>

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Actionability refers to the likelihood that the reader or viewer will know how to act on the information presented in the material.” (p. 2)  </p></li>
<li><p>“Understanding the material alone is insufficient; a separate evaluation is necessary to determine whether audience can translate the material’s content into actionable behavior.” (p. 2)  </p></li>
<li><p>“On the practical side, the PEMAT visualizes the challenges of materials to find the most understandable and actionable materials among the many available.” (p. 2)  </p></li>
</ul>

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Shoemaker et al., 2014 — Original PEMAT development.  </p></li>
<li><p>Garner et al., 2012 — Framework for evaluating patient information leaflets.  </p></li>
<li><p>CDC Clear Communication Index.</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR  </p>

<p>Authors: Sandra Wachter, Brent Mittelstadt, Chris Russell  </p>

<p>DOI: 10.2139/ssrn.3063289  </p>

<p>Year: 2018  </p>

<p>Publication Type: Journal Article  </p>

<p>Discipline/Domain: Law &amp; Technology  </p>

<p>Subdomain/Topic: Algorithmic Decision-Making, Data Protection, Explainable AI  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 95  </p>

<p>Operationalization Score: 90  </p>

<p>Contains Definition of Actionability: Yes (implicit, in terms of what makes explanations actionable for data subjects)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Yes  </p>

<p>Contains Interpretability: Yes  </p>

<p>Contains Framework/Model: Yes (Counterfactual Explanation framework)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Conceptual + Technical Demonstration  </p>

<p>Study Context: Automated decision-making under GDPR constraints  </p>

<p>Geographic/Institutional Context: European Union, GDPR context  </p>

<p>Target Users/Stakeholders: Data subjects, policymakers, data controllers, AI developers  </p>

<p>Primary Contribution Type: Conceptual framework + technical method proposal  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Partial  </p>

<p>TI: Partial  </p>

<p>EX: Yes  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR  </p>

<p><strong>Authors:</strong>  </p>

<p>Sandra Wachter, Brent Mittelstadt, Chris Russell  </p>

<p><strong>DOI:</strong>  </p>

<p>10.2139/ssrn.3063289  </p>

<p><strong>Year:</strong>  </p>

<p>2018  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal Article  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Law &amp; Technology  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Algorithmic Decision-Making, Data Protection, Explainable AI  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The paper addresses the problem of explaining complex algorithmic decisions under the GDPR without revealing proprietary or technical details of the models. It proposes counterfactual explanations as an alternative that focuses on actionable changes to input variables to achieve desired outcomes, aligning with transparency and accountability goals while bypassing technical and legal barriers to “opening the black box.”  </p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>European Union, GDPR regulatory environment.  </p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Data subjects, policymakers, regulators, AI system designers, data controllers.  </p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Conceptual analysis with technical implementation examples.  </p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Proposal and justification of a new explanation method (counterfactual explanations) with legal, philosophical, and technical grounding.</p>

<hr />

<h2>General Summary of the Paper</h2>

<p>The authors critique the GDPR’s limited and ambiguous provisions on explaining automated decisions, noting that they do not mandate disclosure of algorithms’ internal logic. They propose <em>counterfactual explanations</em>—“if-then” statements showing minimal changes to input variables that would alter a decision—as a practical, legally compatible way to inform, empower, and guide data subjects. This approach avoids revealing trade secrets or requiring technical interpretability, but still supports understanding, contesting, and influencing future decisions. They situate counterfactuals in philosophical discussions of knowledge and causality, review related work in AI, and present methods for generating them, illustrated with LSAT and Pima Diabetes datasets. They evaluate their legal compatibility with GDPR’s transparency, notification, and access provisions, and conclude that unconditional counterfactual explanations could balance transparency, accountability, and practical feasibility.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<hr />

<h2>How Actionability is Understood</h2>

<p>Actionability is framed in terms of explanations that enable the <em>data subject to act</em>—to understand a decision, contest it, or modify future outcomes—without needing to understand the internal model logic.  </p>

<blockquote>
  <p>“Looking at explanations as a means to help a data subject act rather than merely understand…” (p. 843)  </p>
</blockquote>

<blockquote>
  <p>“An explanation… does not necessarily hinge on… understanding how algorithmic systems function.” (p. 843)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Provides clear, minimal, relevant changes to variables that would alter the decision.  </p></li>
<li><p>Expressed in terms directly relevant to the individual’s circumstances.  </p></li>
<li><p>Supports specific goals: understanding, contesting, or changing outcomes.  </p></li>
<li><p>Avoids unnecessary technical or internal model details.  </p></li>
<li><p>Must be intelligible, concise, and accessible to non-experts.  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Counterfactual Explanations.  </p></li>
<li><p><strong>Methods/Levers:</strong> Optimization to find minimally different “possible worlds” producing a different decision outcome; distance metrics (e.g., weighted L1 norm).  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong>  </p>

<p> 1. Fix model parameters after training.  </p>

<p> 2. Search for an alternative input vector close to the original.  </p>

<p> 3. Ensure minimal and realistic changes (sparse changes).  </p>

<p> 4. Output human-readable “if-then” statements.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> LSAT and Pima Diabetes datasets; performance measured by plausibility and sparsity of changes.  </p></li>
<li><p><strong>Implementation Context:</strong> Applicable across domains where individual-level decisions are made.  </p></li>
</ul>

<blockquote>
  <p>“Unconditional counterfactual explanations should be given for positive and negative automated decisions…” (p. 844)  </p>
</blockquote>

<blockquote>
  <p>“If your LSAT was 34.0, you would have an average predicted score (0).” (p. 858)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — Must be “concise, transparent, intelligible” (p. 871).  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — Tailored to the individual’s data and decision context (p. 843–844).  </p></li>
<li><p><strong>FE (Feasibility):</strong> Partial — Mutability and practicality of changes considered but not guaranteed (p. 845).  </p></li>
<li><p><strong>TI (Timeliness):</strong> Partial — Can be given post-decision; real-time use possible but not core focus (p. 882).  </p></li>
<li><p><strong>EX (Explainability):</strong> Yes — Provides rationale via dependency on external facts (p. 845).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — Aims to support user goals (understand, contest, alter), but no explicit goal-matching process.  </p></li>
<li><p><strong>Other Dimensions:</strong> Legal compatibility; minimal intrusion on rights of others.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>Analytic philosophy of knowledge (“justified true belief,” counterfactual reasoning).  </p></li>
<li><p>Possible worlds semantics (David Lewis).  </p></li>
<li><p>Causal reasoning in fairness (Pearl).  </p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>Minimal number of changed variables (sparsity).  </p></li>
<li><p>Plausibility of changes (within realistic ranges).  </p></li>
<li><p>Relevance to individual’s mutable characteristics.</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong> GDPR’s limited scope for explanations; possible unchangeable variables; cost of computation; dynamic models.  </p></li>
<li><p><strong>Enablers:</strong> Model-agnostic applicability; computational efficiency; legal compatibility; minimal trade secret exposure.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>Contrasts with ML interpretability work focusing on internal logic; aligns with fairness literature using counterfactuals but for outcome change rather than fairness testing. Bridges law, philosophy, and AI explainability research.</p>

<hr />

<h2>Summary</h2>

<p>The paper redefines “actionability” for explanations of automated decisions under GDPR as enabling the data subject to understand, contest, or alter outcomes without requiring insight into a system’s internal workings. <em>Counterfactual explanations</em> operationalize this by showing minimal, plausible input changes that would have led to a different result, delivered in human-readable terms. This approach is legally compatible, computationally feasible, and preserves trade secrets and privacy. It is demonstrated through technical examples and evaluated against GDPR provisions, revealing that while the regulation does not mandate such explanations, they could serve as a practical, user-centered transparency mechanism. The authors advocate unconditional provision of counterfactuals to all affected individuals.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 95 — Strong, explicit linkage between explanation design and enabling user action; detailed conceptual and operational guidance.  </p></li>
<li><p><strong>Operationalization Score:</strong> 90 — Clear technical method with worked examples; some limitations on feasibility and mutability consideration.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“An explanation… does not necessarily hinge on… understanding how algorithmic systems function.” (p. 843)  </p></li>
<li><p>“Unconditional counterfactual explanations should be given for positive and negative automated decisions…” (p. 844)  </p></li>
<li><p>“If your LSAT was 34.0, you would have an average predicted score (0).” (p. 858)  </p></li>
<li><p>“Concise, transparent, intelligible and easily accessible form.” (p. 871)  </p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Lewis, <em>Counterfactuals</em> (1973)  </p></li>
<li><p>Pearl, <em>Causation</em> (2000)  </p></li>
<li><p>Kusner et al., “Counterfactual Fairness” (2018)  </p></li>
<li><p>Citron &amp; Pasquale, on hypothetical alterations in credit scoring (2014)</p></li>
</ul>

<h1>Paper Summary</h1>

<!--META_START-->

<p>Title: Assessing the understandability and actionability of online resources for patients undergoing hemodialysis  </p>

<p>Authors: Emi Furukawa, Tsuyoshi Okuhara, Hiroko Okada, Yumiko Fujitomo, Takahiro Kiuchi  </p>

<p>DOI: https://doi.org/10.1111/1744-9987.14221  </p>

<p>Year: 2025  </p>

<p>Publication Type: Journal  </p>

<p>Discipline/Domain: Health Communication / Nephrology  </p>

<p>Subdomain/Topic: Patient education, online health resources, health literacy assessment  </p>

<p>Eligibility: Eligible  </p>

<p>Overall Relevance Score: 78  </p>

<p>Operationalization Score: 85  </p>

<p>Contains Definition of Actionability: Yes (via PEMAT-P framework)  </p>

<p>Contains Systematic Features/Dimensions: Yes  </p>

<p>Contains Explainability: Partial  </p>

<p>Contains Interpretability: Partial  </p>

<p>Contains Framework/Model: Yes (PEMAT-P)  </p>

<p>Operationalization Present: Yes  </p>

<p>Primary Methodology: Quantitative / Cross-sectional evaluation study  </p>

<p>Study Context: Evaluation of Japanese-language online patient education materials on hemodialysis using PEMAT-P, GQS, and jReadability  </p>

<p>Geographic/Institutional Context: Japan (University of Tokyo Hospital, Graduate School of Medicine)  </p>

<p>Target Users/Stakeholders: Patients undergoing hemodialysis, healthcare providers, patient education material developers  </p>

<p>Primary Contribution Type: Empirical assessment and guidance for material improvement  </p>

<p>CL: Yes  </p>

<p>CR: Yes  </p>

<p>FE: Yes  </p>

<p>TI: No  </p>

<p>EX: Partial  </p>

<p>GA: Partial  </p>

<p>Reason if Not Eligible: N/A  </p>

<!--META_END-->

<p><strong>Title:</strong>  </p>

<p>Assessing the understandability and actionability of online resources for patients undergoing hemodialysis  </p>

<p><strong>Authors:</strong>  </p>

<p>Emi Furukawa, Tsuyoshi Okuhara, Hiroko Okada, Yumiko Fujitomo, Takahiro Kiuchi  </p>

<p><strong>DOI:</strong>  </p>

<p>https://doi.org/10.1111/1744-9987.14221  </p>

<p><strong>Year:</strong>  </p>

<p>2025  </p>

<p><strong>Publication Type:</strong>  </p>

<p>Journal  </p>

<p><strong>Discipline/Domain:</strong>  </p>

<p>Health Communication / Nephrology  </p>

<p><strong>Subdomain/Topic:</strong>  </p>

<p>Patient education, online health resources, health literacy assessment  </p>

<p><strong>Contextual Background:</strong>  </p>

<p>The study evaluates whether Japanese-language online materials for patients on hemodialysis are understandable and actionable enough to support health behavior change. It addresses issues in patient education content given the high prevalence and self-management demands of HD in Japan.</p>

<p><strong>Geographic/Institutional Context:</strong>  </p>

<p>Japan; University of Tokyo Hospital; Graduate School of Medicine, University of Tokyo.</p>

<p><strong>Target Users/Stakeholders:</strong>  </p>

<p>Patients undergoing hemodialysis, their families, healthcare providers, and material developers.</p>

<p><strong>Primary Methodology:</strong>  </p>

<p>Quantitative cross-sectional content evaluation.</p>

<p><strong>Primary Contribution Type:</strong>  </p>

<p>Empirical assessment with actionable recommendations for improving educational resources.</p>

<hr />

<h2>General Summary of the Paper</h2>

<p>This cross-sectional study assessed 194 Japanese-language online educational materials for patients undergoing hemodialysis (HD), focusing on understandability and actionability using the Japanese version of the Patient Education Materials Assessment Tool (PEMAT-P). Materials were also evaluated for quality (Global Quality Score, GQS) and readability (jReadability). The median understandability score was 66.7%, with only 38.7% meeting the ≥70% threshold; median actionability was 33.3%, with just 16.5% meeting the threshold. Common shortcomings included lack of plain language, absence of summaries, poor titling of visual aids, and limited use of tools that support concrete actions. Self-management materials scored significantly higher in actionability and readability compared to other content categories. For-profit company-produced materials tended to be more understandable and readable. The study concludes that many HD-related online resources are not presented in ways that enable patients to effectively use them for self-care.</p>

<hr />

<h2>Eligibility</h2>

<p>Eligible for inclusion: <strong>Yes</strong></p>

<hr />

<h2>How Actionability is Understood</h2>

<p>The paper adopts the PEMAT-P definition: actionability refers to how well materials enable patients to identify what to do based on the information, and to take concrete steps toward action.</p>

<blockquote>
  <p>“Actionability… evaluates how well patients can identify what they need to do based on the information presented.” (p. 203)  </p>
</blockquote>

<blockquote>
  <p>“Scores below 70% indicated poor… actionability, whereas scores of 70% or higher were considered… actionable.” (p. 203)</p>
</blockquote>

<hr />

<h2>What Makes Something Actionable</h2>

<ul>
<li><p>Clearly identifies at least one specific action a user can take.  </p></li>
<li><p>Addresses the user directly in describing actions.  </p></li>
<li><p>Breaks down actions into explicit steps.  </p></li>
<li><p>Provides tangible tools (e.g., checklists, planners) to facilitate the action.  </p></li>
<li><p>Uses visual aids to make instructions easier to follow.  </p></li>
<li><p>Explains how to interpret charts, graphs, or tables for taking action.  </p></li>
</ul>

<hr />

<h2>How Actionability is Achieved / Operationalized</h2>

<ul>
<li><p><strong>Framework/Approach Name(s):</strong> Japanese version of PEMAT-P (Patient Education Materials Assessment Tool for Printed Materials).  </p></li>
<li><p><strong>Methods/Levers:</strong> Binary scoring (agree/disagree) across seven actionability items; 70% threshold for acceptable actionability.  </p></li>
<li><p><strong>Operational Steps / Workflow:</strong> Identify actions, address user directly, break down steps, provide tools, explain visual aids, and ensure ease of acting upon instructions.  </p></li>
<li><p><strong>Data &amp; Measures:</strong> PEMAT-P scores; Kruskal–Wallis test for group differences; inter-rater reliability via Gwet’s AC1.  </p></li>
<li><p><strong>Implementation Context:</strong> Applied to Japanese online HD materials from diverse sources and content areas.</p></li>
</ul>

<blockquote>
  <p>“More than half of the materials satisfied Item 19… However, &lt;30%… met Item 21… Item 22… Item 24… and Item 25.” (p. 204)  </p>
</blockquote>

<blockquote>
  <p>“Self-management materials tended to offer more detailed instructions and utilized visual aids to facilitate readers in taking action…” (p. 206)</p>
</blockquote>

<hr />

<h2>Dimensions and Attributes of Actionability (Authors’ Perspective)</h2>

<ul>
<li><p><strong>CL (Clarity):</strong> Yes — linked via plain language and absence of distracting information.  </p></li>
<li><p><strong>CR (Contextual Relevance):</strong> Yes — self-management content most relevant and actionable.  </p></li>
<li><p><strong>FE (Feasibility):</strong> Yes — inclusion of tangible tools and breakdown of steps supports feasibility.  </p></li>
<li><p><strong>TI (Timeliness):</strong> No explicit link found.  </p></li>
<li><p><strong>EX (Explainability):</strong> Partial — explanation of how to use visual aids was rare (&lt;10%).  </p></li>
<li><p><strong>GA (Goal Alignment):</strong> Partial — some materials align with patient self-care goals (e.g., self-management), but not universal.  </p></li>
<li><p><strong>Other Dimensions Named by Authors:</strong> Use of visual aids, chunking information, providing summaries.</p></li>
</ul>

<hr />

<h2>Theoretical or Conceptual Foundations</h2>

<ul>
<li><p>PEMAT-P framework for defining and measuring understandability and actionability.  </p></li>
<li><p>Health literacy principles, including plain language and visual aid effectiveness.  </p></li>
<li><p>Prior literature on patient education in CKD and HD contexts.</p></li>
</ul>

<hr />

<h2>Indicators or Metrics for Actionability</h2>

<ul>
<li><p>PEMAT-P actionability subscore (% of items rated “agree” out of applicable items).  </p></li>
<li><p>≥70% threshold for actionable materials.</p></li>
</ul>

<hr />

<h2>Barriers and Enablers to Actionability</h2>

<ul>
<li><p><strong>Barriers:</strong>  </p>

<p> - Lack of summaries.  </p>

<p> - Inadequate titling/captioning of visual aids.  </p>

<p> - Minimal use of tangible tools for action.  </p>

<p> - Complex syntax and medical jargon.  </p></li>
<li><p><strong>Enablers:</strong>  </p>

<p> - Direct address to the user.  </p>

<p> - Clear identification of actions.  </p>

<p> - Detailed step-by-step instructions in self-management materials.  </p>

<p> - Effective visual aids used by for-profit company materials.</p></li>
</ul>

<hr />

<h2>Relation to Existing Literature</h2>

<p>The authors note similar deficiencies in English-language HD and CKD materials internationally, such as low readability and lack of action-oriented tools, suggesting a common challenge in health communication regardless of language or country.</p>

<hr />

<h2>Summary</h2>

<p>This study provides a quantitative assessment of the understandability and actionability of Japanese online educational materials for patients undergoing hemodialysis using the PEMAT-P framework. It finds that most materials fail to meet the actionability threshold, with common deficits including lack of plain language, absence of summaries, and minimal use of tangible action tools. Self-management materials stood out for higher actionability and readability, largely due to explicit action steps and supportive visual aids. For-profit companies tended to produce more understandable and readable content. The findings offer clear operational guidance — rooted in an established framework — for improving patient education materials so they can effectively bridge the gap between information provision and patient self-management.</p>

<hr />

<h2>Scores</h2>

<ul>
<li><p><strong>Overall Relevance Score:</strong> 78 — Strong definition via PEMAT-P, clear articulation of features tied to actionability, though timeliness and full goal alignment not fully addressed.  </p></li>
<li><p><strong>Operationalization Score:</strong> 85 — Detailed use of PEMAT-P with actionable criteria, scoring method, and applied analysis to identify gaps and improvement strategies.</p></li>
</ul>

<hr />

<h2>Supporting Quotes from the Paper</h2>

<ul>
<li><p>“Actionability… evaluates how well patients can identify what they need to do based on the information presented.” (p. 203)  </p></li>
<li><p>“More than half… satisfied Item 19… &lt;30% met Item 21… Item 22… Item 24… and Item 25.” (p. 204)  </p></li>
<li><p>“Self-management materials… offered more detailed instructions and utilized visual aids… distinguishing them from materials on other topics.” (p. 206)  </p></li>
<li><p>“Development and dissemination of quality materials… can minimize the gap between patient education and health behavior practices.” (p. 208)</p></li>
</ul>

<hr />

<h2>Actionability References to Other Papers</h2>

<ul>
<li><p>Shoemaker SJ et al. (2014) — development of PEMAT.  </p></li>
<li><p>Furukawa E et al. (2022) — Japanese version of PEMAT validation.  </p></li>
<li><p>Studies on readability and quality of CKD/HD patient education (e.g., Bresler et al., 2021; Tuot et al., 2013).  </p></li>
<li><p>Federal Plain Language Guidelines (2011).</p></li>
</ul>
