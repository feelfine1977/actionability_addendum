---
Title: "Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP"
Authors: "Pieter Delobelle, Giuseppe Attanasio, Debora Nozza, Su Lin Blodgett, Zeerak Talat"
DOI: "10.18653/v1/2024.emnlp-main.1315"
Year: "2024"
Publication Type: "Journal/Conference Proceedings (EMNLP 2024)"
Discipline/Domain: "Natural Language Processing, AI Ethics"
Subdomain/Topic: "Bias evaluation metrics, actionability assessment"
Eligibility: "Eligible"
Overall Relevance Score: "95"
Operationalization Score: "90"
Contains Definition of Actionability: "Yes"
Contains Systematic Features/Dimensions: "Yes"
Contains Explainability: "Partial"
Contains Interpretability: "Yes"
Contains Framework/Model: "Yes (desiderata-based framework)"
Operationalization Present: "Yes"
Primary Methodology: "Review and Conceptual Framework + Qualitative Analysis"
Study Context: "NLP bias measures"
Geographic/Institutional Context: "International (Authors from KU Leuven, Instituto de Telecomunicações Lisbon, Bocconi, Microsoft Research, MBZUAI)"
Target Users/Stakeholders: "NLP researchers, metric developers, practitioners, policymakers, regulators"
Primary Contribution Type: "Conceptual framework + systematic literature review"
Reason if Not Eligible: "N/A"
Domain Note: "[[Domain/Natural Language Processing, AI Ethics]]"
Feature Notes:
  - "[[Concept/CL - Clarity (Actionability)]]"
  - "[[Concept/CR - Contextual Relevance (Actionability)]]"
  - "[[Concept/FE - Feasibility (Actionability)]]"
  - "[[Concept/TI - Timeliness (Actionability)]]"
  - "[[Concept/EX - Explainability (Actionability)]]"
  - "[[Concept/GA - Goal Alignment (Actionability)]]"
tags:
  - "feature/cl"
  - "feature/cr"
  - "feature/fe"
  - "feature/ti"
  - "feature/ex"
  - "feature/ga"
---
# Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP (2024)
*Pieter Delobelle, Giuseppe Attanasio, Debora Nozza, Su Lin Blodgett, Zeerak Talat*
**DOI:** 10.18653/v1/2024.emnlp-main.1315
**Domain:** [[Domain/Natural Language Processing, AI Ethics]]
**Subdomain/Topic:** Bias evaluation metrics, actionability assessment

## General Summary of the Paper
The authors define actionability in bias measures as the degree to which a measure’s results enable informed decision-making or intervention. They propose a set of desiderata (motivation, underlying bias construct, interval/ideal result, intended use, and reliability) drawn from responsible NLP, measurement modeling, and AI auditing literature. They apply these criteria in a systematic review of 146 NLP papers proposing bias measures, revealing widespread under-specification in intended use, theoretical constructs, and reliability assessment. The findings show that such omissions hinder the practical uptake of bias metrics in addressing harms. Recommendations include clear articulation of motivations, linking results to impacts, ensuring reliability assessment, and considering the actions afforded to different stakeholders.

## How Actionability is Understood
Actionability is “the degree to which a measure’s results enable decision-making or intervention” — results should help identify who is impacted, the scale and source of bias, and guide potential mitigations, safeguards, redesign, or policy changes. It is related to but distinct from validity, interpretability, transparency, and accountability, focusing on enabling meaningful interventions.

  
“Actionability refers to the degree to which a measure’s results enable decision-making or intervention…” (p. 2)  

  
“…results from actionable bias measures should facilitate informed actions with respect to th

---
